{"#!": {"link": "https://en.wikipedia.org/wiki/Shebang_(Unix)", "full_form": "Shebang", "content": "In computing, a shebang is the character sequence consisting of the characters number sign and exclamation mark (#!) at the beginning of a script. It is also called sha-bang,[1][2] hashbang,[3][4] pound-bang,[5] or hash-pling.[6] Under Unix-like operating systems, when a script with a shebang is run as a program, the program loader parses the rest of the script's initial line as an interpreter directive; the specified interpreter program is run instead, passing to it as an argument the path that was initially used when attempting to run the script.[7] For example, if a script is named with the path path/to/script, and it starts with the following line: then the program loader is instructed to run the program /bin/sh instead, passing path/to/script as the first argument. The shebang line is usually ignored by the interpreter because the \"#\" character is a comment marker in many scripting languages; some language interpreters that do not use the hash mark to begin comments (such as Scheme) still may ignore the shebang line in recognition of its purpose.[8] Other solutions rely on a preprocessor that evaluates and removes the shebang line and sends the remainder of the script to a compiler or interpreter.   The form of a shebang interpreter directive is as follows:[7] in which interpreter is an absolute path to an executable program. The optional argument is a string representing a single argument. White space after #! is optional. In Linux, the file specified by interpreter can be executed if it has the execute right and contains code which the kernel can execute directly, if it has a wrapper defined for it via sysctl (such as for executing Microsoft EXE binaries using wine), or if it contains a shebang. On Linux and Minix, an interpreter can also be a script. A chain of shebangs and wrappers yields a directly executable file that gets the encountered scripts as parameters in reverse order. For example, if file /bin/A is an executable file in ELF format, file /bin/B contains the shebang #!/bin/A optparam, and file /bin/C contains the shebang #!/bin/B, then executing file /bin/C resolves to /bin/B /bin/C, which finally resolves to /bin/A optparam /bin/B /bin/C. Some typical shebang lines: Shebang lines may include specific options that are passed to the interpreter. However, implementations vary in the parsing behavior of options; for portability, only one option should be specified without any embedded whitespace. Further portability guidelines are found below. Interpreter directives allow scripts and data files to be used as commands, hiding the details of their implementation from users and other programs, by removing the need to prefix scripts with their interpreter on the command line. A Bourne shell script that is identified by the path some/path/to/foo, has the initial line, and is executed with parameters bar and baz as provides a similar result as having actually executed the following command line instead: If /bin/sh specifies the Bourne shell, then the end result is that all of the shell commands in the file some/path/to/foo are executed with the positional variables $1 and $2 having the values bar and baz, respectively. Also, because the initial number sign is the character used to introduce comments in the Bourne shell language (and in the languages understood by many other interpreters), the entire shebang line is ignored by the interpreter. However, it is up to the interpreter to ignore the shebang line; thus, a script consisting of the following two lines simply echos both lines to standard output when run: When compared to the use of global association lists between file extensions and the interpreting applications, the interpreter directive method allows users to use interpreters not known at a global system level, and without administrator rights. It also allows specific selection of interpreter, without overloading the filename extension namespace (where one file extension refers to more than one file type), and allows the implementation language of a script to be changed without changing its invocation syntax by other programs. Shebangs must specify absolute paths (or paths relative to current working directory) to system executables; this can cause problems on systems that have a non-standard file system layout. Even when systems have fairly standard paths, it is quite possible for variants of the same operating system to have different locations for the desired interpreter. Python, for example, might be in /usr/bin/python, /usr/local/bin/python, or even something like /home/username/bin/python if installed by an ordinary user. Because of this it is common[citation needed] to need to edit the shebang line after copying a script from one computer to another because the path that was coded into the script may not apply on a new machine, depending on the consistency in past convention of placement of the interpreter. For this reason and because POSIX does not standardize path names, POSIX does not standardize the feature. Often, the program /usr/bin/env can be used to circumvent this limitation by introducing a level of indirection. #! is followed by /usr/bin/env, followed by the desired command without full path, as in this example: This mostly works because the path /usr/bin/env is commonly used for the env utility, and it invokes the first sh found in the user's $PATH, typically /bin/sh. On a system with setuid script support this will reintroduce the race eliminated by the /dev/fd workaround described below. There are still some portability issues with OpenServer 5.0.6 and Unicos 9.0.2 which have only /bin/env and no /usr/bin/env. Another portability problem is the interpretation of the command arguments. Some systems, including Linux, do not split up the arguments;[9] for example, when running the script with the first line like, That is, python -c will be passed as one argument to /usr/bin/env, rather than two arguments. Cygwin also behaves this way. Complex interpreter invocations are possible through the use of an additional wrapper.[citation needed] Another problem is scripts containing a carriage return character immediately after the shebang, perhaps as a result of being edited on a system that uses DOS line breaks, such as Microsoft Windows. Some systems interpret the carriage return character as part of the interpreter command, resulting in an error message.[10] POSIX requires sh to be a shell capable of a syntax similar to the Bourne shell, although it does not require it to be located at /bin/sh; for example, some systems such as Solaris have the POSIX-compatible shell at /usr/xpg4/bin/sh.[11] In many Linux systems and recent releases of Mac OS X, /bin/sh is a hard or symbolic link to /bin/bash, the Bourne Again shell. Using syntax specific to Bash while maintaining a shebang pointing to the Bourne shell is not portable.[12] The shebang is actually a human-readable instance of a magic number in the executable file, the magic byte string being 0x23 0x21, the two-character encoding in ASCII of #!. This magic number is detected by the \"exec\" family of functions, which determine whether a file is a script or an executable binary. The presence of the shebang will result in the execution of the specified executable, usually an interpreter for the script's language. It has been claimed that some old versions of Unix expect the normal shebang to be followed by a space and a slash (#! /), but this appears to be untrue;[citation needed] rather, blanks after the shebang have traditionally been allowed, and sometimes documented with a space (see the 1980 email in history section below). The shebang characters are represented by the same two bytes in extended ASCII encodings, including UTF-8, which is commonly used for scripts and other text files on current Unix-like systems. However, UTF-8 files may begin with the optional byte order mark (BOM); if the \"exec\" function specifically detects the bytes 0x23 and 0x21, then the presence of the BOM (0xEF 0xBB 0xBF) before the shebang will prevent the script interpreter from being executed. Some authorities recommend against using the byte order mark in POSIX (Unix-like) scripts,[13] for this reason and for wider interoperability and philosophical concerns. Additionally, a byte order mark is not necessary in UTF-8, as that encoding does not have endianness issues; it serves only to identify the encoding as UTF-8. An executable file starting with an interpreter directive is simply called a script, often prefaced with the name or general classification of the intended interpreter. The name shebang for the distinctive two characters may have come from an inexact contraction of SHArp bang or haSH bang, referring to the two typical Unix names for them. Another theory on the sh in shebang is that it is from the default shell sh, usually invoked with shebang.[14] This usage was current by December 1989,[15] and probably earlier. The shebang was introduced by Dennis Ritchie between Edition 7 and 8 at Bell Laboratories. It was also added to the BSD releases from Berkeley's Computer Science Research (present at 2.8BSD[16] and activated by default by 4.2BSD). As AT&T Bell Laboratories Edition 8 Unix, and later editions, were not released to the public, the first widely known appearance of this feature was on BSD. The lack of an interpreter directive, but support for shell scripts, is apparent in the documentation from Version 7 Unix in 1979,[17] which describes instead a facility of the Bourne shell where files with execute permission would be handled specially by the shell, which would (sometimes depending on initial characters in the script, such as \":\" or \"#\") spawn a subshell which would interpret and run the commands contained in the file. In this model, scripts would only behave as other commands if called from within a Bourne shell. An attempt to directly execute such a file via the operating system's own exec() system trap would fail, preventing scripts from behaving uniformly as normal system commands. In later versions of Unix-like systems, this inconsistency was removed. Dennis Ritchie introduced kernel support for interpreter directives in January 1980, for Version 8 Unix, with the following description:[16] Kernel support for interpreter directives spread to other versions of Unix, and one modern implementation can be seen in the Linux kernel source in fs/binfmt_script.c.[18] This mechanism allows scripts to be used in virtually any context normal compiled programs can be, including as full system programs, and even as interpreters of other scripts. As a caveat, though, some early versions of kernel support limited the length of the interpreter directive to roughly 32 characters (just 16 in its first implementation), would fail to split the interpreter name from any parameters in the directive, or had other quirks. Additionally, some modern systems allow the entire mechanism to be constrained or disabled for security purposes (for example, set-user-id support has been disabled for scripts on many systems). Note that, even in systems with full kernel support for the #! magic number, some scripts lacking interpreter directives (although usually still requiring execute permission) are still runnable by virtue of the legacy script handling of the Bourne shell, still present in many of its modern descendants. Scripts are then interpreted by the user's default shell."}, "/.": {"link": "https://en.wikipedia.org/wiki/Slashdot", "full_form": "Slashdot", "content": "Slashdot (sometimes abbreviated as /.) is a social news website that originally billed itself as \"News for Nerds. Stuff that Matters\". It features news stories on science and technology that are submitted and evaluated by site users. Each story has a comments section attached to it where users can add online comments. The website was founded in 1997 by Hope College students Rob Malda, also known as \"CmdrTaco\", and classmate Jeff Bates, also known as \"Hemos\". In 2012, it was acquired[4] by DHI Group, Inc. (i.e., Dice Holdings International, which created the Dice.com website for tech job seekers[5][6]). In January, 2016, BizX acquired Slashdot Media, including both slashdot.org and SourceForge.[1][7][8] Summaries of stories and links to news articles are submitted by Slashdot's own users, and each story becomes the topic of a threaded discussion among users. Discussion is moderated by a user-based moderation system. Randomly selected moderators are assigned points (typically 5) which they can use to rate a comment. Moderation applies either \u22121 or +1 to the current rating, based on whether the comment is perceived as either \"normal\", \"offtopic\", \"insightful\", \"redundant\", \"interesting\", or \"troll\" (among others). The site's comment and moderation system is administered by its own open source content management system, Slash, which is available under the GNU General Public License. In 2012, Slashdot had around 3.7 million unique visitors per month and received over 5300 comments per day.[5] The site has won more than 20 awards, including People's Voice Awards in 2000 for \"Best Community Site\" and \"Best News Site\". Occasionally, a news story posted to the site will link to a server causing a large surge of web traffic, which can overwhelm some smaller or independent sites. This phenomenon is known as the \"Slashdot effect\".   Slashdot was preceded by Rob Malda's personal website \"Chips & Dips\", which, launched in October 1997,[9] featured a single \"rant\" each day about something that interested its author \u2013 typically something to do with Linux or open source software. At the time, Malda was a student at Hope College in Holland, Michigan, majoring in computer science. The site became \"Slashdot\" in September 1997 under the slogan \"News for Nerds. Stuff that Matters,\" and quickly became a hotspot on the Internet for news and information of interest to computer geeks.[10] The name \"Slashdot\" came from a somewhat \"obnoxious parody of a URL\" \u2013 when Malda registered the domain, he desired to make a name that was \"silly and unpronounceable\" \u2013 try pronouncing out, \"h-t-t-p-colon-slash-slash-slashdot-dot-org\".[11] By June 1998, the site was seeing as many as 100,000 page views per day and advertisers began to take notice.[10] Slashdot was co-founded by Rob Malda and Jeff Bates. By December 1998, Slashdot had net revenues of $18,000, yet its Internet profile was higher, and revenues were expected to increase. On June 29, 1999, the site was sold to Linux megasite Andover.net for $1.5\u00a0million in cash and $7\u00a0million in Andover stock at the Initial public offering (IPO) price. Part of the deal was contingent upon the continued employment of Malda and Bates and on the achievement of certain \"milestones\". With the acquisition of Slashdot, Andover.net could now advertise itself as \"the leading Linux/Open Source destination on the Internet\".[12][13] Andover.net merged with VA Linux on February 3, 2000,[14] which changed its name to SourceForge, Inc. on May 24, 2007, and became Geeknet, Inc. on November 4, 2009.[15] Slashdot's 10,000th article was posted after two and a half years on February 24, 2000,[16] and the 100,000th article was posted on December 11, 2009 after 12 years online.[17] During the first 12 years, the most active story with the most responses posted was the post-2004 US Presidential Election article \"Kerry Concedes Election To Bush\" with 5,687 posts. This followed the creation of a new article section, politics.slashdot.org, created at the start of the 2004 election on September 7, 2004.[18] Many of the most popular stories are political, with \"Strike on Iraq\" (March 19, 2003) the second-most-active article and \"Barack Obama Wins US Presidency\" (November 5, 2008) the third-most-active. The rest of the 10 most active articles are an article announcing the 2005 London bombings, and several articles about Evolution vs. Intelligent Design, Saddam Hussein's capture, and Fahrenheit 9/11. Articles about Microsoft and its Windows Operating System are popular. A thread posted in 2002 titled \"What's Keeping You On Windows?\" was the 10th-most-active story, and an article about Windows 2000/NT4 source-code leaks the most visited article with more than 680,000 hits.[19] Some controversy erupted on March 9, 2001 after an anonymous user posted the full text of Scientology's \"Operating Thetan Level Three\" (OT III) document in a comment attached to a Slashdot article. The Church of Scientology demanded that Slashdot remove the document under the Digital Millennium Copyright Act. A week later, in a long article, Slashdot editors explained their decision to remove the page while providing links and information on how to get the document from other sources.[20] Slashdot Japan was launched on May 28, 2001 (although the first article was published April 5, 2001) and is an official offshoot of the US-based Web site. As of January 2010[update] the site was owned by OSDN-Japan, Inc., and carried some of the US-based Slashdot articles as well as localized stories.[21][22] An external site, New Media Services, has reported the importance of Online Moderation last December 1, 2011.[23] On Valentine's Day 2002, founder Rob Malda proposed to longtime girlfriend Kathleen Fent using the front page of Slashdot.[24][25] They were married on December 8, 2002, in Las Vegas, Nevada.[26] Slashdot implemented a paid subscription service on March 1, 2002. Slashdot's subscription model works by allowing users to pay a small fee to be able to view pages without banner ads, starting at a rate of $5 per 1,000 page views \u2013 non-subscribers may still view articles and respond to comments, with banner ads in place.[27] On March 6, 2003, subscribers were given the ability to see articles 10 to 20 minutes before they are released to the public.[28] Slashdot altered its threaded discussion forum display software to explicitly show domains for links in articles, as \"users made a sport out of tricking unsuspecting readers into visiting [Goatse.cx].\"[29] In observance of April Fools' Day in 2006, Slashdot temporarily changed its signature teal color theme to a warm palette of bubblegum pink and changed its masthead from the usual, \"News for Nerds\" motto to, \"OMG!!! Ponies!!!\" Editors joked that this was done to increase female readership.[30] In another supposed April Fools' Day joke, User Achievement tags were introduced on April 1, 2009.[31] This system allowed users to be tagged with various achievements, such as \"The Tagger\" for tagging a story or \"Member of the {1,2,3,4,5} Digit UID Club\" for having a Slashdot UID consisting of a certain number of digits. While it was posted on April Fools' Day to allow for certain joke achievements, the system is real.[32] Slashdot unveiled its newly redesigned site on June 4, 2006, following a CSS Redesign Competition. The winner of the competition was Alex Bendiken, who built on the initial CSS framework of the site. The new site looks similar to the old one but is more polished with more rounded curves, collapsible menus, and updated fonts.[33] On November 9 that same year, Malda wrote that Slashdot attained 16,777,215 (or 224 \u2212 1) comments, which broke the database for three hours until the administrators fixed the issue.[34] On January 25, 2011, the site launched its third major redesign in its 13.5-year history, which gutted the HTML and CSS, and updated the graphics.[35] On August 25, 2011, Malda resigned as Editor-in-Chief with immediate effect. He did not mention any plans for the future, other than spending more time with his family, catching up on some reading, and possibly writing a book.[36][37] His final farewell message received over 1,400 comments within 24 hours on the site.[38] On December 7, 2011, Slashdot announced that it would start to push what the company described as \"sponsored\" Ask Slashdot questions.[39] On March 28, 2012, Slashdot launched Slashdot TV.[40] Two months later, in May 2012, Slashdot launched SlashBI, SlashCloud, and SlashDataCenter, three Websites dedicated to original journalistic content. The Websites proved controversial, with longtime Slashdot users commenting that the original content ran counter to the Website's longtime focus on user-generated submissions.[41] Nick Kolakowski, the editor of the three Websites, told The Next Web that the Websites were \u201cmeant to complement Slashdot with an added layer of insight into a very specific area of technology, without interfering with Slashdot\u2019s longtime focus on tech-community interaction and discussion.\u201d Despite the debate, articles published on SlashCloud and SlashBI attracted attention from io9,[42] NPR,[43] Nieman Lab,[44] Vanity Fair, and other publications. In September 2012, Slashdot, SourceForge, and Freecode were acquired by online job site Dice.com for $20 million, and incorporated into a subsidiary known as Slashdot Media.[5][6] While initially stating that there were no plans for major changes to Slashdot,[6] in October 2013, Slashdot launched a \"beta\" for a significant redesign of the site, which featured a simpler appearance and commenting system.[45][46] While initially an opt-in beta, the site automatically began migrating selected users to the new design in February 2014; the rollout led to a negative response from many longtime users, upset by the added visual complexity, and the removal of features, such as comment viewing, that distinguished Slashdot from other news sites. An organized boycott of the site was held from February 10 to 17, 2014.[45] The \"beta\" site was eventually shelved. In July 2015, Dice announced that it planned to sell Slashdot and SourceForge; in particular, the company stated in a filing that it was unable to \"successfully [leverage] the Slashdot user base to further Dice's digital recruitment business\".[47] On January 27, 2016, the two sites were sold to the San Diego-based BizX, LLC for an undisclosed amount.[8][47][48] It was run by its founder, Rob \"CmdrTaco\" Malda, from 1998 until 2011. He shared editorial responsibilities with several other editors including Timothy Lord, Patrick \"Scuttlemonkey\" McGarry, Jeff \"Soulskill\" Boehm, Rob \"Samzenpus\" Rozeboom, and Keith Dawson.[49][50] Jonathan \"cowboyneal\" Pater is another popular editor of Slashdot, who came to work for Slashdot as a programmer and systems administrator. His online nickname (handle), CowboyNeal, is inspired by a Grateful Dead tribute to Neal Cassady in their song, \"That's It for the Other One\". He is best known as the target of the usual comic poll option,[51] a tradition started by Chris DiBona.[52] Slashdot runs on Slash, a content management system available under the GNU General Public License.[53] Early versions of Slash were written by Rob Malda, co-founder of Slashdot, in the spring of 1998. After Andover.net bought Slashdot in June 1999,[54] several programmers were hired to structure the code and render it scalable, as its users had increased from a few hundred to tens of thousands. This work was done by Brian Aker, Patrick Galbraith and Chris Nandor, resulting in version 2 of the software, released in 2001. Slash remains Free software and anyone can contribute to development. Slashdot's editors are primarily responsible for selecting and editing the primary stories that are posted daily by submitters. The editors provide a one-paragraph summary for each story and a link to an external website where the story originated. Each story becomes the topic for a threaded discussion among the site's users.[55] A user-based moderation system is employed to filter out abusive or offensive comments.[56] Every comment is initially given a score of \u22121 to +2, with a default score of +1 for registered users, 0 for anonymous users (Anonymous Coward), +2 for users with high \"karma\", or \u22121 for users with low \"karma\". As moderators read comments attached to articles, they click to moderate the comment, either up (+1) or down (\u22121). Moderators may choose to attach a particular descriptor to the comments as well, such as \"normal\", \"offtopic\", \"flamebait\", \"troll\", \"redundant\", \"insightful\", \"interesting\", \"informative\", \"funny\", \"overrated\", or \"underrated\", with each corresponding to a \u22121 or +1 rating. So a comment may be seen to have a rating of \"+1 insightful\" or \"\u22121 troll\".[50] Comments are very rarely deleted, even if they contain hateful remarks.[57][58] Moderation points add to a user's rating, which is known as \"karma\" on Slashdot. Users with high \"karma\" are eligible to become moderators themselves. The system does not promote regular users as \"moderators\" and instead assigns five moderation points at a time to users based on the number of comments they have entered in the system \u2013 once a user's moderation points are used up, they can no longer moderate articles (though they can be assigned more moderation points at a later date). Paid staff editors have an unlimited number of moderation points.[50][55][59] A given comment can have any integer score from \u22121 to +5, and registered users of Slashdot can set a personal threshold so that no comments with a lesser score are displayed.[55][59] For instance, a user reading Slashdot at level +5 will only see the highest rated comments, while a user reading at level \u22121 will see a more \"unfiltered, anarchic version\".[50] A meta-moderation system was implemented on September 7, 1999,[60] to moderate the moderators and help contain abuses in the moderation system.[61][unreliable source?][page\u00a0needed] Meta-moderators are presented with a set of moderations that they may rate as either fair or unfair. For each moderation, the meta-moderator sees the original comment and the reason assigned by the moderator (e.g. troll, funny), and the meta-moderator can click to see the context of comments surrounding the one that was moderated.[55][59] Slashdot features discussion forums on a variety of technology- and science-related topics, or \"News for Nerds\", as its motto states. Articles are divided into the following sections:[62] Slashdot uses a system of \"tags\" where users can categorize a story to group them together and sorting them. Tags are written in all lowercase, with no spaces, and limited to 64 characters. For example, articles could be tagged as being about \"security\" or \"mozilla\". Some articles are tagged with longer tags, such as \"whatcouldpossiblygowrong\" (expressing the perception of catastrophic risk), \"suddenoutbreakofcommonsense\" (used when the community feels that the subject has finally figured out something obvious), \"correlationnotcausation\" (used when scientific articles lack direct evidence; see correlation does not imply causation), or \"getyourasstomars\" (commonly seen in articles about Mars or space exploration).[64][65] As an online community with primarily user-generated content, many in-jokes and internet memes have developed over the course of the site's history. A popular meme (based on an unscientific Slashdot user poll[66]) is, \"In Soviet Russia, noun verb you!\"[67] This type of joke has its roots in the 1960s or earlier, and is known as a \"Russian reversal\". Other popular memes usually pertain to computing or technology, such as \"Imagine a Beowulf cluster of these\",[68] \"But does it run Linux?\",[69] or \"Netcraft now confirms: BSD (or some other software package or item) is dying.\"[70] Users will also typically refer to articles referring to data storage and data capacity by inquiring how much it is in units of Libraries of Congress.[71] Sometimes bandwidth speeds are referred to in units of Libraries of Congress per second. When numbers are quoted, people will comment that the number happens to be the \"combination to their luggage\" (a reference to the Mel Brooks film Spaceballs) and express false anger at the person who revealed it. Slashdotters often use the abbreviation TFA which stands for The fucking article or RTFA (\"Read the fucking article\"), which itself is derived from the abbreviation RTFM.[72] Usage of this abbreviation often exposes comments from posters who have not read the article linked to in the main story. Slashdotters typically like to mock then United States Senator Ted Stevens' 2006 description of the Internet as a \"series of tubes\"[73][74] or former Microsoft CEO Steve Ballmer's chair-throwing incident from 2005.[75][76] Microsoft founder Bill Gates is a popular target of jokes by Slashdotters, and all stories about Microsoft were once identified with a graphic of Gates looking like a Borg from Star Trek: The Next Generation.[77] Many Slashdotters have long talked about the supposed release of Duke Nukem Forever, which was promised in 1997 but was delayed indefinitely (the game was eventually released in 2011).[78] References to the game are commonly brought up in other articles about software packages that are not yet in production even though the announced delivery date has long passed (see vaporware). Having a low Slashdot user identifier (user ID) is highly valued since they are assigned sequentially; having one is a sign that someone has an older account and has contributed to the site longer. For Slashdot's 10-year anniversary in 2007, one of the items auctioned off in the charity auction for the Electronic Frontier Foundation was a 3-digit Slashdot user ID.[32][79] As of 2006, Slashdot had approximately 5.5\u00a0million users per month. As of January 2013, the site's Alexa rank is 2,000, with the average user spending 3\u00a0minutes and 18\u00a0seconds per day on the site and 82,665 sites linking in.[2] The primary stories on the site consist of a short synopsis paragraph, a link to the original story, and a lengthy discussion section, all contributed by users. Discussion on stories can get up to 10,000 posts per day. Slashdot has been considered a pioneer in user-driven content, influencing other sites such as Google News and Wikipedia.[80][81] There has been a dip in readership as of 2011, primarily due to the increase of technology-related blogs and Twitter feeds.[82] In 2002, approximately 50% of Slashdot's traffic consisted of people who simply check out the headlines and click through, while others participate in discussion boards and take part in the community.[83] Many links in Slashdot stories caused the linked site to get swamped by heavy traffic and its server to collapse. This is known as the \"Slashdot effect\",[80][83] a term which was first coined on February 15, 1999 that refers to an article about a \"new generation of niche Web portals driving unprecedented amounts of traffic to sites of interest\".[81][84] Today, most major websites can handle the surge of traffic, but the effect continues to occur on smaller or independent sites.[85] These sites are then said to have been \"Slashdotted\". Slashdot has received over twenty awards, including People's Voice Awards in 2000 in both of the categories for which it was nominated (Best Community Site and Best News Site).[86] It was also voted as one of Newsweek's favorite technology Web sites and rated in Yahoo!'s Top 100 Web sites as the \"Best Geek Hangout\" (2001).[87] The main antagonists in the 2004 novel Century Rain, by Alastair Reynolds \u2013 The Slashers \u2013 are named after Slashdot users.[88] The site was mentioned briefly in the 2000 novel Cosmonaut Keep, written by Ken MacLeod.[89] Several celebrities have stated that they either checked the website regularly or participated in its discussion forums using an account. Some of these celebrities include: Apple co-founder Steve Wozniak,[90] writer and actor Wil Wheaton,[91] and id Software technical director John Carmack.[92]"}, "1GL": {"link": "https://en.wikipedia.org/wiki/First-generation_programming_language", "full_form": "First-Generation Programming Language", "content": "A first-generation programming language (1GL) is a machine-level programming language.[1] A first generation (programming) language (1GL) is a grouping of programming languages that are machine level languages used to program first-generation computers. Originally, no translator was used to compile or assemble the first-generation language. The first-generation programming instructions were entered through the front panel switches of the computer system. The instructions in 1GL are made of binary numbers, represented by 1s and 0s. This makes the language suitable for the understanding of the machine but far more difficult to interpret and learn by the human programmer. The main advantage of programming in 1GL is that the code can run very fast and very efficiently, precisely because the instructions are executed directly by the central processing unit (CPU). One of the main disadvantages of programming in a low level language is that when an error occurs, the code is not as easy to fix. First generation languages are very much adapted to a specific computer and CPU, and code portability is therefore significantly reduced in comparison to higher level languages. Modern day programmers still occasionally use machine level code, especially when programming lower level functions of the system, such as drivers, interfaces with firmware and hardware devices. Modern tools such as native-code compilers are used to produce machine level from a higher-level language. 1. Nwankwogu S.E (2016). Programming Languages and their history."}, "1NF": {"link": "https://en.wikipedia.org/wiki/First_normal_form", "full_form": "First Normal Form", "content": "First normal form (1NF) is a property of a relation in a relational database. A relation is in first normal form if and only if the domain of each attribute contains only atomic (indivisible) values, and the value of each attribute contains only a single value from that domain.[1] The first definition of the term, in a 1971 conference paper by Edgar Codd, defined a relation to be in first normal form when none of its domains have any sets as elements.[2] First normal form is an essential property of a relation in a relational database. Database normalization is the process of representing a database in terms of relations in standard normal forms, where first normal is a minimal requirement. First normal form enforces these criteria:   The following scenario illustrates how a database design might violate first normal form.[3][4] Below is a table that stores the names and telephone numbers of customers. One requirement though is to retain multiple telephone numbers for some customers. The simplest way of satisfying this requirement is to allow the \"Telephone Number\" column in any given row to contain more than one value: Note that the telephone number column simply contains text: numbers of different formats, and more importantly, more than one number for two of the customers. We are duplicating related information in the same column. If we would be satisfied with such arbitrary text, we would be fine. But it's not arbitrary text at all: we obviously intended this column to contain telephone number(s). Seen as telephone numbers, the text is not atomic: it can be subdivided. As well, when seen as telephone numbers, the text contains more than one number in two of our rows. This representation of telephone numbers is not in first normal form: our columns contain non-atomic values, and they contain more than one of them. An apparent solution is to introduce more columns: Technically, this table does not violate the requirement for values to be atomic. However, informally, the two telephone number columns still form a \"repeating group\": they repeat what is conceptually the same attribute, namely a telephone number. An arbitrary and hence meaningless ordering has been introduced: why is 555-861-2025 put into the Telephone Number1 column rather than the Telephone Number2 column? There's no reason why customers could not have more than two telephone numbers, so how many Telephone NumberN columns should there be? It is not possible to search for a telephone number without searching an arbitrary number of columns. Adding an extra telephone number may require the table to be reorganized by the addition of a new column rather than just having a new row (tuple) added. (The null value for Telephone Number2 for customer 789 is also an issue.) To bring the model into the first normal form, we split the strings we used to hold our telephone number information into \"atomic\" (i.e. indivisible) entities: single phone numbers. And we ensure no row contains more than one phone number. Note that the \"ID\" is no longer unique in this solution with duplicated customers. To uniquely identify a row, we need to use a combination of (ID, Telephone Number). The value of the combination is unique although each column separately contains repeated values. Being able to uniquely identify a row (tuple) is a requirement of 1NF. An alternative design uses two tables: Columns do not contain more than one telephone number in this design. Instead, each Customer-to-Telephone Number link appears on its own row. Using Customer ID as key, a one-to-many relationship exists between the name and the number tables. A row in the \"parent\" table, Customer Name, can be associated with many telephone number rows in the \"child\" table, Customer Telephone Number, but each telephone number belongs to one, and only one customer.[5] It is worth noting that this design meets the additional requirements for second and third normal form. Edgar F. Codd's definition of 1NF makes reference to the concept of 'atomicity'. Codd states that the \"values in the domains on which each relation is defined are required to be atomic with respect to the DBMS.\"[6] Codd defines an atomic value as one that \"cannot be decomposed into smaller pieces by the DBMS (excluding certain special functions)\"[7] meaning a column should not be divided into parts with more than one kind of data in it such that what one part means to the DBMS depends on another part of the same column. Hugh Darwen and Chris Date have suggested that Codd's concept of an \"atomic value\" is ambiguous, and that this ambiguity has led to widespread confusion about how 1NF should be understood.[8][9] In particular, the notion of a \"value that cannot be decomposed\" is problematic, as it would seem to imply that few, if any, data types are atomic: Date suggests that \"the notion of atomicity has no absolute meaning\":[10] a value may be considered atomic for some purposes, but may be considered an assemblage of more basic elements for other purposes. If this position is accepted, 1NF cannot be defined with reference to atomicity. Columns of any conceivable data type (from string types and numeric types to array types and table types) are then acceptable in a 1NF table\u2014although perhaps not always desirable; for example, it would be more desirable to separate a Customer Name column into two separate columns as First Name, Surname. According to Date's definition, a table is in first normal form if and only if it is \"isomorphic to some relation\", which means, specifically, that it satisfies the following five conditions:[11] Violation of any of these conditions would mean that the table is not strictly relational, and therefore that it is not in first normal form. Examples of tables (or views) that would not meet this definition of first normal form are: First normal form, as defined by Chris Date, permits relation-valued attributes (tables within tables). Date argues that relation-valued attributes, by means of which a column within a table can contain a table, are useful in rare cases.[15]"}, "10B2": {"link": "https://en.wikipedia.org/wiki/10BASE2", "full_form": "10BASE-2", "content": "10BASE2 (also known as cheapernet, thin Ethernet, thinnet, and thinwire) is a variant of Ethernet that uses thin coaxial cable, terminated with BNC connectors. During the mid to late 1980s this was the dominant 10\u00a0Mbit/s Ethernet standard, but due to the immense demand for high speed networking, the low cost of Category 5 Ethernet cable, and the popularity of 802.11 wireless networks, both 10BASE2 and 10BASE5 have become increasingly obsolete, though devices still exist in some locations.[1] As of 2011, IEEE 802.3 has deprecated this standard for new installations.[2]   The name 10BASE2 is derived from several characteristics of the physical medium. The 10 comes from the transmission speed of 10\u00a0Mbit/s (millions of bits per second). The BASE stands for baseband signalling, and the 2 for a maximum segment length approaching 200\u00a0m (the actual maximum length is 185\u00a0m). 10 Mbit/s Ethernet uses Manchester coding. A binary zero is indicated by a low to high transition in the middle of the bit period and a binary one is indicated by a high to low transition in the middle of the bit period. This allows the clock to be recovered from the signal. However, the additional transitions double the signal bandwidth. 10BASE2 coax cables have a maximum length of 185 metres (607\u00a0ft). The maximum practical number of nodes that can be connected to a 10BASE2 segment is limited to 30[3] with a minimum distance of 50\u00a0cm.[4] In a 10BASE2 network, each stretch of cable is connected to the transceiver (which is usually built into the network adaptor) using a BNC T-connector, with one stretch connected to each female connector of the T. The T-connector must be plugged directly into the network adaptor with no cable in between. As is the case with most other high-speed buses, Ethernet segments have to be terminated with a resistor at each end. Each end of the cable has a 50 ohm (\u03a9) resistor attached. Typically this resistor is built into a male BNC and attached to the last device on the bus. This is most commonly connected directly to the T-connector on a workstation though it does not technically have to be. A few devices such as Digital's DEMPR and DESPR have a built-in terminator and so can only be used at one physical end of the cable run. If termination is missing, or if there is a break in the cable, the AC signal on the bus is reflected, rather than dissipated, when it reaches the end. This reflected signal is indistinguishable from a collision, so no communication can take place. Some terminators have a metallic chain attached to them for grounding purposes, however many people never understood how to properly ground cabling and thus grounded the terminators at both ends rather than just one end. This caused many of the grounding loop problems during that era which caused network outages and/or data corruption when swells of electricity traversed the coaxial cabling's outer shield on its path to the ground with the least resistance. When wiring a 10BASE2 network, special care has to be taken to ensure that cables are properly connected to all T-connectors, and appropriate terminators are installed. One, and only one, terminator must be connected to ground via a ground wire. Bad contacts or shorts are especially difficult to diagnose, though a time-domain reflectometer will find most problems quickly. A failure at any point of the network cabling tends to prevent all communications. For this reason, 10BASE2 networks can be difficult to maintain and were often replaced by 10BASE-T networks, which (provided category 5 cable or better was used) also provided a good upgrade path to 100BASE-TX. An alternative, more reliable connection was established by the introduction of EAD sockets. 10BASE2 networks cannot generally be extended without breaking service temporarily for existing users and the presence of many joints in the cable also makes them very vulnerable to accidental or malicious disruption. There were proprietary wallport/cable systems that claimed to avoid these problems (e.g. SaferTap) but these never became widespread, possibly due to a lack of standardization. 10BASE2 systems do have a number of advantages over 10BASE-T. No hub is required as with 10BASE-T, so the hardware cost is minimal, and wiring can be particularly easy since only a single wire run is needed, which can be sourced from the nearest computer. These characteristics mean that 10BASE2 is ideal for a small network of two or three machines, perhaps in a home where easily concealed wiring may be an advantage. For a larger complex office network, the difficulties of tracing poor connections make it impractical. Unfortunately for 10BASE2, by the time multiple home computer networks became common, the format had already been practically superseded. It is very difficult to find 10BASE2-compatible network cards as distinct pieces of equipment, and integrated LAN controllers on motherboards don't have the connector. 10BASE2 uses RG-58A/U or similar for a maximum segment length of 185\u00a0m as opposed to the thicker RG-8-like cable used in 10BASE5 networks with a maximum length of 500\u00a0m. 10BASE2 uses BNC T-connectors with tranceivers integrated into the NIC instead of the 10BASE5 \"vampire tap\" system with external transceivers connected via AUI. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "10B5": {"link": "https://en.wikipedia.org/wiki/10BASE5", "full_form": "10BASE-5", "content": "10BASE5 (also known as thick Ethernet or thicknet) was the first commercially available variant of Ethernet. 10BASE5 uses a thick and stiff coaxial cable up to 500 metres (1,600\u00a0ft) in length. Up to 100 stations can be connected to the cable using vampire taps and share a single collision domain with 10\u00a0Mbit/s of bandwidth shared among them. The system is difficult to install and maintain. 10BASE5 was superseded by much cheaper and more convenient alternatives: first by 10BASE2 based on a thinner coaxial cable, and then once Ethernet over twisted pair was developed, by 10BASE-T and its successors 100BASE-TX and 1000BASE-T. As of 2003, IEEE 802.3 has deprecated this standard for new installations.[1]   The name 10BASE5 is derived from several characteristics of the physical medium. The 10 refers to its transmission speed of 10\u00a0Mbit/s. The BASE is short for baseband signalling (as opposed to broadband), and the 5 stands for the maximum segment length of 500 metres (1,600\u00a0ft).[2] For its physical layer 10BASE5 uses cable similar to RG-8/U coaxial cable but with extra braided shielding. This is a stiff, 0.375-inch (9.5\u00a0mm) diameter cable with an impedance of 50\u00a0ohms, a solid center conductor, a foam insulating filler, a shielding braid, and an outer jacket. The outer jacket is often yellow-to-orange fluorinated ethylene propylene (for fire resistance) so it often is called \"yellow cable\", \"orange hose\", or sometimes humorously \"frozen yellow garden hose\".[3] 10BASE5 coaxial cables had a maximum length of 500 metres (1,600\u00a0ft). Up to 100 nodes could be connected to a 10BASE5 segment.[4] Transceiver nodes can be connected to cable segments with N connectors, or via a vampire tap, which allows new nodes to be added while existing connections are live. A vampire tap clamps onto the cable, a hole is drilled through the outer shielding, and a spike is forced to pierce and contact the inner conductor while other spikes bite into the outer braided shield. Care is required to keep the outer shield from touching the spike; installation kits include a \"coring tool\" to drill through the outer layers and a \"braid pick\" to clear stray pieces of the outer shield. Transceivers should be installed only at precise 2.5-metre intervals. This distance was chosen to not correspond to the wavelength of the signal; this ensures that the reflections from multiple taps are not in phase.[5] These suitable points are marked on the cable with black bands. The cable is required to be one continuous run; T-connections are not allowed. As is the case with most other high-speed buses, segments must be terminated at each end. For coaxial-cable-based Ethernet, each end of the cable has a 50\u00a0ohm resistor attached. Typically this resistor is built into a male N connector and attached to the end of the cable just past the last device. With termination missing, or if there is a break in the cable, the signal on the bus will be reflected, rather than dissipated when it reached the end. This reflected signal is indistinguishable from a collision, and prevents communication. Adding new stations to network is complicated by the need to accurately pierce the cable. The cable is stiff and difficult to bend around corners. One improper connection could take down the whole network and finding the source of the trouble is difficult.[6] This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "10B-F": {"link": "https://en.wikipedia.org/wiki/10BASE-F", "full_form": "10BASE-F", "content": "10BASE-F is a generic term for the family of 10\u00a0Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents its maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. In fact, there are at least three different kinds of 10BASE-F. All require two strands of 62.5/125\u00a0\u00b5m multimode fiber.[1] One strand is used for data transmission and one strand is used for reception, making 10BASE-F a full-duplex technology. The 10BASE-F variants include 10BASE-FL, 10BASE-FB and 10BASE-FP. Of these only 10BASE-FL experienced widespread use. All 10BASE-F variants deliver 10\u00a0Mbit/s over a fiber pair.[1] These 10\u00a0Mbit/s standards have been largely replaced by faster Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards.   10BASE-FL is the most commonly used 10BASE-F specification of Ethernet over optical fiber. In 10BASE-FL, FL stands for fiber optic link. It replaces the original fiber-optic inter-repeater link (FOIRL) specification, but retains compatibility with FOIRL-based equipment. The maximum segment length supported is 2000 meters[2] When mixed with FOIRL equipment, maximum segment length is limited to FOIRL's 1000 meters.[1] Today, 10BASE-FL is rarely used in networking and has been replaced by the family of Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards. The 10BASE-FB (10BASE-FiberBackbone) is a network segment used to bridge network hubs. Due to the synchronous operation of 10BASE-FB, delays normally associated with Ethernet repeaters are reduced, thus allowing segment distances to be extended without compromising the collision detection mechanism. The maximum allowable segment length for 10BASE-FB is 2000 meters.[3] 10BASE-FP calls for a non-powered signal coupler capable of linking up to 33 devices, with each segment being up to 500m in length.[1][4] This formed a star-type network centered on the signal coupler. There are no devices known to have implemented this standard.[1]"}, "10B-FB": {"link": "https://en.wikipedia.org/wiki/10BASE-FB", "full_form": "10BASE-FB", "content": "10BASE-F is a generic term for the family of 10\u00a0Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents its maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. In fact, there are at least three different kinds of 10BASE-F. All require two strands of 62.5/125\u00a0\u00b5m multimode fiber.[1] One strand is used for data transmission and one strand is used for reception, making 10BASE-F a full-duplex technology. The 10BASE-F variants include 10BASE-FL, 10BASE-FB and 10BASE-FP. Of these only 10BASE-FL experienced widespread use. All 10BASE-F variants deliver 10\u00a0Mbit/s over a fiber pair.[1] These 10\u00a0Mbit/s standards have been largely replaced by faster Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards.   10BASE-FL is the most commonly used 10BASE-F specification of Ethernet over optical fiber. In 10BASE-FL, FL stands for fiber optic link. It replaces the original fiber-optic inter-repeater link (FOIRL) specification, but retains compatibility with FOIRL-based equipment. The maximum segment length supported is 2000 meters[2] When mixed with FOIRL equipment, maximum segment length is limited to FOIRL's 1000 meters.[1] Today, 10BASE-FL is rarely used in networking and has been replaced by the family of Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards. The 10BASE-FB (10BASE-FiberBackbone) is a network segment used to bridge network hubs. Due to the synchronous operation of 10BASE-FB, delays normally associated with Ethernet repeaters are reduced, thus allowing segment distances to be extended without compromising the collision detection mechanism. The maximum allowable segment length for 10BASE-FB is 2000 meters.[3] 10BASE-FP calls for a non-powered signal coupler capable of linking up to 33 devices, with each segment being up to 500m in length.[1][4] This formed a star-type network centered on the signal coupler. There are no devices known to have implemented this standard.[1]"}, "10B-FL": {"link": "https://en.wikipedia.org/wiki/10BASE-FL", "full_form": "10BASE-FL", "content": "10BASE-F is a generic term for the family of 10\u00a0Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents its maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. In fact, there are at least three different kinds of 10BASE-F. All require two strands of 62.5/125\u00a0\u00b5m multimode fiber.[1] One strand is used for data transmission and one strand is used for reception, making 10BASE-F a full-duplex technology. The 10BASE-F variants include 10BASE-FL, 10BASE-FB and 10BASE-FP. Of these only 10BASE-FL experienced widespread use. All 10BASE-F variants deliver 10\u00a0Mbit/s over a fiber pair.[1] These 10\u00a0Mbit/s standards have been largely replaced by faster Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards.   10BASE-FL is the most commonly used 10BASE-F specification of Ethernet over optical fiber. In 10BASE-FL, FL stands for fiber optic link. It replaces the original fiber-optic inter-repeater link (FOIRL) specification, but retains compatibility with FOIRL-based equipment. The maximum segment length supported is 2000 meters[2] When mixed with FOIRL equipment, maximum segment length is limited to FOIRL's 1000 meters.[1] Today, 10BASE-FL is rarely used in networking and has been replaced by the family of Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards. The 10BASE-FB (10BASE-FiberBackbone) is a network segment used to bridge network hubs. Due to the synchronous operation of 10BASE-FB, delays normally associated with Ethernet repeaters are reduced, thus allowing segment distances to be extended without compromising the collision detection mechanism. The maximum allowable segment length for 10BASE-FB is 2000 meters.[3] 10BASE-FP calls for a non-powered signal coupler capable of linking up to 33 devices, with each segment being up to 500m in length.[1][4] This formed a star-type network centered on the signal coupler. There are no devices known to have implemented this standard.[1]"}, "10B-FP": {"link": "https://en.wikipedia.org/wiki/10BASE%E2%80%91FP", "full_form": "10BASE-FP", "content": "10BASE-F is a generic term for the family of 10\u00a0Mbit/s Ethernet standards using fiber optic cable. In 10BASE-F, the 10 represents its maximum throughput of 10 Mbit/s, BASE indicates its use of baseband transmission, and F indicates that it relies on medium of fiber-optic cable. In fact, there are at least three different kinds of 10BASE-F. All require two strands of 62.5/125\u00a0\u00b5m multimode fiber.[1] One strand is used for data transmission and one strand is used for reception, making 10BASE-F a full-duplex technology. The 10BASE-F variants include 10BASE-FL, 10BASE-FB and 10BASE-FP. Of these only 10BASE-FL experienced widespread use. All 10BASE-F variants deliver 10\u00a0Mbit/s over a fiber pair.[1] These 10\u00a0Mbit/s standards have been largely replaced by faster Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards.   10BASE-FL is the most commonly used 10BASE-F specification of Ethernet over optical fiber. In 10BASE-FL, FL stands for fiber optic link. It replaces the original fiber-optic inter-repeater link (FOIRL) specification, but retains compatibility with FOIRL-based equipment. The maximum segment length supported is 2000 meters[2] When mixed with FOIRL equipment, maximum segment length is limited to FOIRL's 1000 meters.[1] Today, 10BASE-FL is rarely used in networking and has been replaced by the family of Fast Ethernet, Gigabit Ethernet and 100 Gigabit Ethernet standards. The 10BASE-FB (10BASE-FiberBackbone) is a network segment used to bridge network hubs. Due to the synchronous operation of 10BASE-FB, delays normally associated with Ethernet repeaters are reduced, thus allowing segment distances to be extended without compromising the collision detection mechanism. The maximum allowable segment length for 10BASE-FB is 2000 meters.[3] 10BASE-FP calls for a non-powered signal coupler capable of linking up to 33 devices, with each segment being up to 500m in length.[1][4] This formed a star-type network centered on the signal coupler. There are no devices known to have implemented this standard.[1]"}, "10B-T": {"link": "https://en.wikipedia.org/wiki/10BASE-T", "full_form": "10BASE-T", "content": "Ethernet over twisted pair technologies use twisted-pair cables for the physical layer of an Ethernet computer network. Early Ethernet had used various grades of coaxial cable, but in 1984, StarLAN showed the potential of simple unshielded twisted pair. This led to the development of 10BASE-T and its successors 100BASE-TX and 1000BASE-T, supporting speeds of 10, 100 and 1,000\u00a0Mbit/s respectively.[a] All these three standards define both full-duplex and half-duplex communication. However, half-duplex operation for gigabit speed isn't supported by any existing hardware.[2][3] Higher speed standards, 2.5GBASE-T up to 40GBASE-T[4] running at 2.5 to 40\u00a0Gbit/s, consequently define only full-duplex point-to-point links which are generally connected by network switches, and don't support the traditional shared-medium CSMA/CD operation.[5] All these standards use 8P8C connectors,[b] and the cables from Cat3[c] to Cat8 have four pairs of wires; though 10BASE-T and 100BASE-TX only use two of the pairs.   The Institute of Electrical and Electronics Engineers (IEEE) standards association ratified several versions of the technology. The first two early designs were StarLAN, standardized in 1986, at one megabit per second,[6] and LattisNet, developed in January 1987, at 10 megabit per second.[7][8] Both were developed before the 10BASE-T standard (published in 1990 as IEEE 802.3i) and used different signalling, so they were not directly compatible with it.[9] In 1988 AT&T released StarLAN 10, named for working at 10 Mbit/s.[10] The StarLAN 10 signalling was used as the basis of 10BASE-T, with the addition of \"link beat\" to quickly indicate connection status. (A number of network interface cards at the time could work with either StarLAN 10 or 10BASE-T, by switching link beat on or off.[11]) Using twisted pair cabling, in a star topology, for Ethernet addressed several weaknesses of the previous standards: The common names for the standards derive from aspects of the physical media. The leading number (10 in 10BASE-T) refers to the transmission speed in Mbit/s. BASE denotes that baseband transmission is used. The T designates twisted pair cable, where the pair of wires for each signal is twisted together to reduce radio frequency interference and crosstalk between pairs. Where there are several standards for the same transmission speed, they are distinguished by a letter or digit following the T, such as TX, referring to the encoding method and number of lanes.[12] Twisted-pair Ethernet standards are such that the majority of cables can be wired \"straight-through\" (pin 1 to pin 1, pin 2 to pin 2 and so on), but others may need to be wired in the \"crossover\" form (receive to transmit and transmit to receive). It is conventional to wire cables for 10- or 100-Mbit/s Ethernet to either the T568A or T568B standards. Since these standards differ only in that they swap the positions of the two pairs used for transmitting and receiving (TX/RX), a cable with T568A wiring at one end and T568B wiring at the other is referred to as a crossover cable. The terms used in the explanations of the 568 standards, tip and ring, refer to older communication technologies, and equate to the positive and negative parts of the connections. A 10BASE-T or 100BASE-TX node such as a PC uses a connector wiring called medium dependent interfaces (MDI), transmitting on pin 1 and 2 and receiving on pin 3 and 6 to a network device. An infrastructure node (a hub or a switch) accordingly uses a connector wiring called MDI-X, transmitting on pin 3 and 6 and receiving on pin 1 and 2. These ports are connected using a \"straight-through\" cable, so each transmitter talks to the receiver on the other side. Nodes can have two types of ports: MDI (uplink port) or MDI-X (regular port, 'X' for internal crossover). Hubs and switches have regular ports. Routers, servers and end hosts (e.g. personal computers) have uplink ports. When two nodes having the same type of ports need to be connected, a crossover cable is often required at speeds of 10 or 100\u00a0Mbit/s, else connecting nodes having different type of ports (i.e. MDI to MDI-X and vice versa) requires straight-through cable. Thus connecting an end host to a hub or switch requires a straight-through cable. On switches/hubs sometimes a button is provided to allow a port to act as either a normal (regular) or an uplink port, i.e. using MDI-X or MDI pinout respectively. Many modern Ethernet host adapters can automatically detect another computer connected with a straight-through cable and then automatically introduce the required crossover, if needed; if neither of the adapters has this capability, then a crossover cable is required. Most newer switches have automatic crossover (\"auto MDI-X\" or \"auto-uplink\") on all ports, eliminating the uplink port and the MDI/MDI-X switch, and allowing all connections to be made with straight-through cables. If both devices being connected support 1000BASE-T according to the standards, they will connect regardless of whether a straight-through or crossover cable is used.[13] A 10BASE-T transmitter sends two differential voltages, +2.5\u00a0V or \u22122.5\u00a0V. 100BASE-TX follows the same wiring patterns as 10BASE-T, but is more sensitive to wire quality and length, due to the higher bit rates. A 100BASE-TX transmitter sends three differential voltages, +1\u00a0V, 0\u00a0V, or \u22121\u00a0V.[14] 1000BASE-T uses all four pairs bi-directionally and the standard includes auto MDI-X; however, implementation is optional. With the way that 1000BASE-T implements signaling, how the cable is wired is immaterial in actual usage. The standard on copper twisted pair is IEEE 802.3ab for Cat 5e UTP, or 4D-PAM5; four dimensions using PAM (pulse amplitude modulation) with five voltages, \u22122\u00a0V, \u22121\u00a0V, 0\u00a0V, +1\u00a0V, and +2\u00a0V.[15] While +2\u00a0V to \u22122\u00a0V voltage may appear at the pins of the line driver, the voltage on the cable is nominally +1\u00a0V, +0.5\u00a0V, 0\u00a0V, \u22120.5\u00a0V and \u22121\u00a0V.[16] 100BASE-TX and 1000BASE-T were both designed to require a minimum of Category 5 cable and also specify a maximum cable length of 100 meters. Category 5 cable has since been deprecated and new installations use Category 5e. Unlike earlier Ethernet standards using broadband and coaxial cable, such as 10BASE5 (thicknet) and 10BASE2 (thinnet), 10BASE-T does not specify the exact type of wiring to be used, but instead specifies certain characteristics that a cable must meet. This was done in anticipation of using 10BASE-T in existing twisted-pair wiring systems that may not conform to any specified wiring standard. Some of the specified characteristics are attenuation, characteristic impedance, timing jitter, propagation delay, and several types of noise. Cable testers are widely available to check these parameters to determine if a cable can be used with 10BASE-T. These characteristics are expected to be met by 100 meters of 24-gauge unshielded twisted-pair cable. However, with high quality cabling, cable runs of 150 meters or longer are often obtained and are considered viable by most technicians familiar with the 10BASE-T specification.[citation needed] 10BASE-T and 100BASE-TX only require two pairs (pins 1\u20132, 3\u20136) to operate. Since Category 5 cable has four pairs, it is possible, but not necessarily standards compliant, to use the spare pairs (pins 4\u20135, 7\u20138) in 10- and 100-Mbit/s configurations. The spare pairs may be used for Power over Ethernet (PoE), or two phone lines, or a second 10BASE-T or 100BASE-TX connection. In practice, great care must be taken to separate these pairs as most 10/100-Mbit/s hubs, switches, and PCs electrically terminate the unused pins.[citation needed] Moreover, 1000BASE-T requires all four pairs to operate. Many different modes of operations (10BASE-T half duplex, 10BASE-T full duplex, 100BASE-TX half duplex, ...) exist for Ethernet over twisted pair, and most network adapters are capable of different modes of operation. 1000BASE-T requires autonegotiation to be on in order to operate. When two linked interfaces are set to different duplex modes, the effect of this duplex mismatch is a network that functions much more slowly than its nominal speed. Duplex mismatch may be inadvertently caused when an administrator configures an interface to a fixed mode (e.g. 100\u00a0Mbit/s full duplex) and fails to configure the remote interface, leaving it set to autonegotiate. Then, when the autonegotiation process fails, half duplex is assumed by the autonegotiating side of the link.  "}, "100B-FX": {"link": "https://en.wikipedia.org/wiki/100BASE-FX", "full_form": "100BASE-FX", "content": "In computer networking, Fast Ethernet is a collective term for a number of Ethernet standards that carry traffic at the nominal rate of 100\u00a0Mbit/s (the earlier Ethernet speed was 10\u00a0Mbit/s). Of the Fast Ethernet standards, 100BASE-TX is by far the most common. Fast Ethernet was introduced in 1995 as the IEEE 802.3u standard[1] and remained the fastest version of Ethernet for three years before the introduction of Gigabit Ethernet.[2] The acronym GE/FE is sometimes used for devices supporting both standards.[3]   Fast Ethernet is an extension of the 10-megabit Ethernet standard. It runs on UTP data or optical fiber cable in a star wired bus topology, similar to 10BASE-T where all cables are attached to a hub. Fast Ethernet devices are generally backward compatible with existing 10BASE-T systems, enabling plug-and-play upgrades from 10BASE-T. Fast Ethernet is sometimes referred to as 100BASE-X, where \"X\" is a placeholder for the FX and TX variants.[4] The standard specifies the use of CSMA/CD for media access control. A full-duplex mode is also specified and in practice all modern networks use Ethernet switches and operate in full-duplex mode. The \"100\" in the media type designation refers to the transmission speed of 100\u00a0Mbit/s, while the \"BASE\" refers to baseband signalling. The letter following the dash (\"T\" or \"F\") refers to the physical medium that carries the signal (twisted pair or fiber, respectively), while the last character (\"X\", \"4\", etc.) refers to the used encoding method. A Fast Ethernet adapter can be logically divided into a media access controller (MAC), which deals with the higher-level issues of medium availability, and a Physical Layer Interface (PHY). The MAC may be linked to the PHY by a four-bit 25\u00a0MHz synchronous parallel interface known as a media-independent interface (MII), or by a two-bit 50\u00a0MHz variant called reduced media independent interface (RMII). In rare cases the MII may be an external connection but is usually a connection between ICs in a network adapter or even within a single IC. The specs are written based on the assumption that the interface between MAC and PHY will be a MII but they do not require it. Repeaters (hubs) may use the MII to connect to multiple PHYs for their different interfaces. The MII fixes the theoretical maximum data bit rate for all versions of Fast Ethernet to 100\u00a0Mbit/s. The data signaling rate actually observed on real networks is less than the theoretical maximum, due to the necessary header and trailer (addressing and error-detection bits) on every frame, the occasional \"lost frame\" due to noise, and time waiting after each sent frame for other devices on the network to finish transmitting. 100BASE-T is any of several Fast Ethernet standards for twisted pair cables, including: 100BASE-TX (100\u00a0Mbit/s over two-pair Cat5 or better cable), 100BASE-T4 (100 Mbit/s over four-pair Cat3 or better cable, defunct), 100BASE-T2 (100\u00a0Mbit/s over two-pair Cat3 or better cable, also defunct). The segment length for a 100BASE-T cable is limited to 100 metres (328\u00a0ft) (as with 10BASE-T and gigabit Ethernet). All are or were standards under IEEE 802.3 (approved 1995). Almost all 100BASE-T installations are 100BASE-TX. In the early days of Fast Ethernet, much vendor advertising centered on claims by competing standards that said vendors' standards will work better with existing cables than other standards. In practice, it was quickly discovered that few existing networks actually met the assumed standards, because 10 Megabit Ethernet was very tolerant of minor deviations from specified electrical characteristics and few installers ever bothered to make exact measurements of cable and connection quality; if Ethernet worked over a cable, no matter how well it worked, it was deemed acceptable. Thus most networks had to be rewired for 100 Megabit speed whether or not there had supposedly been CAT3 or CAT5 cable runs.[citation needed] 100BASE-TX is the predominant form of Fast Ethernet, and runs over two wire-pairs inside a category 5 or above cable. Like 10BASE-T, the active pairs in a standard connection are terminated on pins 1, 2, 3 and 6. Since a typical category 5 cable contains 4 pairs, it can support two 100BASE-TX links with a wiring adaptor.[5] Cabling is conventional wired to TIA/EIA-568-B's termination standards, T568A or T568B. This places the active pairs on the orange and green pairs (canonical second and third pairs). Each network segment can have a maximum cabling distance of 100 metres (328\u00a0ft). In its typical configuration, 100BASE-TX uses one pair of twisted wires in each direction, providing 100\u00a0Mbit/s of throughput in each direction (full-duplex). See IEEE 802.3 for more details. The configuration of 100BASE-TX networks is very similar to 10BASE-T. When used to build a local area network, the devices on the network (computers, printers etc.) are typically connected to a hub or switch, creating a star network. Alternatively it is possible to connect two devices directly using a crossover cable. With 100BASE-TX hardware, the raw bits (4 bits wide clocked at 25\u00a0MHz at the MII) go through 4B5B binary encoding to generate a series of 0 and 1 symbols clocked at 125\u00a0MHz symbol rate. The 4B5B encoding provides DC equalization and spectrum shaping (see the standard for details). Just as in the 100BASE-FX case, the bits are then transferred to the physical medium attachment layer using NRZI encoding. However, 100BASE-TX introduces an additional, medium dependent sublayer, which employs MLT-3 as a final encoding of the data stream before transmission, resulting in a maximum \"fundamental frequency\" of 31.25\u00a0MHz. The procedure is borrowed from the ANSI X3.263 FDDI specifications, with minor discrepancies.[6] 100BASE-T4 was an early implementation of Fast Ethernet. It requires four twisted copper pairs, but those pairs were only required to be category 3 rather than the category 5 required by TX. One pair is reserved for transmit, one for receive, and the remaining two will switch direction as negotiated. A very unusual 8B6T code is used to convert 8 data bits into 6 base-3 digits (the signal shaping is possible as there are nearly three times as many 6-digit base-3 numbers as there are 8-digit base-2 numbers). The two resulting 3-digit base-3 symbols are sent in parallel over 3 pairs using 3-level pulse-amplitude modulation (PAM-3). The fact that 3 pairs are used to transmit in each direction makes 100BASE-T4 inherently half-duplex. This standard can be implemented with CAT 3, 4, 5 UTP cables, or STP if needed against interference. Maximum distance is limited to 100 meters. 100BASE-T4 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T2, standardized in IEEE 802.3y, the data is transmitted over two copper pairs, 4 bits per symbol, but these pairs are only required to be category 3 rather than the category 5 required by TX. It uses these two pairs for simultaneously transmitting and receiving on both pairs[8] thus allowing full-duplex operation. First, a 4-bit symbol is expanded into two 3-bit symbols through a non-trivial scrambling procedure based on a linear feedback shift register; see the standard for details. This is needed to flatten the bandwidth and emission spectrum of the signal, as well as to match transmission line properties. The mapping of the original bits to the symbol codes is not constant in time and has a fairly large period (appearing as a pseudo-random sequence). The final mapping from symbols to PAM-5 line modulation levels obeys the table on the right. 100BASE-T2 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T1, standardized in IEEE 802.3bw-2015 Clause 96, the data is transmitted over a single copper pair, 3 bits per symbol (PAM3). It supports only full-duplex, transmitting in both directions simultaneously. The twisted-pair cable is required to support 66\u00a0MHz, with a maximum length of 15\u00a0m. No specific connector is defined. The standard is intended for automotive applications or when Fast Ethernet is to be integrated into another application. It has been developed as BroadR-Reach before IEEE standardization.[9] Proposed and marketed by Hewlett Packard, 100BaseVG was an alternative design using category 3 cabling and a token concept instead of CSMA/CD. It was slated for standardization as IEEE 802.12 but it quickly vanished when switched 100BASE-TX became popular. 100BASE-FX is a version of Fast Ethernet over optical fiber. It uses a 1300 nm near-infrared (NIR) light wavelength transmitted via two strands of optical fiber, one for receive (RX) and the other for transmit (TX). Maximum length is 412 metres (1,350\u00a0ft)[citation needed] for half-duplex connections (to ensure collisions are detected), and 2 kilometres (6,600\u00a0ft) for full-duplex over multi-mode optical fiber.[10] 100BASE-FX uses the same 4B5B encoding and NRZI line code that 100BASE-TX does. 100BASE-FX should use SC, ST, LC, MTRJ or MIC connectors with SC being the preferred option.[11] The 100BASE-FX Physical Medium Dependent (PMD) sublayer is defined by FDDI's PMD,[12] so 100BASE-FX is not compatible with 10BASE-FL, the 10\u00a0MBit/s version over optical fiber. 100BASE-SX is a version of Fast Ethernet over optical fiber standardized in TIA/EIA-785-1-2002. It uses two strands of multi-mode optical fiber for receive and transmit. It is a lower cost alternative to using 100BASE-FX, because it uses short wavelength optics which are significantly less expensive than the long wavelength optics used in 100BASE-FX. Depending on fiber quality, 100BASE-SX has a minimum reach of 300 metres (980\u00a0ft).[13] 100BASE-SX uses the same wavelength as 10BASE-FL, the 10\u00a0Mbit/s version over optical fiber. Unlike 100BASE-FX, this allows 100BASE-SX to be backwards-compatible with 10BASE-FL. Because of the shorter wavelength used (850\u00a0nm) and the shorter distance it can support, 100BASE-SX uses less expensive optical components (LEDs instead of lasers) which makes it an attractive option for those upgrading from 10BASE-FL and those who do not require long distances. 100BASE-BX10 is a version of Fast Ethernet over a single strand of optical fiber (unlike 100BASE-FX, which uses a pair of fibers). Single-mode fiber is used, along with a special optics which splits the signal into transmit and receive wavelengths; the two wavelengths used for transmit and receive are 1310\u00a0nm and 1550\u00a0nm. The transceivers on each side of the fiber are not equal, as the one transmitting \"downstream\" (from the center of the network to the outside) uses the 1550\u00a0nm wavelength, and the one transmitting \"upstream\" uses the 1310\u00a0nm wavelength.[14] Transceivers for longer distances of 20 or 40\u00a0km are also available. 100BASE-LX10 is a version of Fast Ethernet over two single-mode optical fibers. It has a nominal reach of 10\u00a0km and a nominal wavelength of 1310\u00a0nm.[14] This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "100B-T": {"link": "https://en.wikipedia.org/wiki/100BASE-T", "full_form": "100BASE-T", "content": "In computer networking, Fast Ethernet is a collective term for a number of Ethernet standards that carry traffic at the nominal rate of 100\u00a0Mbit/s (the earlier Ethernet speed was 10\u00a0Mbit/s). Of the Fast Ethernet standards, 100BASE-TX is by far the most common. Fast Ethernet was introduced in 1995 as the IEEE 802.3u standard[1] and remained the fastest version of Ethernet for three years before the introduction of Gigabit Ethernet.[2] The acronym GE/FE is sometimes used for devices supporting both standards.[3]   Fast Ethernet is an extension of the 10-megabit Ethernet standard. It runs on UTP data or optical fiber cable in a star wired bus topology, similar to 10BASE-T where all cables are attached to a hub. Fast Ethernet devices are generally backward compatible with existing 10BASE-T systems, enabling plug-and-play upgrades from 10BASE-T. Fast Ethernet is sometimes referred to as 100BASE-X, where \"X\" is a placeholder for the FX and TX variants.[4] The standard specifies the use of CSMA/CD for media access control. A full-duplex mode is also specified and in practice all modern networks use Ethernet switches and operate in full-duplex mode. The \"100\" in the media type designation refers to the transmission speed of 100\u00a0Mbit/s, while the \"BASE\" refers to baseband signalling. The letter following the dash (\"T\" or \"F\") refers to the physical medium that carries the signal (twisted pair or fiber, respectively), while the last character (\"X\", \"4\", etc.) refers to the used encoding method. A Fast Ethernet adapter can be logically divided into a media access controller (MAC), which deals with the higher-level issues of medium availability, and a Physical Layer Interface (PHY). The MAC may be linked to the PHY by a four-bit 25\u00a0MHz synchronous parallel interface known as a media-independent interface (MII), or by a two-bit 50\u00a0MHz variant called reduced media independent interface (RMII). In rare cases the MII may be an external connection but is usually a connection between ICs in a network adapter or even within a single IC. The specs are written based on the assumption that the interface between MAC and PHY will be a MII but they do not require it. Repeaters (hubs) may use the MII to connect to multiple PHYs for their different interfaces. The MII fixes the theoretical maximum data bit rate for all versions of Fast Ethernet to 100\u00a0Mbit/s. The data signaling rate actually observed on real networks is less than the theoretical maximum, due to the necessary header and trailer (addressing and error-detection bits) on every frame, the occasional \"lost frame\" due to noise, and time waiting after each sent frame for other devices on the network to finish transmitting. 100BASE-T is any of several Fast Ethernet standards for twisted pair cables, including: 100BASE-TX (100\u00a0Mbit/s over two-pair Cat5 or better cable), 100BASE-T4 (100 Mbit/s over four-pair Cat3 or better cable, defunct), 100BASE-T2 (100\u00a0Mbit/s over two-pair Cat3 or better cable, also defunct). The segment length for a 100BASE-T cable is limited to 100 metres (328\u00a0ft) (as with 10BASE-T and gigabit Ethernet). All are or were standards under IEEE 802.3 (approved 1995). Almost all 100BASE-T installations are 100BASE-TX. In the early days of Fast Ethernet, much vendor advertising centered on claims by competing standards that said vendors' standards will work better with existing cables than other standards. In practice, it was quickly discovered that few existing networks actually met the assumed standards, because 10 Megabit Ethernet was very tolerant of minor deviations from specified electrical characteristics and few installers ever bothered to make exact measurements of cable and connection quality; if Ethernet worked over a cable, no matter how well it worked, it was deemed acceptable. Thus most networks had to be rewired for 100 Megabit speed whether or not there had supposedly been CAT3 or CAT5 cable runs.[citation needed] 100BASE-TX is the predominant form of Fast Ethernet, and runs over two wire-pairs inside a category 5 or above cable. Like 10BASE-T, the active pairs in a standard connection are terminated on pins 1, 2, 3 and 6. Since a typical category 5 cable contains 4 pairs, it can support two 100BASE-TX links with a wiring adaptor.[5] Cabling is conventional wired to TIA/EIA-568-B's termination standards, T568A or T568B. This places the active pairs on the orange and green pairs (canonical second and third pairs). Each network segment can have a maximum cabling distance of 100 metres (328\u00a0ft). In its typical configuration, 100BASE-TX uses one pair of twisted wires in each direction, providing 100\u00a0Mbit/s of throughput in each direction (full-duplex). See IEEE 802.3 for more details. The configuration of 100BASE-TX networks is very similar to 10BASE-T. When used to build a local area network, the devices on the network (computers, printers etc.) are typically connected to a hub or switch, creating a star network. Alternatively it is possible to connect two devices directly using a crossover cable. With 100BASE-TX hardware, the raw bits (4 bits wide clocked at 25\u00a0MHz at the MII) go through 4B5B binary encoding to generate a series of 0 and 1 symbols clocked at 125\u00a0MHz symbol rate. The 4B5B encoding provides DC equalization and spectrum shaping (see the standard for details). Just as in the 100BASE-FX case, the bits are then transferred to the physical medium attachment layer using NRZI encoding. However, 100BASE-TX introduces an additional, medium dependent sublayer, which employs MLT-3 as a final encoding of the data stream before transmission, resulting in a maximum \"fundamental frequency\" of 31.25\u00a0MHz. The procedure is borrowed from the ANSI X3.263 FDDI specifications, with minor discrepancies.[6] 100BASE-T4 was an early implementation of Fast Ethernet. It requires four twisted copper pairs, but those pairs were only required to be category 3 rather than the category 5 required by TX. One pair is reserved for transmit, one for receive, and the remaining two will switch direction as negotiated. A very unusual 8B6T code is used to convert 8 data bits into 6 base-3 digits (the signal shaping is possible as there are nearly three times as many 6-digit base-3 numbers as there are 8-digit base-2 numbers). The two resulting 3-digit base-3 symbols are sent in parallel over 3 pairs using 3-level pulse-amplitude modulation (PAM-3). The fact that 3 pairs are used to transmit in each direction makes 100BASE-T4 inherently half-duplex. This standard can be implemented with CAT 3, 4, 5 UTP cables, or STP if needed against interference. Maximum distance is limited to 100 meters. 100BASE-T4 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T2, standardized in IEEE 802.3y, the data is transmitted over two copper pairs, 4 bits per symbol, but these pairs are only required to be category 3 rather than the category 5 required by TX. It uses these two pairs for simultaneously transmitting and receiving on both pairs[8] thus allowing full-duplex operation. First, a 4-bit symbol is expanded into two 3-bit symbols through a non-trivial scrambling procedure based on a linear feedback shift register; see the standard for details. This is needed to flatten the bandwidth and emission spectrum of the signal, as well as to match transmission line properties. The mapping of the original bits to the symbol codes is not constant in time and has a fairly large period (appearing as a pseudo-random sequence). The final mapping from symbols to PAM-5 line modulation levels obeys the table on the right. 100BASE-T2 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T1, standardized in IEEE 802.3bw-2015 Clause 96, the data is transmitted over a single copper pair, 3 bits per symbol (PAM3). It supports only full-duplex, transmitting in both directions simultaneously. The twisted-pair cable is required to support 66\u00a0MHz, with a maximum length of 15\u00a0m. No specific connector is defined. The standard is intended for automotive applications or when Fast Ethernet is to be integrated into another application. It has been developed as BroadR-Reach before IEEE standardization.[9] Proposed and marketed by Hewlett Packard, 100BaseVG was an alternative design using category 3 cabling and a token concept instead of CSMA/CD. It was slated for standardization as IEEE 802.12 but it quickly vanished when switched 100BASE-TX became popular. 100BASE-FX is a version of Fast Ethernet over optical fiber. It uses a 1300 nm near-infrared (NIR) light wavelength transmitted via two strands of optical fiber, one for receive (RX) and the other for transmit (TX). Maximum length is 412 metres (1,350\u00a0ft)[citation needed] for half-duplex connections (to ensure collisions are detected), and 2 kilometres (6,600\u00a0ft) for full-duplex over multi-mode optical fiber.[10] 100BASE-FX uses the same 4B5B encoding and NRZI line code that 100BASE-TX does. 100BASE-FX should use SC, ST, LC, MTRJ or MIC connectors with SC being the preferred option.[11] The 100BASE-FX Physical Medium Dependent (PMD) sublayer is defined by FDDI's PMD,[12] so 100BASE-FX is not compatible with 10BASE-FL, the 10\u00a0MBit/s version over optical fiber. 100BASE-SX is a version of Fast Ethernet over optical fiber standardized in TIA/EIA-785-1-2002. It uses two strands of multi-mode optical fiber for receive and transmit. It is a lower cost alternative to using 100BASE-FX, because it uses short wavelength optics which are significantly less expensive than the long wavelength optics used in 100BASE-FX. Depending on fiber quality, 100BASE-SX has a minimum reach of 300 metres (980\u00a0ft).[13] 100BASE-SX uses the same wavelength as 10BASE-FL, the 10\u00a0Mbit/s version over optical fiber. Unlike 100BASE-FX, this allows 100BASE-SX to be backwards-compatible with 10BASE-FL. Because of the shorter wavelength used (850\u00a0nm) and the shorter distance it can support, 100BASE-SX uses less expensive optical components (LEDs instead of lasers) which makes it an attractive option for those upgrading from 10BASE-FL and those who do not require long distances. 100BASE-BX10 is a version of Fast Ethernet over a single strand of optical fiber (unlike 100BASE-FX, which uses a pair of fibers). Single-mode fiber is used, along with a special optics which splits the signal into transmit and receive wavelengths; the two wavelengths used for transmit and receive are 1310\u00a0nm and 1550\u00a0nm. The transceivers on each side of the fiber are not equal, as the one transmitting \"downstream\" (from the center of the network to the outside) uses the 1550\u00a0nm wavelength, and the one transmitting \"upstream\" uses the 1310\u00a0nm wavelength.[14] Transceivers for longer distances of 20 or 40\u00a0km are also available. 100BASE-LX10 is a version of Fast Ethernet over two single-mode optical fibers. It has a nominal reach of 10\u00a0km and a nominal wavelength of 1310\u00a0nm.[14] This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "100B-TX": {"link": "https://en.wikipedia.org/wiki/100BASE-TX", "full_form": "100BASE-TX", "content": "In computer networking, Fast Ethernet is a collective term for a number of Ethernet standards that carry traffic at the nominal rate of 100\u00a0Mbit/s (the earlier Ethernet speed was 10\u00a0Mbit/s). Of the Fast Ethernet standards, 100BASE-TX is by far the most common. Fast Ethernet was introduced in 1995 as the IEEE 802.3u standard[1] and remained the fastest version of Ethernet for three years before the introduction of Gigabit Ethernet.[2] The acronym GE/FE is sometimes used for devices supporting both standards.[3]   Fast Ethernet is an extension of the 10-megabit Ethernet standard. It runs on UTP data or optical fiber cable in a star wired bus topology, similar to 10BASE-T where all cables are attached to a hub. Fast Ethernet devices are generally backward compatible with existing 10BASE-T systems, enabling plug-and-play upgrades from 10BASE-T. Fast Ethernet is sometimes referred to as 100BASE-X, where \"X\" is a placeholder for the FX and TX variants.[4] The standard specifies the use of CSMA/CD for media access control. A full-duplex mode is also specified and in practice all modern networks use Ethernet switches and operate in full-duplex mode. The \"100\" in the media type designation refers to the transmission speed of 100\u00a0Mbit/s, while the \"BASE\" refers to baseband signalling. The letter following the dash (\"T\" or \"F\") refers to the physical medium that carries the signal (twisted pair or fiber, respectively), while the last character (\"X\", \"4\", etc.) refers to the used encoding method. A Fast Ethernet adapter can be logically divided into a media access controller (MAC), which deals with the higher-level issues of medium availability, and a Physical Layer Interface (PHY). The MAC may be linked to the PHY by a four-bit 25\u00a0MHz synchronous parallel interface known as a media-independent interface (MII), or by a two-bit 50\u00a0MHz variant called reduced media independent interface (RMII). In rare cases the MII may be an external connection but is usually a connection between ICs in a network adapter or even within a single IC. The specs are written based on the assumption that the interface between MAC and PHY will be a MII but they do not require it. Repeaters (hubs) may use the MII to connect to multiple PHYs for their different interfaces. The MII fixes the theoretical maximum data bit rate for all versions of Fast Ethernet to 100\u00a0Mbit/s. The data signaling rate actually observed on real networks is less than the theoretical maximum, due to the necessary header and trailer (addressing and error-detection bits) on every frame, the occasional \"lost frame\" due to noise, and time waiting after each sent frame for other devices on the network to finish transmitting. 100BASE-T is any of several Fast Ethernet standards for twisted pair cables, including: 100BASE-TX (100\u00a0Mbit/s over two-pair Cat5 or better cable), 100BASE-T4 (100 Mbit/s over four-pair Cat3 or better cable, defunct), 100BASE-T2 (100\u00a0Mbit/s over two-pair Cat3 or better cable, also defunct). The segment length for a 100BASE-T cable is limited to 100 metres (328\u00a0ft) (as with 10BASE-T and gigabit Ethernet). All are or were standards under IEEE 802.3 (approved 1995). Almost all 100BASE-T installations are 100BASE-TX. In the early days of Fast Ethernet, much vendor advertising centered on claims by competing standards that said vendors' standards will work better with existing cables than other standards. In practice, it was quickly discovered that few existing networks actually met the assumed standards, because 10 Megabit Ethernet was very tolerant of minor deviations from specified electrical characteristics and few installers ever bothered to make exact measurements of cable and connection quality; if Ethernet worked over a cable, no matter how well it worked, it was deemed acceptable. Thus most networks had to be rewired for 100 Megabit speed whether or not there had supposedly been CAT3 or CAT5 cable runs.[citation needed] 100BASE-TX is the predominant form of Fast Ethernet, and runs over two wire-pairs inside a category 5 or above cable. Like 10BASE-T, the active pairs in a standard connection are terminated on pins 1, 2, 3 and 6. Since a typical category 5 cable contains 4 pairs, it can support two 100BASE-TX links with a wiring adaptor.[5] Cabling is conventional wired to TIA/EIA-568-B's termination standards, T568A or T568B. This places the active pairs on the orange and green pairs (canonical second and third pairs). Each network segment can have a maximum cabling distance of 100 metres (328\u00a0ft). In its typical configuration, 100BASE-TX uses one pair of twisted wires in each direction, providing 100\u00a0Mbit/s of throughput in each direction (full-duplex). See IEEE 802.3 for more details. The configuration of 100BASE-TX networks is very similar to 10BASE-T. When used to build a local area network, the devices on the network (computers, printers etc.) are typically connected to a hub or switch, creating a star network. Alternatively it is possible to connect two devices directly using a crossover cable. With 100BASE-TX hardware, the raw bits (4 bits wide clocked at 25\u00a0MHz at the MII) go through 4B5B binary encoding to generate a series of 0 and 1 symbols clocked at 125\u00a0MHz symbol rate. The 4B5B encoding provides DC equalization and spectrum shaping (see the standard for details). Just as in the 100BASE-FX case, the bits are then transferred to the physical medium attachment layer using NRZI encoding. However, 100BASE-TX introduces an additional, medium dependent sublayer, which employs MLT-3 as a final encoding of the data stream before transmission, resulting in a maximum \"fundamental frequency\" of 31.25\u00a0MHz. The procedure is borrowed from the ANSI X3.263 FDDI specifications, with minor discrepancies.[6] 100BASE-T4 was an early implementation of Fast Ethernet. It requires four twisted copper pairs, but those pairs were only required to be category 3 rather than the category 5 required by TX. One pair is reserved for transmit, one for receive, and the remaining two will switch direction as negotiated. A very unusual 8B6T code is used to convert 8 data bits into 6 base-3 digits (the signal shaping is possible as there are nearly three times as many 6-digit base-3 numbers as there are 8-digit base-2 numbers). The two resulting 3-digit base-3 symbols are sent in parallel over 3 pairs using 3-level pulse-amplitude modulation (PAM-3). The fact that 3 pairs are used to transmit in each direction makes 100BASE-T4 inherently half-duplex. This standard can be implemented with CAT 3, 4, 5 UTP cables, or STP if needed against interference. Maximum distance is limited to 100 meters. 100BASE-T4 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T2, standardized in IEEE 802.3y, the data is transmitted over two copper pairs, 4 bits per symbol, but these pairs are only required to be category 3 rather than the category 5 required by TX. It uses these two pairs for simultaneously transmitting and receiving on both pairs[8] thus allowing full-duplex operation. First, a 4-bit symbol is expanded into two 3-bit symbols through a non-trivial scrambling procedure based on a linear feedback shift register; see the standard for details. This is needed to flatten the bandwidth and emission spectrum of the signal, as well as to match transmission line properties. The mapping of the original bits to the symbol codes is not constant in time and has a fairly large period (appearing as a pseudo-random sequence). The final mapping from symbols to PAM-5 line modulation levels obeys the table on the right. 100BASE-T2 was not widely adopted but the technology developed for it is used in 1000BASE-T.[7] In 100BASE-T1, standardized in IEEE 802.3bw-2015 Clause 96, the data is transmitted over a single copper pair, 3 bits per symbol (PAM3). It supports only full-duplex, transmitting in both directions simultaneously. The twisted-pair cable is required to support 66\u00a0MHz, with a maximum length of 15\u00a0m. No specific connector is defined. The standard is intended for automotive applications or when Fast Ethernet is to be integrated into another application. It has been developed as BroadR-Reach before IEEE standardization.[9] Proposed and marketed by Hewlett Packard, 100BaseVG was an alternative design using category 3 cabling and a token concept instead of CSMA/CD. It was slated for standardization as IEEE 802.12 but it quickly vanished when switched 100BASE-TX became popular. 100BASE-FX is a version of Fast Ethernet over optical fiber. It uses a 1300 nm near-infrared (NIR) light wavelength transmitted via two strands of optical fiber, one for receive (RX) and the other for transmit (TX). Maximum length is 412 metres (1,350\u00a0ft)[citation needed] for half-duplex connections (to ensure collisions are detected), and 2 kilometres (6,600\u00a0ft) for full-duplex over multi-mode optical fiber.[10] 100BASE-FX uses the same 4B5B encoding and NRZI line code that 100BASE-TX does. 100BASE-FX should use SC, ST, LC, MTRJ or MIC connectors with SC being the preferred option.[11] The 100BASE-FX Physical Medium Dependent (PMD) sublayer is defined by FDDI's PMD,[12] so 100BASE-FX is not compatible with 10BASE-FL, the 10\u00a0MBit/s version over optical fiber. 100BASE-SX is a version of Fast Ethernet over optical fiber standardized in TIA/EIA-785-1-2002. It uses two strands of multi-mode optical fiber for receive and transmit. It is a lower cost alternative to using 100BASE-FX, because it uses short wavelength optics which are significantly less expensive than the long wavelength optics used in 100BASE-FX. Depending on fiber quality, 100BASE-SX has a minimum reach of 300 metres (980\u00a0ft).[13] 100BASE-SX uses the same wavelength as 10BASE-FL, the 10\u00a0Mbit/s version over optical fiber. Unlike 100BASE-FX, this allows 100BASE-SX to be backwards-compatible with 10BASE-FL. Because of the shorter wavelength used (850\u00a0nm) and the shorter distance it can support, 100BASE-SX uses less expensive optical components (LEDs instead of lasers) which makes it an attractive option for those upgrading from 10BASE-FL and those who do not require long distances. 100BASE-BX10 is a version of Fast Ethernet over a single strand of optical fiber (unlike 100BASE-FX, which uses a pair of fibers). Single-mode fiber is used, along with a special optics which splits the signal into transmit and receive wavelengths; the two wavelengths used for transmit and receive are 1310\u00a0nm and 1550\u00a0nm. The transceivers on each side of the fiber are not equal, as the one transmitting \"downstream\" (from the center of the network to the outside) uses the 1550\u00a0nm wavelength, and the one transmitting \"upstream\" uses the 1310\u00a0nm wavelength.[14] Transceivers for longer distances of 20 or 40\u00a0km are also available. 100BASE-LX10 is a version of Fast Ethernet over two single-mode optical fibers. It has a nominal reach of 10\u00a0km and a nominal wavelength of 1310\u00a0nm.[14] This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "100BVG": {"link": "https://en.wikipedia.org/wiki/100BaseVG", "full_form": "100BASE-VG", "content": "100BaseVG is a 100 Mbit/s Ethernet standard specified to run over four pairs of category 3 UTP wires (known as voice grade, hence the \"VG\"). It is also called 100VG-AnyLAN because it was defined to carry both Ethernet and token ring frame types. 100BaseVG was originally proposed by Hewlett-Packard, ratified by the ISO in 1995 and was practically extinct by 1998.   100BaseVG started in the IEEE 802.3 committee as Fast Ethernet. One faction wanted to keep CSMA/CD in order to keep it pure Ethernet, even though the collision domain problem limited the distances to one tenth that of 10BASE-T. Another faction wanted to change to a polling architecture from the hub (they called it \"Demand Priority Protocol\") in order to maintain the 10BASE-T distances, and also to make it a deterministic protocol. The first faction argued that, since IEEE 802.3 was the Ethernet committee, it was not the place to develop a different protocol. Thus, the IEEE 802.12 committee was formed and standardized 100BaseVG. The physical layer requires four twisted pairs of \"voice-grade\" cabling for a link, so category 3 cables or better can be used. While control signaling uses two pairs for each direction simultaneously, all four pairs are switched to a single direction during data transmission, as required and defined during control signaling. This makes 100BaseVG an inherently half-duplex medium like e.g. 10BASE5 (yet faster) but without the CSMA/CD drawbacks. 100BaseVG also supports full-duplex operation over optical fiber or over two pairs of shielded twisted pair. Instead of following the Fast Ethernet standard for twisted pair cabling by using only 2 pairs of wires, 100VG-AnyLAN used all four pairs in either Category 3 or Category 5 twisted pair cable. The design goals were to avoid the radio frequency radiation emitted at the higher frequencies required by Fast Ethernet and to leverage existing wiring installations of Category 3 cabling that most organizations had recently installed to support 10 megabit twisted-pair Ethernet. This had the additional advantage of being less susceptible to external sources of RF interference such as other network cables, fluorescent lights, and high power lines. They multiplexed the signal across all 8 wires thereby lowering the frequency and making it more robust. This presented a problem with early installations that borrowed one unused twisted pair for telephone traffic but those installations were uncommon.[citation needed] When Ethernet became Fast Ethernet, it continued to use the Carrier Sense Multiple Access With Collision Detection (CSMA/CD) mechanism to manage traffic on the network cable. 100VG took advantage of the token passing concept that made ARCNET and Token Ring popular in order to provide consistent performance no matter how large the network became. It removed the token passing responsibility from the wiring and network nodes and placed it internal to the 100VG-AnyLAN hubs. These hubs contained the rotating token that never left the hub itself. When a node wanted to transmit data, it would raise a bit on its hub port connection that indicating to the hub that it was ready. As the token passed by a ready hub port, it would then open up traffic to that node. Because the token stayed within the hub, it did not have to traverse long cables going to every node as in ARCNET and Token Ring therefore becoming faster than those other deterministic networking standards and being less susceptible to cabling problems, network card failures, and line interference. Real-life load testing showed 100VG-AnyLAN reaching 95% of its theoretical network speed instead of about 45% as in Fast Ethernet when using hubs. Fast Ethernet switches were not commonplace at first because of high cost and limited availability so, initially, 100VG had a significant performance advantage. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "286": {"link": "https://en.wikipedia.org/wiki/Intel_80286", "full_form": "Intel 80286 processor", "content": "The Intel 80286[1] (also marketed as the iAPX 286[2] and often called Intel 286) is a 16-bit microprocessor that was introduced on 1 February 1982. It was the first 8086 based CPU with separate, non-multiplexed, address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186,[3] it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.[4] The 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.   Intel's first 80286 chips were specified for a maximum clockrate of 4, 6 or 8\u00a0MHz and later releases for 12.5\u00a0MHz. AMD and Harris later produced 16\u00a0MHz, 20\u00a0MHz and 25\u00a0MHz parts, respectively. Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery powered devices. On average, the 80286 was reportedly measured to have a speed of about 0.21 instructions per clock on \"typical\" programs,[5] although it could be significantly faster on optimized code and in tight loops, as many instructions could execute in 2 clock cycles each. The 6\u00a0MHz, 10\u00a0MHz and 12\u00a0MHz models were reportedly measured to operate at 0.9\u00a0MIPS, 1.5\u00a0MIPS and 2.66\u00a0MIPS respectively.[6] The later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones).[7] The 80286 was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: address unit, bus unit, instruction unit and execution unit organized into a loosely coupled (buffered) pipeline just as in the 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address calculation hardware (most importantly a dedicated adder) and a faster (more hardware based) multiplier.[8] It was produced in a 68-pin package including PLCC (Plastic Leaded Chip Carrier), LCC (Leadless chip carrier) and PGA (Pin Grid Array) packages.[9] The performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e. a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286 while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.[8] The 80286 included, in addition to all of the 8086 instructions, all of the new instructions of the 80186: ENTER, LEAVE, BOUND, INS, OUTS, PUSHA, POPA, PUSH immediate, IMUL immediate, and immediate shifts and rotates. The 80286 also added new instructions for protected mode: ARPL, CLTS, LAR, LGDT, LIDT, LLDT, LMSW, LSL, LTR, SGDT, SIDT, SLDT, SMSW, STR, VERR, and VERW. Some of the instructions for protected mode can (or must) be used in real mode to set up and switch to protected mode, and a few (such as SMSW and LMSW) are useful for real mode itself. The Intel 80286 had a 24-bit address bus and was able to address up to 16\u00a0MB of RAM, compared to the 1\u00a0MB addressability of its predecessor. However, memory cost and the initial rarity of software using the memory above 1\u00a0MB meant that 80286 computers were rarely shipped with more than one megabyte of RAM.[8] Additionally, there was a performance penalty involved in accessing extended memory from real mode (in which DOS, the dominant PC operating system until the mid-1990s, ran), as noted below. The 286 was the first of the x86 CPU family to support Protected virtual address mode, commonly called \"protected mode\". In addition, it was the first commercially available microprocessor with on-chip MMU capabilities. (Systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller.) This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market. Several additional instructions were introduced in protected mode of 80286, which are helpful for multitasking operating systems. Another important feature of 80286 is Prevention of Unauthorized Access. This is achieved by: In 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers: By design, the 286 could not revert from protected mode to the basic 8086-compatible Real address mode (\"real mode\") without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry as well as specialized code in the ROM BIOS and the 8042 peripheral microcontroller to enable software to cause the reset, allowing real-mode reentry while retaining active memory and returning control to the program that initiated the reset. (The BIOS is necessarily involved because it obtains control directly whenever the CPU resets.) Though it worked correctly, the method imposed a huge performance penalty. In theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules (newly proposed with the introduction of the 80286) were followed; however, as many DOS programs did not conform to those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily and to provide an emulation of real mode within protected mode. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode; essentially, in protected mode the 80286 was designed to be a new processor with many similarities to its predecessors, while real mode on the 80286 was offered for smaller-scale systems that could benefit from a more advanced version of the 80186 CPU core, with advantages such as higher clock rates, faster instruction execution (measured in clock cycles), and unmultiplexed buses, but not the 24-bit (16 MB) memory space. To support protected mode, new instructions have been added: ARPL, VERR, VERW, LAR, LSL, SMSW, SGDT, SIDT, SLDT, STR, LMSW, LGDT, LIDT, LLDT, LTR, CLTS. There are also new exceptions (internal interrupts): Invalid opcode, Coprocessor not available, Double Fault, Coprocessor segment overrun, Stack fault, Segment overrun/General protection fault, and others only for protected mode. The protected mode of the 80286 was not utilized until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A DOS could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory.[8] The difficulty lay in the incompatibility of older real mode DOS programs with protected mode. They simply could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs typically would directly access data and code segments that did not belong to them, as real mode allowed them to do without restriction; in contrast, the design intent of protected mode was to prevent programs from accessing any segments other than their own unless special access was explicitly allowed. While it was possible to set up a protected mode environment that allowed all programs access to all segments (by putting all segment descriptors into the GDT and assigning them all the same privilege level), this undermined nearly all of the advantages of protected mode except the extended (24-bit) address space. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly (i.e., ugly from an internal technical viewpoint) but would still run a majority of the old programs. Protected mode also did not provide a significant enough performance advantage over the 8086-compatible real mode to justify supporting its capabilities; actually, except for task switches when multitasking, it actually yielded only a performance disadvantage, by slowing down many instructions through a litany of added privilege checks. In protected mode, registers were still 16-bit, and the programmer was still forced to use a memory map composed of 64k segments, just like in real mode.[10] In January 1985, Digital Research previewed the Concurrent DOS 286 operating system developed in cooperation with Intel. The product would function strictly as an 80286 native mode (i.e. protected mode) operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation.[11] This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip.[11] In August, after extensive testing on E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, in the E-2 step, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster.[7] Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and Point-of-Sale terminals in 1986.[12] Digital Research's FlexOS 286 version 1.0, a derivation of Concurrent DOS 286, was developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS, but the same limitations affected it. The problems led to Bill Gates famously referring to the 80286 as a \"brain dead chip\",[13][when?] since it was clear that the new Microsoft Windows environment would not be able to run multiple MS-DOS applications with the 286. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode). Other operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984),[14] Coherent,[15] and Minix.[16] These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS applications or other real-mode programs. In its successor 80386 chip, Intel enhanced the protected mode to address more memory and also added the separate virtual 8086 mode, a mode within protected mode which has much better MS-DOS compatibility, in order to satisfy the diverging needs of the market.[17]"}, "2B1Q": {"link": "https://en.wikipedia.org/wiki/2B1Q", "full_form": "2 Binary 1 Quaternary", "content": "Two-binary, one-quaternary (2B1Q) is a line code used in the U interface of the Integrated Services Digital Network (ISDN) Basic Rate Interface (BRI) and the high-bit-rate digital subscriber line (HDSL).[1] 2B1Q is a four-level pulse amplitude modulation (PAM-4) scheme without redundancy, mapping two bits (2B) into one quaternary symbol (1Q). A competing encoding technique in the ISDN basic rate U interface, mainly used in Europe, is 4B3T. To minimize error propagation, bit pairs (dibits) are assigned to voltage levels according to a Gray code, as follows: If the voltage is misread as an adjacent level, this causes only a 1-bit error in the decoded data. 2B1Q code is not DC-balanced. Symbol rate is half of data rate. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later. "}, "2FA": {"link": "https://en.wikipedia.org/wiki/Multi-factor_authentication", "full_form": "Two-factor authentication", "content": "Multi-factor authentication (MFA) is a method of computer access control in which a user is granted access only after successfully presenting several separate pieces of evidence to an authentication mechanism \u2013 typically at least two of the following categories: knowledge (something they know), possession (something they have), and inherence (something they are).[1][2] Two-factor authentication (also known as 2FA) is a method of confirming a user's claimed identity by utilizing a combination of two different components. Two-factor authentication is a type of multi-factor authentication. A good example from everyday life is the withdrawing of money from a cash machine; only the correct combination of a bank card (something that the user possesses) and a PIN (personal identification number, something that the user knows) allows the transaction to be carried out.   The use of multiple authentication factors to prove one's identity is based on the premise that an unauthorized actor is unlikely to be able to supply the factors required for access. If, in an authentication attempt, at least one of the components is missing or supplied incorrectly, the user's identity is not established with sufficient certainty and access to the asset (e.g., a building, or data) being protected by multi-factor authentication then remains blocked. The authentication factors of a multi-factor authentication scheme may include: Knowledge factors are the most commonly used form of authentication. In this form, the user is required to prove knowledge of a secret in order to authenticate. A password is a secret word or string of characters that is used for user authentication. This is the most commonly used mechanism of authentication. Many multi-factor authentication techniques rely on password as one factor of authentication.[4] Variations include both longer ones formed from multiple words (a passphrase) and the shorter, purely numeric, personal identification number (PIN) commonly used for ATM access. Traditionally, passwords are expected to be memorized. Many secret questions such as \"Where were you born?\" are poor examples of a knowledge factor because they may be known to a wide group of people, or be able to be researched. Possession factors (\"something only the user has\") have been used for authentication for centuries, in the form of a key to a lock. The basic principle is that the key embodies a secret which is shared between the lock and the key, and the same principle underlies possession factor authentication in computer systems. A security token is an example of a possession factor. Disconnected tokens have no connections to the client computer. They typically use a built-in screen to display the generated authentication data, which is manually typed in by the user.[5] Connected tokens are devices that are physically connected to the computer to be used, and transmit data automatically.[6] There are a number of different types, including card readers, wireless tags and USB tokens.[6] These are factors associated with the user, and are usually bio-metric methods, including fingerprint readers, retina scanners or voice recognition. The major drawback of authentication performed including something that the user possesses is that the physical token (the USB stick, the bank card, the key or similar) must be carried around by the user, practically at all times. Loss and theft are a risk. Many organisations forbid USB and electronic devices being carried in or out owing to malware and data theft risks, and most important machines do not have USB ports for the same reason. Physical tokens do not scale, typically requiring a new token for each new account and system. There are also costs involved in procuring and subsequently replacing tokens of this kind. In addition, there are inherent conflicts and unavoidable trade-offs[7] between usability and security. Mobile phone two-factor authentication, where devices such as mobile phones and smartphones serve as \"something that the user possesses\", was developed to provide an alternative method that would avoid such issues. To authenticate themselves, people can use their personal access license (i.e. something that only the individual user knows) plus a one-time-valid, dynamic passcode consisting of digits. The code can be sent to their mobile device by SMS or via a special app. The advantage of this method is that there is no need for an additional, dedicated token, as users tend to carry their mobile devices around at all times anyway. Some professional two-factor authentication solutions also ensure that there is always a valid passcode available for users. If one has already used a sequence of digits (passcode), this is automatically deleted and the system sends a new code to the mobile device. And if the new code is not entered within a specified time limit, the system automatically replaces it. This ensures that no old, already used codes are left on mobile devices. For added security, it is possible to specify how many incorrect entries are permitted before the system blocks access.[8] Security of the mobile-delivered security tokens fully depends on the mobile operator's operational security and can be easily breached by wiretapping or SIM cloning by national security agencies.[9] Advantages of mobile phone two-factor authentication Disadvantages of mobile phone two-factor authentication Advances in research of two-factor authentication for mobile devices consider different methods in which a second factor can be implemented while not posing a hindrance to the user. With the continued use and improvements in the accuracy of mobile hardware such as GPS,[14] microphone,[15] and gyro/acceleromoter,[16] the ability to use them as a second factor of authentication is becoming more trustworthy. For example, by recording the ambient noise of the user\u2019s location from a mobile device and comparing it with the recording of the ambient noise from the computer in the same room on which the user is trying to authenticate, one is able to have an effective second factor of authentication.[17] This also reduces the amount of time and effort needed to complete the process. Details for authentication in the USA are defined with the Homeland Security Presidential Directive 12 (HSPD-12).[18] Existing authentication methodologies involve the explained three types of basic \"factors\". Authentication methods that depend on more than one factor are more difficult to compromise than single-factor methods.[citation needed][19] IT regulatory standards for access to Federal Government systems require the use of multi-factor authentication to access sensitive IT resources, for example when logging on to network devices to perform administrative tasks[20] and when accessing any computer using a privileged login.[21] NIST Special Publication 800-63-2 discusses various forms of two-factor authentication and provides guidance on using them in business processes requiring different levels of assurance.[22] In 2005, the United States' Federal Financial Institutions Examination Council issued guidance for financial institutions recommending financial institutions conduct risk-based assessments, evaluate customer awareness programs, and develop security measures to reliably authenticate customers remotely accessing online financial services, officially recommending the use of authentication methods that depend on more than one factor (specifically, what a user knows, has, and is) to determine the user's identity.[23] In response to the publication, numerous authentication vendors began improperly promoting challenge-questions, secret images, and other knowledge-based methods as \"multi-factor\" authentication. Due to the resulting confusion and widespread adoption of such methods, on August 15, 2006, the FFIEC published supplemental guidelines\u2014which states that by definition, a \"true\" multi-factor authentication system must use distinct instances of the three factors of authentication it had defined, and not just use multiple instances of a single factor.[24] According to proponents, multi-factor authentication could drastically reduce the incidence of online identity theft and other online fraud, because the victim's password would no longer be enough to give a thief permanent access to their information.[citation needed] However, many multi-factor authentication approaches remain vulnerable to phishing,[25] man-in-the-browser, and man-in-the-middle attacks.[26] Multi-factor authentication may be ineffective against modern threats, like ATM skimming, phishing, and malware.[27] In May 2017 O2 Telef\u00f3nica, a German mobile service provider, confirmed that cybercriminals had exploited SS7 vulnerabilities to bypass two-factor authetication (2FA) to do unauthorized withdrawals from users bank accounts. The criminals first infected the account holder's computers in an attempt to steal their bank account credentials and phone numbers. Then the attackers purchased access to a fake telecom provider and set-up a redirect for the victim's phone number to a handset controlled by them. Finally the attackers logged into victims' online bank accounts and requested for the money on the accounts to be withdrawn to accounts owned by the criminals. 2FA confirmation codes were routed to phone numbers controlled by the attackers and the criminals transferred the money out.[28] The Payment Card Industry (PCI) Data Security Standard, requirement 8.3, requires the use of MFA for all remote network access that originates from outside the network to a Card Data Environment (CDE).[29] Beginning with PCI-DSS version 3.2, the use of MFA is required for all administrative access to the CDE, even if the user is within a trusted network.[30] Many multi-factor authentication products require users to deploy client software to make multi-factor authentication systems work. Some vendors have created separate installation packages for network login, Web access credentials and VPN connection credentials. For such products, there may be four or five different software packages to push down to the client PC in order to make use of the token or smart card. This translates to four or five packages on which version control has to be performed, and four or five packages to check for conflicts with business applications. If access can be operated using web pages, it is possible to limit the overheads outlined above to a single application. With other multi-factor authentication solutions, such as \"virtual\" tokens and some hardware token products, no software must be installed by end users. There are drawbacks to multi-factor authentication that are keeping many approaches from becoming widespread. Some consumers have difficulty keeping track of a hardware token or USB plug. Many consumers do not have the technical skills needed to install a client-side software certificate by themselves. Generally, multi-factor solutions require additional investment for implementation and costs for maintenance. Most hardware token-based systems are proprietary and some vendors charge an annual fee per user. Deployment of hardware tokens is logistically challenging. Hardware tokens may get damaged or lost and issuance of tokens in large industries such as banking or even within large enterprises needs to be managed. In addition to deployment costs, multi-factor authentication often carries significant additional support costs. A 2008 survey[31] of over 120 U.S. credit unions by the Credit Union Journal reported on the support costs associated with two-factor authentication. In their report, software certificates and software toolbar approaches were reported to have the highest support costs. Several popular web services employ multi-factor authentication, usually as an optional feature that is deactivated by default.[32]"}, "2GL": {"link": "https://en.wikipedia.org/wiki/Second-generation_programming_language", "full_form": "Second-Generation Programming Language", "content": "Second-generation programming language (2GL) is a generational way to categorize assembly languages.[1] The term was coined to provide a distinction from higher level third-generation programming languages (3GL) such as COBOL and earlier first-generation programming language (machine code languages). Second-generation programming languages have the following properties: Second-generation languages are sometimes used in kernels and device drivers (though C is generally employed for this in modern kernels), but more often find use in extremely intensive processing such as games, video editing, graphic manipulation/rendering. One method for creating such code is by allowing a compiler to generate a machine-optimized assembly language version of a particular function. This code is then hand-tuned, gaining both the brute-force insight of the machine optimizing algorithm and the intuitive abilities of the human optimizer."}, "2NF": {"link": "https://en.wikipedia.org/wiki/2NF", "full_form": "Second Normal Form", "content": "Second normal form (2NF) is a normal form used in database normalization. 2NF was originally defined by E.F. Codd in 1971.[1] A relation that is in first normal form (1NF) must meet additional criteria if it is to qualify for second normal form. Specifically: a relation is in 2NF if it is in 1NF and no non-prime attribute is dependent on any proper subset of any candidate key of the relation. A non-prime attribute of a relation is an attribute that is not a part of any candidate key of the relation. Put simply, a relation is in 2NF if it is in 1NF and every non-prime attribute of the relation is dependent on the whole of every candidate key.   A functional dependency on part of any candidate key is a violation of 2NF. In addition to the primary key, the relation may contain other candidate keys; it is necessary to establish that no non-prime attributes have part-key dependencies on any of these candidate keys. Multiple candidate keys occur in the following relation: Even if the designer has specified the primary key as {Model Full Name}, the relation is not in 2NF because of the other candidate keys. {Manufacturer, Model} is also a candidate key, and Manufacturer Country is dependent on a proper subset of it: Manufacturer. To make the design conform to 2NF, it is necessary to have two relations:"}, "3GL": {"link": "https://en.wikipedia.org/wiki/Third-generation_programming_language", "full_form": "Third-Generation Programming Language", "content": "A third-generation programming language (3GL) is a generational way to categorize high-level computer programming languages.[1]   Assembly languages are categorized as second-generation programming languages, and are machine-dependent. 3GLs are much more machine independent and more programmer-friendly. This includes features like improved support for aggregate data types, and expressing concepts in a way that favors the programmer, not the computer. A third generation language improves over a second generation language by having the computer take care of non-essential details. 3GLs feature more abstraction than previous generations of languages, and thus can be considered higher level languages than their first and second generation counterparts. Fortran, ALGOL, and COBOL are examples of third generation language Most popular general-purpose languages today, such as C, C++, C#, Java, BASIC and Pascal, are also third-generation languages, although each of these languages can be further subdivided into other categories based on other contemporary traits. Most 3GLs support structured programming. A programming language such as C, FORTRAN, or Pascal enables a programmer to write programs that are more or less independent from a particular type of computer. Such languages are considered high-level because they are closer to human languages and further from machine languages. In contrast, assembly languages are considered as low-level because they are very close to machine languages. The main advantage of high-level languages over low-level languages is that they are easier to read, write, and maintain. Ultimately, programs written in a high-level language must be translated into machine language by a compiler or interpreter. These programs could run on different machines so they were machine-independent. As new, more abstract languages have been developed, however, the concept of high and low level languages have become rather relative. Many of the early \"high level\" languages are now considered relatively low level in comparison to languages such as Python, Ruby, and Common Lisp."}, "3GPP": {"link": "https://en.wikipedia.org/wiki/3GPP", "full_form": "3rd Generation Partnership Project", "content": "The 3rd Generation Partnership Project (3GPP) is a collaboration between groups of telecommunications associations, known as the Organizational Partners. The initial scope of 3GPP was to make a globally applicable third-generation (3G) mobile phone[1] system specification based on evolved Global System for Mobile Communications (GSM) specifications within the scope of the International Mobile Telecommunications-2000 project of the International Telecommunication Union (ITU). The scope was later enlarged to include the development and maintenance of:[2] 3GPP standardization encompasses Radio Access Network, Services and Systems Aspects, and Core Network and Terminals.[3] The project was established in December 1998 and should not be confused with 3rd Generation Partnership Project 2 (3GPP2), which specifies standards for another 3G technology based on IS-95 (CDMA), commonly known as CDMA2000.[4] The 3GPP support team (also known as the \"Mobile Competence Centre\") is located at the European Telecommunications Standards Institute (ETSI) headquarters in the Sophia Antipolis technology park in France.[5]   The 3rd Generation Partnership Project initiative eventually arose from a strategic initiative between Nortel Networks and AT&T Wireless. In 1998 AT&T Wireless was operating an IS-136 (TDMA) wireless network in the United States. In 1997 Nortel Networks' Wireless R&D center in Richardson, Texas, the wireless division of Bell Northern Research had developed a vision for \"an all Internet Protocol (IP)\" wireless network that went under the internal name \"Cell Web\". As the concept progressed, Nortel launched the industry vision as \"Wireless Internet\". AT&T Wireless, poised to evolve its network in the United States, took a strong interest in Wireless Internet and its promise of Internet Protocol (with Nortel Networks as the potential supplier). Within 12 months or so, AT&T launched a global initiative that they named \"3GIP\", a third generation wireless standard that was \"natively\" Internet Protocol based.[6] Initially, principal participants included British Telecom, France Telecom, Telecom Italia, and Nortel Networks, but were eventually joined by NTT DoCoMo, BellSouth, Telenor, and Lucent, Ericsson, Motorola, Nokia, and others.[7] A 3GIP standards forum was instituted and standards began to be developed. The forum progressed into the 2000 time frame, up until AT&T Wireless and British Telecom formed a strategic \"partnership project\" to facilitate \"global roaming\" between U.S. and European markets. With this business arrangement, GSM, the prevailing European standard was adopted as the basis of AT&T Wireless' network evolution for North America. Very specifically, this included the deployment of GSM data capabilities, i.e. GPRS, EDGE, and its evolution to UMTS. The seven 3GPP Organizational Partners are from Asia, Europe and North America. Their aim is to determine the general policy and strategy of 3GPP and perform the following tasks: Together with the Market Representation Partners (MRPs) perform the following tasks: The Organizational Partners are: The 3GPP Organizational Partners can invite a Market Representation Partner to take part in 3GPP, which: As of June 2017, the Market Representation Partners are: 3GPP standards are structured as Releases. Discussion of 3GPP thus frequently refers to the functionality in one release or another. Each release incorporates hundreds of individual standards documents, each of which may have been through many revisions. Current 3GPP standards incorporate the latest revision of the GSM standards. The documents are available freely on 3GPP's Web site. While 3GPP standards can be bewildering to the newcomer, they are remarkably complete and detailed, and provide insight into how the cellular industry works. They cover not only the radio part (\"Air Interface\") and Core Network, but also billing information and speech coding down to source code level. Cryptographic aspects (authentication, confidentiality) are also specified in detail. 3GPP2 offers similar information about its system. The 3GPP specification work is done in Technical Specification Groups (TSGs) and Working Groups (WGs).[18] There are three Technical Specifications Groups, each of which consists of multiple WGs: The closure of GERAN was announced in January 2016.[19] The specification work on legacy GSM/EDGE system was transferred to a new RAN WG, RAN6. The 3GPP structure also includes a Project Coordination Group, which is the highest decision-making body. Its missions include the management of overall timeframe and work progress. 3GPP standardization work is contribution-driven. Companies (\"individual members\") participate through their membership to a 3GPP Organizational Partner. As of April 2011, 3GPP is composed of more than 370 individual members.[20] Specification work is done at WG and at TSG level:[21] 3GPP follows a three-stage methodology as defined in ITU-T Recommendation I.130:[22] Test specifications are sometimes defined as stage 4, as they follow stage 3. Specifications are grouped into releases. A release consists of a set of internally consistent set of features and specifications. Timeframes are defined for each release by specifying freezing dates. Once a release is frozen, only essential corrections are allowed (i.e. addition and modifications of functions are forbidden). Freezing dates are defined for each stage. The 3GPP specifications are transposed into deliverables by the Organizational Partners. 3GPP systems are deployed across much of the established GSM market.[23][24] They are primarily Release 6 systems, but as of 2010, growing interest in HSPA+ and LTE is driving adoption of Release 7 and its successors. Since 2005, 3GPP systems were seeing deployment in the same markets as 3GPP2 systems (for example, North America[25]). With LTE the official successor to 3GPP2's CDMA systems, 3GPP-based systems will eventually become the single global mobile standard.[citation needed]"}, "3GPP2": {"link": "https://en.wikipedia.org/wiki/3GPP2", "full_form": "3rd Generation Partnership Project 2", "content": "The 3rd Generation Partnership Project 2 (3GPP2) is a collaboration between telecommunications associations to make a globally applicable third generation (3G) mobile phone system specification within the scope of the ITU's IMT-2000 project. In practice, 3GPP2 is the standardization group for CDMA2000, the set of 3G standards based on the earlier cdmaOne 2G CDMA technology. The participating associations are ARIB/TTC (Japan), China Communications Standards Association, Telecommunications Industry Association (North America) and Telecommunications Technology Association (South Korea). The agreement was established in December 1998. Ultra Mobile Broadband (UMB) was a 3GPP2 project to develop a fourth-generation successor to CDMA2000. In November 2008, Qualcomm, UMB's lead sponsor, announced it was ending development of the technology, favoring LTE instead.[1] 3GPP2 should not be confused with 3GPP; 3GPP is the standard body behind the Universal Mobile Telecommunications System (UMTS) that is the 3G upgrade to GSM networks, while 3GPP2 is the standard body behind the competing 3G standard CDMA2000 that is the 3G upgrade to cdmaOne networks used mostly in the United States (and to some extent also in Japan, China, Canada, South Korea and India). GSM/GPRS/EDGE/W-CDMA is the most widespread wireless standard in the world. A few countries (such as China, the United States, Canada, India, South Korea and Japan) use both sets of standards, but most countries use only the GSM family."}, "3NF": {"link": "https://en.wikipedia.org/wiki/3NF", "full_form": "Third Normal Form", "content": "Third normal form is a normal form that is used in normalizing a database design to reduce the duplication of data and ensure referential integrity by ensuring that (1) the entity is in second normal form, and (2) all the attributes in a table are determined only by the candidate keys of that relation and not by any non-prime attributes. 3NF was designed to improve database processing while minimizing storage costs. 3NF data modeling was ideal for online transaction processing (OLTP) applications with heavy order entry type of needs.[1]   The third normal form (3NF) is a normal form used in database normalization. 3NF was originally defined by E.F. Codd in 1971.[2] Codd's definition states that a table is in 3NF if and only if both of the following conditions hold: A non-prime attribute of R is an attribute that does not belong to any candidate key of R.[3] A transitive dependency is a functional dependency in which X \u2192 Z (X determines Z) indirectly, by virtue of X \u2192 Y and Y \u2192 Z (where it is not the case that Y \u2192 X).[4] A 3NF definition that is equivalent to Codd's, but expressed differently, was given by Carlo Zaniolo in 1982. This definition states that a table is in 3NF if and only if, for each of its functional dependencies X \u2192 A, at least one of the following conditions holds: Zaniolo's definition gives a clear sense of the difference between 3NF and the more stringent Boyce\u2013Codd normal form (BCNF). BCNF simply eliminates the third alternative (\"Every element of A-X, the set difference between A and X, is a prime attribute\"). An approximation of Codd's definition of 3NF, paralleling the traditional pledge to give true evidence in a court of law, was given by Bill Kent: \"[Every] non-key [attribute] must provide a fact about the key, the whole key, and nothing but the key.\"[7] A common variation supplements this definition with the oath: \"so help me Codd\".[8] Requiring existence of \"the key\" ensures that the table is in 1NF; requiring that non-key attributes be dependent on \"the whole key\" ensures 2NF; further requiring that non-key attributes be dependent on \"nothing but the key\" ensures 3NF. While this phrase is a useful mnemonic, the fact that it only mentions a single key means it defines some necessary but not sufficient conditions to satisfy the 2nd and 3rd Normal Forms. Both 2NF and 3NF are concerned equally with all candidate keys of a table and not just any one key. Chris Date refers to Kent's summary as \"an intuitively attractive characterization\" of 3NF, and notes that with slight adaptation it may serve as a definition of the slightly stronger Boyce\u2013Codd normal form: \"Each attribute must represent a fact about the key, the whole key, and nothing but the key.\"[9] The 3NF version of the definition is weaker than Date's BCNF variation, as the former is concerned only with ensuring that non-key attributes are dependent on keys. Prime attributes (which are keys or parts of keys) must not be functionally dependent at all; they each represent a fact about the key in the sense of providing part or all of the key itself. (It should be noted here that this rule applies only to functionally dependent attributes, as applying it to all attributes would implicitly prohibit composite candidate keys, since each part of any such key would violate the \"whole key\" clause.) An example of a 2NF table that fails to meet the requirements of 3NF is: Because each row in the table needs to tell us who won a particular Tournament in a particular Year, the composite key {Tournament, Year} is a minimal set of attributes guaranteed to uniquely identify a row. That is, {Tournament, Year} is a candidate key for the table. The breach of 3NF occurs because the non-prime attribute Winner Date of Birth is transitively dependent on the candidate key {Tournament, Year} via the non-prime attribute Winner. The fact that Winner Date of Birth is functionally dependent on Winner makes the table vulnerable to logical inconsistencies, as there is nothing to stop the same person from being shown with different dates of birth on different records. In order to express the same facts without violating 3NF, it is necessary to split the table into two: Update anomalies cannot occur in these tables, because unlike before, Winner is now a primary key in the second table, thus allowing only one value for Date of Birth for each Winner. The definition of 3NF offered by Carlo Zaniolo in 1982, and given above, is proven in the following way: Let X \u2192 A be a nontrivial FD (i.e. one where X does not contain A) and let A be a non-key attribute. Also let Y be a key of R. Then Y \u2192 X. Most 3NF tables are free of update, insertion, and deletion anomalies. Certain types of 3NF tables, rarely met with in practice, are affected by such anomalies; these are tables which either fall short of Boyce\u2013Codd normal form (BCNF) or, if they meet BCNF, fall short of the higher normal forms 4NF or 5NF. While 3NF was ideal for machine processing, the segmented nature of the data model was difficult to consume by a human user. Analytics via query, reporting, and dashboards required a different type of data model that supported analysis such as trend lines, period-to-date calculations (month-to-date, quarter-to-date, year-to-date), cumulative calculations, basic statistics (average, standard deviation, moving averages) and previous period comparisons (year ago, month ago, week ago) e.g. dimensional modeling and beyond dimensional modeling, flattening of stars via Hadoop and data science.[10][11]"}, "386": {"link": "https://en.wikipedia.org/wiki/Intel_80386", "full_form": "Intel 80386 processor", "content": "The Intel 80386, also known as i386 or just 386, is a 32-bit microprocessor introduced in 1985.[1] The first versions had 275,000 transistors[2] and were the CPU of many workstations and high-end personal computers of the time. As the original implementation of the 32-bit extension of the 80286 architecture,[3] the 80386 instruction set, programming model, and binary encodings are still the common denominator for all 32-bit x86 processors, which is termed the i386-architecture, x86, or IA-32, depending on context. The 32-bit 80386 can correctly execute most code intended for the earlier 16-bit processors such as 8088 and 80286 that were ubiquitous in early PCs. (Following the same tradition, modern 64-bit x86 processors are able to run most programs written for older x86 CPUs, all the way back to the original 16-bit 8086 of 1978.) Over the years, successively newer implementations of the same architecture have become several hundreds of times faster than the original 80386 (and thousands of times faster than the 8086).[4] A 33\u00a0MHz 80386 was reportedly measured to operate at about 11.4 MIPS.[5] The 80386 was introduced in October 1985, while manufacturing of the chips in significant quantities commenced in June 1986.[6][7] Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was rationalized upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq[8] and marked the first time a fundamental component in the IBM PC compatible de facto-standard was updated by a company other than IBM. In May 2006, Intel announced that 80386 production would stop at the end of September 2007.[9] Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an 80386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the 80386 processor, such as BlackBerry 950[10] and Nokia 9000 Communicator.   The processor was a significant evolution in the x86 architecture, and extended a long line of processors that stretched back to the Intel 8008. The predecessor of the 80386 was the Intel 80286, a 16-bit processor with a segment-based memory management and protection system. The 80386 added a 32-bit architecture and a paging translation unit, which made it much easier to implement operating systems that used virtual memory. It also offered support for register debugging. The 80386 featured three operating modes: real mode, protected mode and virtual mode. The protected mode which debuted in the 286 was extended to allow the 386 to address up to 4 GB of memory. The all new virtual 8086 mode (or VM86) made it possible to run one or more real mode programs in a protected environment, although some programs were not compatible. The ability for a 386 to be set up to act like it had a flat memory model in protected mode despite the fact that it uses a segmented memory model in all modes would arguably be the most important feature change for the x86 processor family until AMD released x86-64 in 2003. Several new instructions have been added to 386: BSF, BSR, BT, BTS, BTR, BTC, CDQ, CWDE, LFS, LGS, LSS, MOVSX, MOVZX, SETcc, SHLD, SHRD. Two new segment registers have been added (FS and GS) for general purpose programs, single Machine Status Word of 286 grew into eight control registers CR0-CR7. Debug registers DR0-DR7 were added for hardware breakpoints. New forms of MOV instruction are used to access them. Chief architect in the development of the 80386 was John H. Crawford.[11] He was responsible for extending the 80286 architecture and instruction set to 32-bit, and then led the microprogram development for the 80386 chip. The 80486 and P5 Pentium line of processors were descendants of the 80386 design. The following data types are directly supported and thus implemented by one or more 80386 machine instructions; these data types are described here in brief. (source:,[12] page 514): The following 80386 assembly source code is for a subroutine named _strtolower that copies a null-terminated ASCIIZ character string from one location to another, converting all alphabetic characters to lower case. The string is copied one byte (8-bit character) at a time. The example code uses the EBP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by Algol-like languages since the late 1950s. A flat memory model is assumed, specifically, that the DS and ES segments address the same region of memory. In 1988, Intel introduced the 80386SX, most often referred to as the 386SX, a cut-down version of the 80386 with a 16-bit data bus mainly intended for lower-cost PCs aimed at the home, educational, and small business markets while the 386DX would remain the high-end variant used in workstations, servers, and other demanding tasks. The CPU remained fully 32-bit internally, but the 16-bit bus was intended to simplify circuit board layout and reduce total cost.[13] The 16-bit bus simplified designs but hampered performance. Only 24 pins were connected to the address bus, therefore limiting addressing to 16 MB,[14] but this was not a critical constraint at the time. Performance differences were due not only to differing data bus-widths, but also due to performance-enhancing cache memories often employed on boards using the original chip. The original 80386 was subsequently renamed 80386DX to avoid confusion. However, Intel subsequently used the 'DX' suffix to refer to the floating-point capability of the 80486DX. The 80387SX was an 80387 part that was compatible with the 386SX (i.e. with a 16-bit databus). The 386SX was packaged in a surface-mount QFP, and sometimes offered in a socket to allow for an upgrade. The i386SL was introduced as a power-efficient version for laptop computers. The processor offered several power management options (e.g. SMM), as well as different \"sleep\" modes to conserve battery power. It also contained support for an external cache of 16 to 64 kB. The extra functions and circuit implementation techniques caused this variant to have over 3 times as many transistors as the i386DX. The i386SL was first available at 20\u00a0MHz clock speed,[15] with the 25\u00a0MHz model later added.[16] The first company to design and manufacture a PC based on the Intel 80386 was Compaq. By extending the 16/24-bit IBM PC/AT standard into a natively 32-bit computing environment, Compaq became the first third party to implement a major technical hardware advance on the PC platform. IBM was offered use of the 80386, but had manufacturing rights for the earlier 80286. IBM therefore chose to rely on that processor for a couple more years. The early success of the Compaq 386 PC played an important role in legitimizing the PC \"clone\" industry, and in de-emphasizing IBM's role within it. Prior to the 386, the difficulty of manufacturing microchips and the uncertainty of reliable supply made it desirable that any mass-market semiconductor be multi-sourced, that is, made by two or more manufacturers, the second and subsequent companies manufacturing under license from the originating company. The 386 was for a time (4.7 years) only available from Intel, since Andy Grove, Intel's CEO at the time, made the decision not to encourage other manufacturers to produce the processor as second sources. This decision was ultimately crucial to Intel's success in the market.[citation needed] The 386 was the first significant microprocessor to be single-sourced. Single-sourcing the 386 allowed Intel greater control over its development and substantially greater profits in later years. AMD introduced its compatible Am386 processor in March 1991 after overcoming legal obstacles, thus ending Intel's 4.7 year monopoly on 386-compatible processors. From 1991 IBM also manufactured 386 chips under license for use only in IBM PCs and boards. Intel originally intended for the 80386 to debut at 16\u00a0MHz. However, due to poor yields, it was instead introduced at 12\u00a0MHz. Early in production, Intel discovered a marginal circuit that could cause a system to return incorrect results from 32-bit multiply operations. Not all of the processors already manufactured were affected, so Intel tested its inventory. Processors that were found to be bug-free were marked with a double-sigma (\u03a3\u03a3), and affected processors were marked \"16 BIT S/W ONLY\". These latter processors were sold as good parts, since at the time 32 bit capability was not relevant for most users. Such chips are now extremely rare and became collectible. The i387 math coprocessor was not ready in time for the introduction of the 80386, and so many of the early 80386 motherboards instead provided a socket and hardware logic to make use of an 80287. In this configuration the FPU would operate asynchronously to the CPU, usually with a clock rate of 10\u00a0MHz. The original Compaq Deskpro 386 is an example of such design. However, this was an annoyance to those who depended on floating point performance, as the performance advantages of the 80387 over the 80287 were significant. Intel later offered a modified version of its 80486DX in 80386 packaging, branded as the Intel RapidCAD. This provided an upgrade path for users with 80386-compatible hardware. The upgrade was a pair of chips that replaced both the 80386 and 80387. Since the 80486DX design contained an FPU, the chip that replaced the 80386 contained the floating point functionality, and the chip that replaced the 80387 served very little purpose. However, the latter chip was necessary in order to provide the FERR signal to the mainboard and appear to function as a normal floating point unit. Third parties offered a wide range of upgrades, for both SX and DX systems. The most popular ones were based on the Cyrix 486DLC/SLC core, which typically offered a substantial speed improvement due to its more efficient instruction pipeline and internal L1 SRAM cache. The cache was usually 1 kB, or sometimes 8 kB in the TI variant. Some of these upgrade chips (such as the 486DRx2/SRx2) were marketed by Cyrix themselves, but they were more commonly found in kits offered by upgrade specialists such as Kingston, Evergreen and Improve-It Technologies. Some of the fastest CPU upgrade modules featured the IBM SLC/DLC family (notable for its 16 kB L1 cache), or even the Intel 486 itself. Many 386 upgrade kits were advertised as being simple drop-in replacements, but often required complicated software to control the cache or clock doubling. Part of the problem was that on most 386 motherboards, the A20 line was controlled entirely by the motherboard with the CPU being unaware, which caused problems on CPUs with internal caches. \nOverall it was very difficult to configure upgrades to produce the results advertised on the packaging, and upgrades were often less than 100% stable or less than 100% compatible. Original version, released in October 1985. A specially packaged Intel 486DX and a dummy floating point unit (FPU) designed as pin-compatible replacements for an Intel 80386 processor and 80387 FPU. This was an embedded version of the 80386SX which did not support real mode and paging in the MMU. System and power management and built in peripheral and support functions: Two 82C59A interrupt controllers; Timer, Counter (3 channels); Asynchronous SIO (2 channels); Synchronous SIO (1 channel); Watchdog timer (Hardware/Software); PIO. Usable with 80387SX or i387SL FPUs. Transparent power management mode, integrated MMU and TTL compatible inputs (only 386SXSA). Usable with i387SX or i387SL FPUs. Transparent power management mode and integrated MMU. Usable with i387SX or i387SL FPUs."}, "486": {"link": "https://en.wikipedia.org/wiki/Intel_i486", "full_form": "Intel 80486 processor", "content": "The Intel 80486, also known as the i486 or 486 (\"four-eighty-six\"), is a higher performance follow-up to the Intel 80386 microprocessor. The 80486 was introduced in 1989 and was the first tightly[a] pipelined x86 design as well as the first x86 chip to use more than a million transistors, due to a large on-chip cache and an integrated floating-point unit. It represents a fourth generation of binary compatible CPUs since the original 8086 of 1978. A 50\u00a0MHz 80486 executes around 40 million instructions per second on average and is able to reach 50 MIPS peak performance. The i486 does not have the usual 80-prefix because of a court ruling that prohibits trademarking numbers (such as 80486). Later, with the introduction of the Pentium brand, Intel began branding its chips with words rather than numbers.   The 80486 was announced at Spring Comdex in April 1989. At the announcement, Intel stated that samples would be available in the third quarter of 1989 and production quantities would ship in the fourth quarter of 1989.[2] The first 80486-based PCs were announced in late 1989, but some advised that people wait until 1990 to purchase a 80486 PC because there were early reports of bugs and software incompatibilities.[3] The instruction set of the i486 is very similar to its predecessor, the Intel 80386, with the addition of only a few extra instructions, such as CMPXCHG which implements a compare-and-swap atomic operation and XADD, a fetch-and-add atomic operation returning the original value (unlike a standard ADD which returns flags only). From a performance point of view, the architecture of the i486 is a vast improvement over the 80386. It has an on-chip unified instruction and data cache, an on-chip floating-point unit (FPU) and an enhanced bus interface unit. Due to the tight pipelining, sequences of simple instructions (such as ALU reg,reg and ALU reg,im) could sustain a single clock cycle throughput (one instruction completed every clock). These improvements yielded a rough doubling in integer ALU performance over the 386 at the same clock rate. A 16-MHz 80486 therefore had a performance similar to a 33-MHz 386, and the older design had to reach 50\u00a0MHz to be comparable with a 25-MHz 80486 part.[b] Just as in the 80386, a simple flat 4 GB memory model could be implemented by setting all \"segment selector\" registers to a neutral value in protected mode, or setting (the same) \"segment registers\" to zero in real mode, and using only the 32-bit \"offset registers\" (x86-terminology for general CPU registers used as address registers) as a linear 32-bit virtual address bypassing the segmentation logic. Virtual addresses were then normally mapped onto physical addresses by the paging system except when it was disabled. (Real mode had no virtual addresses.) Just as with the 80386, circumventing memory segmentation could substantially improve performance in some operating systems and applications. On a typical PC motherboard, either four matched 30-pin (8-bit) SIMMs or one 72-pin (32-bit) SIMM per bank were required to fit the 80486's 32-bit data bus. The address bus used 30-bits (A31..A2) complemented by four byte-select pins (instead of A0,A1) to allow for any 8/16/32-bit selection. This meant that the limit of directly addressable physical memory was 4\u00a0gigabytes as well (230 32-bit words = 232 8-bit words). There are several suffixes and variants. (see Table). Other variants include: The specified maximum internal clock frequency (on Intel's versions) ranged from 16 to 100\u00a0MHz. The 16\u00a0MHz i486SX model was used by Dell Computers. One of the few 80486 models specified for a 50\u00a0MHz bus (486DX-50) initially had overheating problems and was moved to the 0.8 micrometre fabrication process. However, problems continued when the 486DX-50 was installed in local bus systems due to the high bus speed, making it rather unpopular with mainstream consumers as local bus video was considered a requirement at the time, though it remained popular with users of EISA systems. The 486DX-50 was soon eclipsed by the clock-doubled i486DX2 which instead ran the CPU logic at twice the external bus speed which actually means it was slower due to the bus running at only 25 or 33\u00a0MHz More powerful 80486 iterations such as the OverDrive and DX4 were less popular (the latter available as an OEM part only), as they came out after Intel had released the next generation P5 Pentium processor family. Certain steppings of the DX4 also officially supported 50\u00a0MHz bus operation but was a seldom used feature. WT = Write-Through cache strategy, WB = Write-Back cache strategy 80486 compatible processors have been produced by other companies such as IBM, Texas Instruments, AMD, Cyrix, UMC, and SGS Thompson. Some were clones (identical at the microarchitectural level), others were clean room implementations of the Intel instruction-set. (IBM's multiple source requirement is one of the reasons behind its x86-manufacturing since the 80286.) The 80486 was, however, covered by many of Intel's patents covering new R&D as well as that of the prior 80386. Intel and IBM have broad cross-licenses of these patents, and AMD was granted rights to the relevant patents in the 1995 settlement of a lawsuit between the companies.[5] AMD produced several clones of the 80486 using a 40\u00a0MHz bus (486DX-40, 486DX/2-80, and 486DX/4-120) which had no equivalent available from Intel, as well as a part specified for 90\u00a0MHz, using a 30\u00a0MHz external clock, that was sold only to OEMs. The fastest running 80486 CPU, the Am5x86, ran at 133\u00a0MHz and was released by AMD in 1995. 150\u00a0MHz and 160\u00a0MHz parts were planned but never officially released. Cyrix made a variety of 80486-compatible processors, positioned at the cost-sensitive desktop and low-power (laptop) markets. Unlike AMD's 80486 clones, the Cyrix processors were the result of clean-room reverse-engineering. Cyrix's early offerings included the 486DLC and 486SLC, two hybrid chips which plugged into 386DX or SX sockets respectively, and offered 1 KB of cache (versus 8 KB for the then-current Intel/AMD parts). Cyrix also made \"real\" 80486 processors, which plugged into the i486's socket and offered 2 or 8 KB of cache. Clock-for-clock, the Cyrix-made chips were generally slower than their Intel/AMD equivalents, though later products with 8 KB caches were more competitive, if late to market. The Motorola 68040 (e.g. used in the Macintosh Quadra series), while not compatible with the 80486, was often positioned as the 80486's equivalent in features and performance. Clock-for-clock basis the Motorola 68040 could significantly outperform the Intel 80486 chip.[6][7] However, the 80486 had the ability to be clocked significantly faster without suffering from overheating problems. The Motorola 68040 performance lagged behind the later production 80486 systems.[citation needed] Early 80486 machines were equipped with several ISA slots (using an emulated PC/AT-bus) and sometimes one or two 8-bit\u2013only slots (compatible with the PC/XT-bus).[c] Many motherboards enabled overclocking of these up from the default 6 or 8\u00a0MHz to perhaps 16.7 or 20\u00a0MHz (half the i486 bus clock) in a number of steps, often from within the BIOS setup. Especially older peripheral cards normally worked well at such speeds as they often used standard MSI chips instead of slower (at the time) custom VLSI designs. This could give significant performance gains (such as for old video cards moved from a 386 or 286 computer, for example). However, operation beyond 8 or 10\u00a0MHz could sometimes lead to stability problems, at least in systems equipped with SCSI or sound cards. Some motherboards came equipped with a 32-bit bus called EISA that was backward compatible with the ISA-standard. EISA offered a number of attractive features such as increased bandwidth, extended addressing, IRQ sharing, and card configuration through software (rather than through jumpers, DIP switches, etc.) However, EISA cards were expensive and therefore mostly employed in servers and workstations. Consumer desktops often used the simpler but faster VESA Local Bus (VLB), unfortunately somewhat prone to electrical and timing-based instability; typical consumer desktops had ISA slots combined with a single VLB slot for a video card. VLB was gradually replaced by PCI during the final years of the 80486 period. Few Pentium class motherboards had VLB support as VLB was based directly on the i486 bus; it was no trivial matter adapting it to the quite different P5 Pentium-bus. ISA persisted through the P5 Pentium generation and was not completely displaced by PCI until the Pentium III era. Late 80486 boards were normally equipped with both PCI and ISA slots, and sometimes a single VLB slot as well. In this configuration VLB or PCI throughput suffered depending on how buses were bridged. Initially, the VLB slot in these systems was usually fully compatible only with video cards (quite fitting as \"VESA\" stands for Video Electronics Standards Association); VLB-IDE, multi I/O, or SCSI cards could have problems on motherboards with PCI slots. The VL-Bus operated at the same clock speed as the i486-bus (basically being a local 80486-bus) while the PCI bus also usually depended on the i486 clock but sometimes had a divider setting available via the BIOS. This could be set to 1/1 or 1/2, sometimes even 2/3 (for 50\u00a0MHz CPU clocks). Some motherboards limited the PCI clock to the specified maximum of 33\u00a0MHz and certain network cards depended on this frequency for correct bit-rates. The ISA clock was typically generated by a divider of the CPU/VLB/PCI clock (as implied above). One of the earliest complete systems to use the 80486 chip was the Apricot VX FT, produced by United Kingdom hardware manufacturer Apricot Computers. Even overseas in the United States it was popularised as \"The World's First 80486\" in the September 1989 issue of Byte magazine (shown right). Later 80486 boards also supported Plug-And-Play, a specification designed by Microsoft that began as a part of Windows 95 to make component installation easier for consumers. The 486DX2 66\u00a0MHz processor was popular on home-oriented PCs during the early to mid 1990s, toward the end of the MS-DOS gaming era. It was often coupled with a VESA Local Bus video card. The introduction of 3D computer graphics spelled the end of the 80486's reign, because 3D graphics make heavy use of floating point calculations and require a faster CPU cache and more memory bandwidth. Developers began to target the P5 Pentium processor family almost exclusively with x86 assembly language optimizations (e.g., Quake) which led to the usage of terms like \"Pentium compatible processor\" for software requirements. Many of these games required the speed of the P5 Pentium processor family's double-pipelined architecture. The AMD Am5x86, up to 133\u00a0MHz, and Cyrix Cx5x86, up to 120\u00a0MHz, were the last 80486 processors that were often used in late generation 80486 motherboards with PCI slots and 72-pin SIMMs that are designed to be able to run Windows 95, and also often used as upgrades for older 80486 motherboards. While the Cyrix Cx5x86 faded quite quickly when the Cyrix 6x86 took over, the AMD Am5x86 was important during the time when the AMD K5 was delayed. In the general purpose desktop computer role, 80486-based machines remained in use into the early-2000s, especially as Windows 95, Windows 98, and Windows NT 4.0 were the latest Microsoft operating systems to officially support installation on a 80486-based system.[8][9] However, as Windows 95/98 and Windows NT 4.0 were eventually overtaken by newer operating systems, 80486 systems likewise fell out of use. Still, a number of 80486 machines have remained in use today, mostly for backward compatibility with older programs (most notably games), especially since many of them have problems running on newer operating systems. However, DOSBox is also available for current operating systems and provides emulation of the 80486 instruction set, as well as full compatibility with most DOS-based programs.[10] Although the 80486 was eventually overtaken by the Pentium for personal computer applications, Intel had continued production for use in embedded systems. In May 2006 Intel announced that production of the 80486 would stop at the end of September 2007.[11] This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "4B5BLF": {"link": "https://en.wikipedia.org/wiki/4B5B", "full_form": "4 Byte 5 Byte Local Fiber", "content": "In telecommunication, 4B5B is a form of data communications Block Coding. 4B5B maps groups of 4 bits (aka quadbits) onto groups of 5 bits, with a minimum density of 1 bits in the output.[clarification needed] When NRZI-encoded, the 1 bits provide necessary clock transitions for the receiver. For example, a run of 4 bits such as 00002 contains no transitions and that causes clocking problems for the receiver. 4B5B solves this problem by assigning each block of 4 consecutive bits an equivalent word of 5 bits. These 5 bit words are pre-determined in a dictionary and they are chosen to ensure that there will be at least two transitions per block of bits. A collateral effect of the code is that more bits are needed to send the same information than with 4 bits. An alternate to using 4B5B coding is to use a scrambler. Depending on the standard or specification of interest, there may be several 4B5B characters left unused. The presence of any of the \"unused\" characters in the data stream can be used as an indication that there is a fault somewhere in the link. Therefore, the unused characters can be used to detect errors in the data stream. 4B5B was popularized by fiber distributed data interface (FDDI) in the mid-1980s, and was later adopted by The name 4B5B is generally taken to mean the FDDI version. Other 4-to-5-bit codes have been used for magnetic recording and are known as group coded recording (GCR). On optical fiber, the 4B5B output is NRZI-encoded. FDDI over copper (CDDI) uses MLT-3 encoding instead, as does 100BASE-TX. The 4B5B encoding is also used for USB Power Delivery communication on CC pin, over BMC protocol. The following character sets are sometimes referred to as command characters. (HDLC = High-Level Data Link Control)"}, "4GL": {"link": "https://en.wikipedia.org/wiki/Fourth-generation_programming_language", "full_form": "Fourth-Generation Programming Language", "content": "A fourth-generation programminglanguage (4GL) is a computer programming language envisioned as a refinement of the style of languages classified as third-generation programming language (3GL). Each of the programming language generations aims to provide a higher level of abstraction of the internal computer hardware details, making the language more programmer-friendly, powerful and versatile. While the definition of 3GL has changed over time, it can be typified by operating more with large collections of information at once rather than focusing on just bits and bytes. Languages claimed to be 4GL may include support for database management, report generation, mathematical optimization, GUI development, or web development. Some researchers state that 4GLs are a subset of domain-specific languages.[1][2] The concept of 4GL was developed from the 1970s through the 1990s, overlapping most of the development of 3GL. While 3GLs like C, C++, C#, Java, and JavaScript remain popular for a wide variety of uses, 4GLs as originally defined found narrower uses.[citation needed] Some advanced 3GLs like Python, Ruby, and Perl combine some 4GL abilities within a general-purpose 3GL environment. Also, libraries with 4GL-like features have been developed as add-ons for most popular 3GLs. This has blurred the distinction of 4GL and 3GL. In the 1980s and 1990s, there were efforts to develop fifth-generation programming languages (5GL).   Though used earlier in papers and discussions, the term 4GL was first used formally by James Martin in his 1981 book Applications Development Without Programmers[3] to refer to non-procedural, high-level specification languages. In some primitive way, early 4GLs were included in the Informatics MARK-IV (1967) product and Sperry's MAPPER (1969 internal use, 1979 release). The motivations for the '4GL' inception and continued interest are several. The term can apply to a large set of software products. It can also apply to an approach that looks for greater semantic properties and implementation power. Just as the 3GL offered greater power to the programmer, so too did the 4GL open up the development environment to a wider population. In a sense, the 4GL is an example of 'black box' processing, each generation (in the sense of the page) is further from the machine (see the Computer Science history in regard to data structure improvements and information hiding). It is this latter nature that is directly associated with 4GL having errors that are harder, in many cases, to debug. In terms of applications, a 4GL could be business oriented or it could deal with some technical domain. Being further from the machine implies being closer to domain. Given the wide disparity of concepts and methods across domains, 4GL limitations lead to recognition of the need for the 5GL.[original research?] The early input scheme for the 4GL supported entry of data within the 72-character limit of the punched card (8 bytes used for sequencing) where a card's tag would identify the type or function. With judicious use of a few cards, the 4GL deck could offer a wide variety of processing and reporting capability whereas the equivalent functionality coded in a 3GL could subsume, perhaps, a whole box or more of cards.[4] The 72-character metaphor continued for a while as hardware progressed to larger memory and terminal interfaces. Even with its limitations, this approach supported highly sophisticated applications. As interfaces improved and allowed longer statement lengths and grammar-driven input handling, greater power ensued. An example of this is described on the Nomad page. The development of the 4GL was influenced by several factors, with the hardware and operating system constraints having a large weight. When the 4GL was first introduced, a disparate mix of hardware and operating systems mandated custom application development support that was specific to the system in order to ensure sales. One example is the MAPPER system developed by Sperry. Though it has roots back to the beginning, the system has proven successful in many applications and has been ported to modern platforms. The latest variant is embedded in the BIS[5] offering of Unisys. MARK-IV is now known as VISION:BUILDER and is offered by Computer Associates. Santa Fe railroad used MAPPER to develop a system, in a project that was an early example of 4GL, rapid prototyping, and programming by users.[6] The idea was that it was easier to teach railroad experts to use MAPPER than to teach programmers the \"intricacies of railroad operations\".[7] One of the early (and portable) languages that had 4GL properties was Ramis developed by Gerald C. Cohen at Mathematica, a mathematical software company. Cohen left Mathematica and founded Information Builders to create a similar reporting-oriented 4GL, called FOCUS. Later 4GL types are tied to a database system and are far different from the earlier types in their use of techniques and resources that have resulted from the general improvement of computing with time. An interesting twist to the 4GL scene is realization that graphical interfaces and the related reasoning done by the user form a 'language' that is poorly understood. A number of different types of 4GLs exist: Some 4GLs have integrated tools which allow for the easy specification of all the required information: General use / versatile Database query languages Report generators Data manipulation, analysis, and reporting languages GUI creators Mathematical optimization Database-driven GUI application development Screen painters and generators Web development languages This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "4NF": {"link": "https://en.wikipedia.org/wiki/4NF", "full_form": "Fourth Normal Form", "content": "Fourth normal form (4NF) is a normal form used in database normalization. Introduced by Ronald Fagin in 1977, 4NF is the next level of normalization after Boyce\u2013Codd normal form (BCNF). Whereas the second, third, and Boyce\u2013Codd normal forms are concerned with functional dependencies, 4NF is concerned with a more general type of dependency known as a multivalued dependency. A table is in 4NF if and only if, for every one of its non-trivial multivalued dependencies X \n\n\n\n\u21a0\n\n\n{\\displaystyle \\twoheadrightarrow }\n\n Y, X is a superkey\u2014that is, X is either a candidate key or a superset thereof.[1]   If the column headings in a relational database table are divided into three disjoint groupings X, Y, and Z, then, in the context of a particular row, we can refer to the data beneath each group of headings as x, y, and z respectively. A multivalued dependency X \n\n\n\n\u21a0\n\n\n{\\displaystyle \\twoheadrightarrow }\n\n Y signifies that if we choose any x actually occurring in the table (call this choice xc), and compile a list of all the xcyz combinations that occur in the table, we will find that xc is associated with the same y entries regardless of z. So essentially the presence of z provides no useful information to constrain the possible values of y. A trivial multivalued dependency X \n\n\n\n\u21a0\n\n\n{\\displaystyle \\twoheadrightarrow }\n\n Y is one where either Y is a subset of X, or X and Y together form the whole set of attributes of the relation. A functional dependency is a special case of multivalued dependency. In a functional dependency X \u2192 Y, every x determines exactly one y, never more than one. Consider the following example: Each row indicates that a given restaurant can deliver a given variety of pizza to a given area. The table has no non-key attributes because its only key is {Restaurant, Pizza Variety, Delivery Area}. Therefore it meets all normal forms up to BCNF. If we assume, however, that pizza varieties offered by a restaurant are not affected by delivery area (i.e. a restaurant offers all pizza varieties it makes to all areas it supplies), then it does not meet 4NF. The problem is that the table features two non-trivial multivalued dependencies on the {Restaurant} attribute (which is not a superkey). The dependencies are: These non-trivial multivalued dependencies on a non-superkey reflect the fact that the varieties of pizza a restaurant offers are independent from the areas to which the restaurant delivers. This state of affairs leads to redundancy in the table: for example, we are told three times that A1 Pizza offers Stuffed Crust, and if A1 Pizza starts producing Cheese Crust pizzas then we will need to add multiple rows, one for each of A1 Pizza's delivery areas. There is, moreover, nothing to prevent us from doing this incorrectly: we might add Cheese Crust rows for all but one of A1 Pizza's delivery areas, thereby failing to respect the multivalued dependency {Restaurant} \n\n\n\n\u21a0\n\n\n{\\displaystyle \\twoheadrightarrow }\n\n {Pizza Variety}. To eliminate the possibility of these anomalies, we must place the facts about varieties offered into a different table from the facts about delivery areas, yielding two tables that are both in 4NF: In contrast, if the pizza varieties offered by a restaurant sometimes did legitimately vary from one delivery area to another, the original three-column table would satisfy 4NF. Ronald Fagin demonstrated that it is always possible to achieve 4NF.[2] Rissanen's theorem is also applicable on multivalued dependencies. A 1992 paper by Margaret S. Wu notes that the teaching of database normalization typically stops short of 4NF, perhaps because of a belief that tables violating 4NF (but meeting all lower normal forms) are rarely encountered in business applications. This belief may not be accurate, however. Wu reports that in a study of forty organizational databases, over 20% contained one or more tables that violated 4NF while meeting all lower normal forms.[3] Only in rare situations does a 4NF table not conform to the higher normal form 5NF. These are situations in which a complex real-world constraint governing the valid combinations of attribute values in the 4NF table is not implicit in the structure of that table."}, "5GL": {"link": "https://en.wikipedia.org/wiki/Fifth-generation_programming_language", "full_form": "Fifth-Generation Programming Language", "content": "A fifth-generation programming language (5GL) is a programming language based on solving using constraints given to the program, rather than using an algorithm written by a programmer.[citation needed] Most constraint-based and logic programming languages and some other declarative languages are fifth-generation languages.   While fourth-generation programming languages are designed to build specific programs, fifth-generation languages are designed to make the computer solve a given problem without the programmer. This way, the user only needs to worry about what problems need to be solved and what conditions need to be met, without worrying about how to implement a routine or algorithm to solve them. Fifth-generation languages are used mainly in artificial intelligence research. Prolog, OPS5 and Mercury are examples of fifth-generation languages.[1] These types of languages were also built upon Lisp, many originating on the Lisp machine, such as ICAD. Then, there are many frame languages, such as KL-ONE.[citation needed] In the 1980s, fifth-generation languages were considered to be the way of the future, and some predicted that they would replace all other languages for system development, with the exception of low-level languages.[citation needed] Most notably, from 1982 to 1993, Japan[2][3] put much research and money into their fifth-generation computer systems project, hoping to design a massive computer network of machines using these tools. However, as larger programs were built, the flaws of the approach became more apparent. It turns out that, given a set of constraints defining a particular problem, deriving an efficient algorithm to solve it is a very difficult problem in itself. This crucial step cannot yet be automated and still requires the insight of a human programmer. Vendors have been known on occasion to advertise their languages as 5GL.[4] Most of the time they actually sell 4GLs with a higher level of automation and knowledge base. Because the hype of the 1980s faded away and the projects were eventually all dropped, 5GL awareness has also dropped; this has opened doors to the vendors to re-use the term in marketing their new tools, without causing much controversy among the current generations of programmers.[citation needed]"}, "5NF": {"link": "https://en.wikipedia.org/wiki/5NF", "full_form": "Fifth Normal Form", "content": "Fifth normal form (5NF), also known as project-join normal form (PJ/NF) is a level of database normalization designed to reduce redundancy in relational databases recording multi-valued facts by isolating semantically related multiple relationships. A table is said to be in the 5NF if and only if every non-trivial join dependency in that table is implied by the candidate keys. A join dependency *{A, B, \u2026 Z} on R is implied by the candidate key(s) of R if and only if each of A, B, \u2026, Z is a superkey for R.[1] The fifth normal form was first described by Ronald Fagin in his 1979 conference paper Normal forms and relational database operators.[2]   Consider the following example: The table's predicate is: Products of the type designated by Product Type, made by the brand designated by Brand, are available from the traveling salesman designated by Traveling Salesman. The primary key is the composite of all three columns. Also note that the table is in 4NF, since there are no multivalued dependencies (2-part join dependencies) in the table: no column (which by itself is not a candidate key or a superkey) is a determinant for the other two columns. In the absence of any rules restricting the valid possible combinations of Traveling Salesman, Brand, and Product Type, the three-attribute table above is necessary in order to model the situation correctly. Suppose, however, that the following rule applies: A Traveling Salesman has certain Brands and certain Product Types in his repertoire. If Brand B1 and Brand B2 are in his repertoire, and Product Type P is in his repertoire, then (assuming Brand B1 and Brand B2 both make Product Type P), the Traveling Salesman must offer products of Product Type P those made by Brand B1 and those made by Brand B2. In that case, it is possible to split the table into three: In this case, it's impossible for Louis Ferguson to refuse to offer Vacuum Cleaners made by ACME (assuming ACME makes Vacuum Cleaners) if he sells anything else made by Acme (Lava Lamp) and he also sells Vacuum Cleaners made by any other brand (Robusto). Note how this setup helps to remove redundancy. Suppose that Jack Schneider starts selling Robusto's products Breadboxes and Vacuum Cleaners. In the previous setup we would have to add two new entries one for each product type (<Jack Schneider, Robusto, Breadboxes>, <Jack Schneider, Robusto, Vacuum Cleaners>). With the new setup we need to add only a single entry (<Jack Schneider, Robusto>)in Brands By Traveling Salesman. Only in rare situations does a 4NF table not conform to 5NF. These are situations in which a complex real-world constraint governing the valid combinations of attribute values in the 4NF table is not implicit in the structure of that table. If such a table is not normalized to 5NF, the burden of maintaining the logical consistency of the data within the table must be carried partly by the application responsible for insertions, deletions, and updates to it; and there is a heightened risk that the data within the table will become inconsistent. In contrast, the 5NF design excludes the possibility of such inconsistencies. A table T is in fifth normal form (5NF) or Project-Join Normal Form (PJNF) if it cannot have a lossless decomposition into any number of smaller tables. The case where all the smaller tables after the decomposition have the same candidate key as the table T is excluded."}, "6NF": {"link": "https://en.wikipedia.org/wiki/6NF", "full_form": "Sixth Normal Form", "content": "Sixth normal form (6NF) is a term in relational database theory, used in two different ways.   Christopher J. Date and others have defined sixth normal form as a normal form, based on an extension of the relational algebra.[1][2][3] Relational operators, such as join, are generalized to support a natural treatment of interval data, such as sequences of dates or moments in time, for instance in temporal databases.[4][2][3] Sixth normal form is then based on this generalized join, as follows: A relvar R [table] is in sixth normal form (abbreviated 6NF) if and only if it satisfies no nontrivial join dependencies at all \u2014 where, as before, a join dependency is trivial if and only if at least one of the projections (possibly U_projections) involved is taken over the set of all attributes of the relvar [table] concerned.[5] Date et al. have also given the following definition: Relvar R is in sixth normal form (6NF) if and only if every JD [Join Dependency] of R is trivial \u2014 where a JD is trivial if and only if one of its components is equal to the pertinent heading in its entirety.[6] Any relation in 6NF is also in 5NF. Sixth normal form is intended to decompose relation variables to irreducible components. Though this may be relatively unimportant for non-temporal relation variables, it can be important when dealing with temporal variables or other interval data. For instance, if a relation comprises a supplier's name, status, and city, we may also want to add temporal data, such as the time during which these values are, or were, valid (e.g., for historical data) but the three values may vary independently of each other and at different rates. We may, for instance, wish to trace the history of changes to Status; a review of production costs may reveal that a change was caused by a supplier changing city and hence what they charged for delivery. For further discussion on Temporal Aggregation in SQL, see also Zimanyi.[7] For a different approach, see TSQL2.[8] Some authors have used the term sixth normal form differently: as a synonym for Domain/key normal form (DKNF). This usage predates Date et al.'s work. [9] The sixth normal form is currently being used in some data warehouses where the benefits outweigh the drawbacks,[10] for example using Anchor Modeling. Although using 6NF leads to an explosion of tables, modern databases can prune the tables from select queries (using a process called 'table elimination') where they are not required and thus speed up queries that only access several attributes. In order for a table to be in 6NF, it has to comply with the 5NF first and then it requires that each table satisfies only trivial join dependencies. Let\u2019s take a simple example[11] with a table already in 5NF: Here, in the users table, every attribute is non null and the primary key is the username: Users_table This table is in 5NF because each join dependency is implied by the candidate key. More specific, the only possible join dependencies are: {username, status}, {username,department}. The 6NF version would look like this: Users Users_dept Nevertheless, you need to think very much before trying to apply the 6NF normalization because this implies a dramatic increase in the number of tables and may not suit your needs. Another example in which we can demonstrate the 6NF is when we look at the space occupied. For this we chose the healthcare domain with this table: TABLE 1 The healthcare domain contains several specializations up until the maximum development in this domain. These are: - resident - probationer - specialist To get promoted to a higher position, it will take several years for someone to obtain his or her proper training. If a doctor has practiced in the field less than the required period, he or she won\u2019t be able to advance in rank. For example: If Michael Miller, orthopedic probationer, has worked in the medical field for 3 years and 11 months, he won\u2019t be able to become an orthopedic specialist because the minimum period to promote from probationare to specialist is 4 years. The transition from one position to another is based on an exam. The exam, required to make the progress from one grade to another (for example: from probationer to specialist), can be taken after a period of 4 years. The next step in applying the 6NF for the Table 1, is to eliminate all non-trivial join dependencies. TABLE 2.1 TABLE 2.2 We will show now that passing from 5NF to 6NF also reduces the space occupied by the table. In the brackets, it is indicated how much space each field of the table occupies (in bytes). TABLE 1 = [366] (bytes) We can see that Table 1, which is in 5NF, occupies, in total, 366 bytes. This table translated into 6NF will consist of tables Table 2.1 and Table 2.2. The last 2 will occupy together 326 bytes. TABLE 2.1 = [270] TABLE 2.2 = [56] => TABLE 2.1 + TABLE 2.2 = [326] (bytes) We can see that, in this example, 6NF occupies less than 5NF (more specific, less with 40 bytes). Going into the 6NF reduces the occupied space. If the initial table is larger, after going into the 6NF, the reduced space will also be larger. In practice though, row overhead makes this separation of information in multiple tables occupy more space. It doesn't however detract from the increased flexibility, consistency - and query complexity."}, "8B10BLF": {"link": "https://en.wikipedia.org/wiki/8b/10b_encoding", "full_form": "8 Byte 10 Byte Local Fiber", "content": "In telecommunications, 8b/10b is a line code that maps 8-bit words to 10-bit symbols to achieve DC-balance and bounded disparity, and yet provide enough state changes to allow reasonable clock recovery. This means that the difference between the counts of ones and zeros in a string of at least 20 bits is no more than two, and that there are not more than five ones or zeros in a row. This helps to reduce the demand for the lower bandwidth limit of the channel necessary to transfer the signal. An 8b/10b code can be implemented in various ways, where the design may focus on specific parameters such as hardware requirements, DC-balance, etc. One implementation was designed by K. Odaka for the DAT digital audio recorder.[1] Kees Schouhamer Immink designed an 8b/10b code for the DCC audio recorder.[2] The IBM implementation was described in 1983 by Al Widmer and Peter Franaszek.[3][4]   As the scheme name suggests, eight bits of data are transmitted as a 10-bit entity called a symbol, or character. The low five bits of data are encoded into a 6-bit group (the 5b/6b portion) and the top three bits are encoded into a 4-bit group (the 3b/4b portion). These code groups are concatenated together to form the 10-bit symbol that is transmitted on the wire. The data symbols are often referred to as D.x.y where x ranges over 0\u201331 and y over 0\u20137. Standards using the 8b/10b encoding also define up to 12 special symbols (or control characters) that can be sent in place of a data symbol. They are often used to indicate start-of-frame, end-of-frame, link idle, skip and similar link-level conditions. At least one of them (i.e. a \"comma\" symbol) needs to be used to define the alignment of the 10 bit symbols. They are referred to as K.x.y and have different encodings from any of the D.x.y symbols. Because 8b/10b encoding uses 10-bit symbols to encode 8-bit words, some of the possible 1024 (10 bit, 210) codes can be excluded to grant a run-length limit of 5 consecutive equal bits and to achieve the difference of the count of zeros and ones to be no more than two. Some of the 256 possible 8-bit words can be encoded in two different ways. Using these alternative encodings, the scheme is able to achieve long-term DC-balance in the serial data stream. This permits the data stream to be transmitted through a channel with a high-pass characteristic, for example Ethernet's transformer-coupled unshielded twisted pair or optical receivers using automatic gain control. Note that in the following tables, for each input byte, A is the least significant bit, and H the most significant. The output gains two extra bits, i and j. The bits are sent low to high: a, b, c, d, e, i, f, g, h, and j; i.e., the 5b/6b code followed by the 3b/4b code. This ensures the uniqueness of the special bit sequence in the comma codes. The residual effect on the stream to the number of zero and one bits transmitted is maintained as the running disparity (RD) and the effect of slew is balanced by the choice of encoding for following symbols. The 5b/6b code is a paired disparity code, and so is the 3b/4b code. Each 6- or 4-bit code word has either equal numbers of zeros and ones (a disparity of zero), or comes in a pair of forms, one with two more zeros than ones (four zeros and two ones, or three zeros and one one, respectively) and one with two less. When a 6- or 4-bit code is used that has a non-zero disparity (count of ones minus count of zeros; i.e., \u22122 or +2), the choice of positive or negative disparity encodings must be the one that toggles the running disparity. In other words, the non zero disparity codes alternate. 8b/10b coding is DC-free, meaning that the long-term ratio of ones and zeros transmitted is exactly 50%. To achieve this, the difference between the number of ones transmitted and the number of zeros transmitted is always limited to \u00b12, and at the end of each symbol, it is either +1 or \u22121. This difference is known as the running disparity (RD). This scheme needs only two states for running disparity of +1 and \u22121. It starts at \u22121.[5] For each 5b/6b and 3b/4b code with an unequal number of ones and zeros, there are two bit patterns that can be used to transmit it: one with two more \"1\" bits, and one with all bits inverted and thus two more zeros. Depending on the current running disparity of the signal, the encoding engine selects which of the two possible six- or four-bit sequences to send for the given data. Obviously, if the six-bit or four-bit code has equal numbers of ones and zeros, there is no choice to make, as the disparity would be unchanged, with the exceptions of sub-blocks D.07 (00111) and D.x.3 (011). In either case the disparity is still unchanged, but if RD is positive when D.07 is encountered, use 000111, and if it's negative use 111000. Likewise, if RD is positive when D.x.3 is encountered use 0011, and if it's negative use 1100. This is accurately reflected in the charts below, but is worth additional mention as these are the only two sub-blocks with equal numbers of 1s and 0s that each have two possible encodings. \u2020 Same code is used for K.x.7 \u2020 For D.x.7, either the Primary (D.x.P7), or the Alternate (D.x.A7) encoding must be selected in order to avoid a run of five consecutive 0s or 1s when combined with the preceding 5b/6b code. Sequences of five identical bits are used in comma codes for synchronization issues. D.x.A7 is used for only x\u00a0=\u00a017, x\u00a0=\u00a018, and x\u00a0=\u00a020 when RD\u00a0=\u00a0\u22121 and for x\u00a0=\u00a011, x\u00a0=\u00a013, and x\u00a0=\u00a014 when RD\u00a0=\u00a0+1. With x\u00a0=\u00a023, x\u00a0=\u00a027, x\u00a0=\u00a029, and x\u00a0=\u00a030, the same code forms the control codes K.x.7. Any other x.A7 code can't be used as it would result in chances for misaligned comma sequences. \u2021 The alternate encoding for the K.x.y codes with disparity 0 make it possible for only K.28.1, K.28.5, and K.28.7 to be \"comma\" codes that contain a bit sequence which can't be found elsewhere in the data stream. The control symbols within 8b/10b are 10b symbols that are valid sequences of bits (no more than six 1s or 0s) but do not have a corresponding 8b data byte. They are used for low-level control functions. For instance, in Fibre Channel, K28.5 is used at the beginning of four-byte sequences (called \"Ordered Sets\") that perform functions such as Loop Arbitration, Fill Words, Link Resets, etc. Resulting from the 5b/6b and 3b/4b tables the following 12 control symbols are allowed to be sent: \u2020 Within the control symbols, K.28.1, K.28.5, and K.28.7 are \"comma symbols\". Comma symbols are used for synchronization (finding the alignment of the 8b/10b codes within a bit-stream). If K.28.7 is not used, the unique comma sequences 0011111 or 1100000 cannot be found at any bit position within any combination of normal codes. \u2021 If K.28.7 is allowed in the actual coding, a more complex definition of the synchronization pattern than suggested by \u2020 needs to be used, as a combination of K.28.7 with several other codes forms a false misaligned comma symbol overlapping the two codes. A sequence of multiple K.28.7 codes is not allowable in any case, as this would result in undetectable misaligned comma symbols. K.28.7 is the only comma symbol that cannot be the result of a single bit error in the data stream. After the above-mentioned IBM patent expired, the scheme became even more popular and was chosen as a DC-free line code for several communication technologies. Among the areas in which 8b/10b encoding finds application are the following: The FC-0 standard defines what encoding scheme is to be used (8b/10b or 64b/66b) in a Fibre Channel system[7]\u00a0\u2013  higher speed variants typically use 64b/66b to optimize bandwidth efficiency (since bandwidth overhead is 25% in 8b/10b versus approximately 3% (~\u00a02/66) in 64b/66b systems). Thus, 8b/10b encoding is used for 4GFC and 8GFC variants; for 10GFC and 16GFC variants, it is 64b/66b.[8] The Fibre Channel FC1 data link layer is then responsible for implementing the 8b/10b encoding and decoding of signals. The Fibre Channel 8b/10b coding scheme is also used in other telecommunications systems. Data is expanded using an algorithm that creates one of two possible 10-bit output values for each input 8-bit value. Each 8-bit input value can map either to a 10-bit output value with odd disparity, or to one with even disparity. This mapping is usually done at the time when parallel input data is converted into a serial output stream for transmission over a fibre channel link. The odd/even selection is done in such a way that a long-term zero disparity between ones and zeroes is maintained. This is often called \"DC balancing\". The 8-bit to 10-bit conversion scheme uses only 512 of the possible 1024 output values. Of the remaining 512 unused output values, most contain either too many ones or too many zeroes so are not allowed. However this still leaves enough spare 10-bit odd+even coding pairs to allow for 12 special non-data characters. The codes that represent the 256 data values are called the data (D) codes. The codes that represent the 12 special non-data characters are called the control (K) codes. All of the codes can be described by stating 3 octal values. This is done with a naming convention of \"Dxx.x\" or \"Kxx.x\". Example: Now these bits are converted to decimal in the way they are paired. Input data E 8B/10B = D03.6 Encoding schemes 8b/10b have found a heavy use in digital audio storage applications, namely A differing but related scheme is used for audio CDs and CD-ROMs: Note that 8b/10b is the encoding scheme, not a specific code. While many applications do use the same code, there exist some incompatible implementations; for example, Transition Minimized Differential Signaling, which also expands 8 bits to 10 bits, but it uses a completely different method to do so. 64b/66b encoding, introduced for 10 Gigabit Ethernet's 10GBASE-R Physical Medium Dependent (PMD) interfaces, is a lower-overhead alternative to 8b/10b encoding, having a two-bit overhead per 64 bits (instead of eight bits) of encoded data. This scheme is considerably different in design from 8b/10b encoding, and does not explicitly guarantee DC balance, short run length, and transition density (these features are achieved statistically via scrambling). 64b/66b encoding has been extended to the 128b/130b and 128b/132b encoding variants for PCI Express\u00a03.0 and USB\u00a03.1, respectively, replacing the 8b/10b encoding in earlier revisions of each standard.[9]"}, "802.11": {"link": "https://en.wikipedia.org/wiki/802.11", "full_form": "Wireless LAN", "content": "IEEE 802.11 is a set of media access control (MAC) and physical layer (PHY) specifications for implementing wireless local area network (WLAN) computer communication in the 900\u00a0MHz and 2.4, 3.6, 5, and 60\u00a0GHz frequency bands. They are created and maintained by the Institute of Electrical and Electronics Engineers (IEEE) LAN/MAN Standards Committee (IEEE 802). The base version of the standard was released in 1997, and has had subsequent amendments. The standard and amendments provide the basis for wireless network products using the Wi-Fi brand. While each amendment is officially revoked when it is incorporated in the latest version of the standard, the corporate world tends to market to the revisions because they concisely denote capabilities of their products. As a result, in the marketplace, each revision tends to become its own standard.   The 802.11 family consists of a series of half-duplex over-the-air modulation techniques that use the same basic protocol. 802.11-1997 was the first wireless networking standard in the family, but 802.11b was the first widely accepted one, followed by 802.11a, 802.11g, 802.11n, and 802.11ac. Other standards in the family (c\u2013f, h, j) are service amendments that are used to extend the current scope of the existing standard, which may also include corrections to a previous specification.[1] 802.11b and 802.11g use the 2.4\u00a0GHz ISM band, operating in the United States under Part 15 of the U.S. Federal Communications Commission Rules and Regulations. Because of this choice of frequency band, 802.11b and g equipment may occasionally suffer interference from microwave ovens, cordless telephones, and Bluetooth devices. 802.11b and 802.11g control their interference and susceptibility to interference by using direct-sequence spread spectrum (DSSS) and orthogonal frequency-division multiplexing (OFDM) signaling methods, respectively. 802.11a uses the 5\u00a0GHz U-NII band, which, for much of the world, offers at least 23 non-overlapping channels rather than the 2.4\u00a0GHz ISM frequency band offering only three non-overlapping channels, where other adjacent channels overlap\u2014see list of WLAN channels. Better or worse performance with higher or lower frequencies (channels) may be realized, depending on the environment. 802.11n can use either the 2.4\u00a0GHz or the 5\u00a0GHz band; 802.11ac uses only the 5\u00a0GHz band. The segment of the radio frequency spectrum used by 802.11 varies between countries. In the US, 802.11a and 802.11g devices may be operated without a license, as allowed in Part 15 of the FCC Rules and Regulations. Frequencies used by channels one through six of 802.11b and 802.11g fall within the 2.4\u00a0GHz amateur radio band. Licensed amateur radio operators may operate 802.11b/g devices under Part 97 of the FCC Rules and Regulations, allowing increased power output but not commercial content or encryption.[2] 802.11 technology has its origins in a 1985 ruling by the U.S. Federal Communications Commission that released the ISM band[1] for unlicensed use.[3] In 1991 NCR Corporation/AT&T (now Nokia Labs and LSI Corporation) invented a precursor to 802.11 in Nieuwegein, the Netherlands. The inventors initially intended to use the technology for cashier systems. The first wireless products were brought to the market under the name WaveLAN with raw data rates of 1\u00a0Mbit/s and 2\u00a0Mbit/s. Vic Hayes, who held the chair of IEEE 802.11 for 10 years, and has been called the \"father of Wi-Fi\", was involved in designing the initial 802.11b and 802.11a standards within the IEEE.[4] In 1999, the Wi-Fi Alliance was formed as a trade association to hold the Wi-Fi trademark under which most products are sold.[5] The original version of the standard IEEE 802.11 was released in 1997 and clarified in 1999, but is now obsolete. It specified two net bit rates of 1 or 2 megabits per second (Mbit/s), plus forward error correction code. It specified three alternative physical layer technologies: diffuse infrared operating at 1\u00a0Mbit/s; frequency-hopping spread spectrum operating at 1\u00a0Mbit/s or 2\u00a0Mbit/s; and direct-sequence spread spectrum operating at 1\u00a0Mbit/s or 2\u00a0Mbit/s. The latter two radio technologies used microwave transmission over the Industrial Scientific Medical frequency band at 2.4\u00a0GHz. Some earlier WLAN technologies used lower frequencies, such as the U.S. 900\u00a0MHz ISM band. Legacy 802.11 with direct-sequence spread spectrum was rapidly supplanted and popularized by 802.11b. Originally described as clause 17 of the 1999 specification, the OFDM waveform at 5.8\u00a0GHz is now defined in clause 18 of the 2012 specification, and provides protocols that allow transmission and reception of data at rates of 1.5 to 54\u00a0Mbit/s. It has seen widespread worldwide implementation, particularly within the corporate workspace. While the original amendment is no longer valid, the term 802.11a is still used by wireless access point (cards and routers) manufacturers to describe interoperability of their systems at 5\u00a0GHz, 54\u00a0Mbit/s. The 802.11a standard uses the same data link layer protocol and frame format as the original standard, but an OFDM based air interface (physical layer). It operates in the 5\u00a0GHz band with a maximum net data rate of 54\u00a0Mbit/s, plus error correction code, which yields realistic net achievable throughput in the mid-20 Mbit/s.[13] Since the 2.4\u00a0GHz band is heavily used to the point of being crowded, using the relatively unused 5\u00a0GHz band gives 802.11a a significant advantage. However, this high carrier frequency also brings a disadvantage: the effective overall range of 802.11a is less than that of 802.11b/g. In theory, 802.11a signals are absorbed more readily by walls and other solid objects in their path due to their smaller wavelength, and, as a result, cannot penetrate as far as those of 802.11b. In practice, 802.11b typically has a higher range at low speeds (802.11b will reduce speed to 5.5\u00a0Mbit/s or even 1\u00a0Mbit/s at low signal strengths). 802.11a also suffers from interference,[14] but locally there may be fewer signals to interfere with, resulting in less interference and better throughput. The 802.11b standard has a maximum raw data rate of 11\u00a0Mbit/s, and uses the same media access method defined in the original standard. 802.11b products appeared on the market in early 2000, since 802.11b is a direct extension of the modulation technique defined in the original standard. The dramatic increase in throughput of 802.11b (compared to the original standard) along with simultaneous substantial price reductions led to the rapid acceptance of 802.11b as the definitive wireless LAN technology. Devices using 802.11b experience interference from other products operating in the 2.4\u00a0GHz band. Devices operating in the 2.4\u00a0GHz range include microwave ovens, Bluetooth devices, baby monitors, cordless telephones, and some amateur radio equipment. In June 2003, a third modulation standard was ratified: 802.11g. This works in the 2.4\u00a0GHz band (like 802.11b), but uses the same OFDM based transmission scheme as 802.11a. It operates at a maximum physical layer bit rate of 54\u00a0Mbit/s exclusive of forward error correction codes, or about 22\u00a0Mbit/s average throughput.[15] 802.11g hardware is fully backward compatible with 802.11b hardware, and therefore is encumbered with legacy issues that reduce throughput by ~21% when compared to 802.11a.[citation needed] The then-proposed 802.11g standard was rapidly adopted in the market starting in January 2003, well before ratification, due to the desire for higher data rates as well as to reductions in manufacturing costs. By summer 2003, most dual-band 802.11a/b products became dual-band/tri-mode, supporting a and b/g in a single mobile adapter card or access point. Details of making b and g work well together occupied much of the lingering technical process; in an 802.11g network, however, activity of an 802.11b participant will reduce the data rate of the overall 802.11g network. Like 802.11b, 802.11g devices suffer interference from other products operating in the 2.4\u00a0GHz band, for example wireless keyboards. In 2003, task group TGma was authorized to \"roll up\" many of the amendments to the 1999 version of the 802.11 standard. REVma or 802.11ma, as it was called, created a single document that merged 8 amendments (802.11a, b, d, e, g, h, i, j) with the base standard. Upon approval on March 8, 2007, 802.11REVma was renamed to the then-current base standard IEEE 802.11-2007.[16] 802.11n is an amendment that improves upon the previous 802.11 standards by adding multiple-input multiple-output antennas (MIMO). 802.11n operates on both the 2.4\u00a0GHz and the 5\u00a0GHz bands. Support for 5\u00a0GHz bands is optional. It operates at a maximum net data rate from 54\u00a0Mbit/s to 600\u00a0Mbit/s. The IEEE has approved the amendment, and it was published in October 2009.[17][18] Prior to the final ratification, enterprises were already migrating to 802.11n networks based on the Wi-Fi Alliance's certification of products conforming to a 2007 draft of the 802.11n proposal. In May 2007, task group TGmb was authorized to \"roll up\" many of the amendments to the 2007 version of the 802.11 standard.[19] REVmb or 802.11mb, as it was called, created a single document that merged ten amendments (802.11k, r, y, n, w, p, z, v, u, s) with the 2007 base standard. In addition much cleanup was done, including a reordering of many of the clauses.[20] Upon publication on March 29, 2012, the new standard was referred to as IEEE 802.11-2012. IEEE 802.11ac-2013 is an amendment to IEEE 802.11, published in December 2013, that builds on 802.11n.[21] Changes compared to 802.11n include wider channels (80 or 160\u00a0MHz versus 40\u00a0MHz) in the 5\u00a0GHz band, more spatial streams (up to eight versus four), higher-order modulation (up to 256-QAM vs. 64-QAM), and the addition of Multi-user MIMO (MU-MIMO). As of October 2013, high-end implementations support 80\u00a0MHz channels, three spatial streams, and 256-QAM, yielding a data rate of up to 433.3\u00a0Mbit/s per spatial stream, 1300\u00a0Mbit/s total, in 80\u00a0MHz channels in the 5\u00a0GHz band.[22] Vendors have announced plans to release so-called \"Wave 2\" devices with support for 160\u00a0MHz channels, four spatial streams, and MU-MIMO in 2014 and 2015.[23][24][25] IEEE 802.11ad is an amendment that defines a new physical layer for 802.11 networks to operate in the 60\u00a0GHz millimeter wave spectrum. This frequency band has significantly different propagation characteristics than the 2.4\u00a0GHz and 5\u00a0GHz bands where Wi-Fi networks operate. Products implementing the 802.11ad standard are being brought to market under the WiGig brand name. The certification program is now being developed by the Wi-Fi Alliance instead of the now defunct WiGig Alliance.[26] The peak transmission rate of 802.11ad is 7\u00a0Gbit/s.[27] TP-Link announced the world's first 802.11ad router in January 2016.[28] IEEE 802.11af, also referred to as \"White-Fi\" and \"Super Wi-Fi\",[29] is an amendment, approved in February 2014, that allows WLAN operation in TV white space spectrum in the VHF and UHF bands between 54 and 790\u00a0MHz.[30][31] It uses cognitive radio technology to transmit on unused TV channels, with the standard taking measures to limit interference for primary users, such as analog TV, digital TV, and wireless microphones.[31] Access points and stations determine their position using a satellite positioning system such as GPS, and use the Internet to query a geolocation database (GDB) provided by a regional regulatory agency to discover what frequency channels are available for use at a given time and position.[31] The physical layer uses OFDM and is based on 802.11ac.[32] The propagation path loss as well as the attenuation by materials such as brick and concrete is lower in the UHF and VHF bands than in the 2.4 and 5\u00a0GHz bands, which increases the possible range.[31] The frequency channels are 6 to 8\u00a0MHz wide, depending on the regulatory domain.[31] Up to four channels may be bonded in either one or two contiguous blocks.[31] MIMO operation is possible with up to four streams used for either space\u2013time block code (STBC) or multi-user (MU) operation.[31] The achievable data rate per spatial stream is 26.7\u00a0Mbit/s for 6 and 7\u00a0MHz channels, and 35.6\u00a0Mbit/s for 8\u00a0MHz channels.[33] With four spatial streams and four bonded channels, the maximum data rate is 426.7\u00a0Mbit/s for 6 and 7\u00a0MHz channels and 568.9\u00a0Mbit/s for 8\u00a0MHz channels.[33] IEEE 802.11ah defines a WLAN system operating at sub-1\u00a0GHz license-exempt bands, with final approval slated for September 2016.[30][34] Due to the favorable propagation characteristics of the low frequency spectra, 802.11ah can provide improved transmission range compared with the conventional 802.11 WLANs operating in the 2.4\u00a0GHz and 5\u00a0GHz\u00a0bands. 802.11ah can be used for various purposes including large scale sensor networks,[35] extended range hotspot, and outdoor Wi-Fi for cellular traffic offloading, whereas the available bandwidth is relatively narrow. The protocol intends consumption to be competitive with low power Bluetooth, at a much wider range.[36] IEEE 802.11ai is an amendment to the 802.11 standard that added new mechanisms for a faster initial link setup time.[37] IEEE 802.11aj is a rebanding of 802.11ad for use in the 45\u00a0GHz unlicensed spectrum available in some regions of the world (specifically China).[37] IEEE 802.11aq is an amendment to the 802.11 standard that will enable pre-association discovery of services. This extends some of the mechanisms in 802.11u that enabled device discovery to further discover the services running on a device, or provided by a network.[37] IEEE 802.11ax is the successor to 802.11ac, and will increase the efficiency of WLAN networks. Currently in development, this project has the goal of providing 4x the throughput of 802.11ac.[38] IEEE 802.11ay is a standard that is being developed. It is an amendment that defines a new physical layer for 802.11 networks to operate in the 60\u00a0GHz millimeter wave spectrum. It will be an extension of the existing 11ad, aimed to extend the throughput, range and use-cases. The main use-cases include: indoor operation, out-door back-haul and short range communications. The peak transmission rate of 802.11ay is 20\u00a0Gbit/s.[39] The main extensions include: channel bonding (2, 3 and 4), MIMO and higher modulation schemes. IEEE 802.11-2016 is a revision based on IEEE 802.11-2012, incorporating 5 amendments (11ae, 11aa, 11ad, 11ac, 11af). In addition, existing MAC and PHY functions have been enhanced and obsolete features were removed or marked for removal. Some clauses and annexes have been renumbered.[40] Across all variations of 802.11, maximum achievable throughputs are given either based on measurements under ideal conditions or in the layer-2 data rates. This, however, does not apply to typical deployments in which data is being transferred between two endpoints, of which at least one is typically connected to a wired infrastructure and the other endpoint is connected to an infrastructure via a wireless link. This means that, typically, data frames pass an 802.11 (WLAN) medium, and are being converted to 802.3 (Ethernet) or vice versa. Due to the difference in the frame (header) lengths of these two media, the application's packet size determines the speed of the data transfer. This means applications that use small packets (e.g., VoIP) create dataflows with high-overhead traffic (i.e., a low goodput). Other factors that contribute to the overall application data rate are the speed with which the application transmits the packets (i.e., the data rate) and, of course, the energy with which the wireless signal is received. The latter is determined by distance and by the configured output power of the communicating devices.[41][42] The same references apply to the attached graphs that show measurements of UDP throughput. Each represents an average (UDP) throughput (please note that the error bars are there, but barely visible due to the small variation) of 25 measurements. Each is with a specific packet size (small or large) and with a specific data rate (10\u00a0kbit/s \u2013 100\u00a0Mbit/s). Markers for traffic profiles of common applications are included as well. These figures assume there are no packet errors, which if occurring will lower transmission rate further. 802.11b, 802.11g, and 802.11n-2.4 utilize the 2.400\u20132.500 GHz spectrum, one of the ISM bands. 802.11a and 802.11n use the more heavily regulated 4.915\u20135.825 GHz band. These are commonly referred to as the \"2.4\u00a0GHz and 5\u00a0GHz bands\" in most sales literature. Each spectrum is sub-divided into channels with a center frequency and bandwidth, analogous to the way radio and TV broadcast bands are sub-divided. The 2.4\u00a0GHz band is divided into 14 channels spaced 5\u00a0MHz apart, beginning with channel\u00a01, which is centered on 2.412\u00a0GHz. The latter channels have additional restrictions or are unavailable for use in some regulatory domains. The channel numbering of the 5.725\u20135.875 GHz spectrum is less intuitive due to the differences in regulations between countries. These are discussed in greater detail on the list of WLAN channels. In addition to specifying the channel center frequency, 802.11 also specifies (in Clause 17) a spectral mask defining the permitted power distribution across each channel. The mask requires the signal be attenuated a minimum of 20\u00a0dB from its peak amplitude at \u00b111\u00a0MHz from the centre frequency, the point at which a channel is effectively 22\u00a0MHz wide. One consequence is that stations can use only every fourth or fifth channel without overlap. Availability of channels is regulated by country, constrained in part by how each country allocates radio spectrum to various services. At one extreme, Japan permits the use of all 14 channels for 802.11b, and 1\u201313 for 802.11g/n-2.4. Other countries such as Spain initially allowed only channels 10 and 11, and France allowed only 10, 11, 12, and 13; however, they now allow channels 1 through 13.[43][44] North America and some Central and South American countries allow only 1 through 11. Since the spectral mask defines only power output restrictions up to \u00b111\u00a0MHz from the center frequency to be attenuated by \u221250\u00a0dBr, it is often assumed that the energy of the channel extends no further than these limits. It is more correct to say that, given the separation between channels, the overlapping signal on any channel should be sufficiently attenuated to minimally interfere with a transmitter on any other channel. Due to the near-far problem a transmitter can impact (desense) a receiver on a \"non-overlapping\" channel, but only if it is close to the victim receiver (within a meter) or operating above allowed power levels. Confusion often arises over the amount of channel separation required between transmitting devices. 802.11b was based on DSSS modulation and utilized a channel bandwidth of 22\u00a0MHz, resulting in three \"non-overlapping\" channels (1, 6, and 11). 802.11g was based on OFDM modulation and utilized a channel bandwidth of 20\u00a0MHz. This occasionally leads to the belief that four \"non-overlapping\" channels (1, 5, 9, and 13) exist under 802.11g, although this is not the case as per 17.4.6.3 Channel Numbering of operating channels of the IEEE Std 802.11 (2012), which states \"In a multiple cell network topology, overlapping and/or adjacent cells using different channels can operate simultaneously without interference if the distance between the center frequencies is at least 25\u00a0MHz.\"[45] and section 18.3.9.3 and Figure 18-13. This does not mean that the technical overlap of the channels recommends the non-use of overlapping channels. The amount of interference seen on a configuration using channels 1, 5, 9, and 13 can have very small difference from a three-channel configuration,[46] and in the paper entitled \"Effect of adjacent-channel interference in IEEE 802.11 WLANs\" by Villegas this is also demonstrated.[47] Although the statement that channels 1, 5, 9, and 13 are \"non-overlapping\" is limited to spacing or product density, the concept has some merit in limited circumstances. Special care must be taken to adequately space AP cells, since overlap between the channels may cause unacceptable degradation of signal quality and throughput.[48] If more advanced equipment such as spectral analyzers are available, overlapping channels may be used under certain circumstances. This way, more channels are available.[47] IEEE uses the phrase regdomain to refer to a legal regulatory region. Different countries define different levels of allowable transmitter power, time that a channel can be occupied, and different available channels.[49] Domain codes are specified for the United States, Canada, ETSI (Europe), Spain, France, Japan, and China. Most Wi-Fi certified devices default to regdomain 0, which means least common denominator settings, i.e., the device will not transmit at a power above the allowable power in any nation, nor will it use frequencies that are not permitted in any nation.[citation needed] The regdomain setting is often made difficult or impossible to change so that the end users do not conflict with local regulatory agencies such as the United States' Federal Communications Commission. The datagrams are called frames. Current 802.11 standards specify frame types for use in transmission of data as well as management and control of wireless links. Frames are divided into very specific and standardized sections. Each frame consists of a MAC header, payload, and frame check sequence (FCS). Some frames may not have a payload. The first two bytes of the MAC header form a frame control field specifying the form and function of the frame. This frame control field is subdivided into the following sub-fields: The next two bytes are reserved for the Duration ID field. This field can take one of three forms: Duration, Contention-Free Period (CFP), and Association ID (AID). An 802.11 frame can have up to four address fields. Each field can carry a MAC address. Address 1 is the receiver, Address 2 is the transmitter, Address 3 is used for filtering purposes by the receiver.[dubious \u2013 discuss] The remaining fields of the header are: The payload or frame body field is variable in size, from 0 to 2304 bytes plus any overhead from security encapsulation, and contains information from higher layers. The Frame Check Sequence (FCS) is the last four bytes in the standard 802.11 frame. Often referred to as the Cyclic Redundancy Check (CRC), it allows for integrity check of retrieved frames. As frames are about to be sent, the FCS is calculated and appended. When a station receives a frame, it can calculate the FCS of the frame and compare it to the one received. If they match, it is assumed that the frame was not distorted during transmission.[50] Management frames allow for the maintenance of communication. Some common 802.11 subtypes include: 2. In terms of ICT, an Information Element (IE) is a part of management frames in the IEEE 802.11 wireless LAN protocol. IEs are a device's way to transfer descriptive information about itself inside management frames. There are usually several IEs inside each such frame, and each is built of TLVs mostly defined outside the basic IEEE 802.11 specification. The common structure of an IE is as follows: Whereas the OUI (organizationally unique identifier) is used only when necessary to the protocol being used, and the data field holds the TLVs relevant to that IE. Control frames facilitate in the exchange of data frames between stations. Some common 802.11 control frames include: Data frames carry packets from web pages, files, etc. within the body.[51] The body begins with an IEEE 802.2 header, with the Destination Service Access Point (DSAP) specifying the protocol; however, if the DSAP is hex AA, the 802.2 header is followed by a Subnetwork Access Protocol (SNAP) header, with the Organizationally Unique Identifier (OUI) and protocol ID (PID) fields specifying the protocol. If the OUI is all zeroes, the protocol ID field is an EtherType value.[52] Almost all 802.11 data frames use 802.2 and SNAP headers, and most use an OUI of 00:00:00 and an EtherType value. Similar to TCP congestion control on the internet, frame loss is built into the operation of 802.11. To select the correct transmission speed or Modulation and Coding Scheme, a rate control algorithm may test different speeds. The actual packet loss rate of an Access points vary widely for different link conditions. There are variations in the loss rate experienced on production Access points, between 10% and 80%, with 30% being a common average.[53] It is important to be aware that the link layer should recover these lost frames. If the sender does not receive an Acknowledgement (ACK) frame, then it will be resent. Within the IEEE 802.11 Working Group,[30] the following IEEE Standards Association Standard and Amendments exist: 802.11F and 802.11T are recommended practices rather than standards, and are capitalized as such. 802.11m is used for standard maintenance. 802.11ma was completed for 802.11-2007, 802.11mb for 802.11-2012, and 802.11mc for 802.11-2016. Both the terms \"standard\" and \"amendment\" are used when referring to the different variants of IEEE standards.[56] As far as the IEEE Standards Association is concerned, there is only one current standard; it is denoted by IEEE 802.11 followed by the date that it was published. IEEE 802.11-2016 is the only version currently in publication, superseding previous releases. The standard is updated by means of amendments. Amendments are created by task groups (TG). Both the task group and their finished document are denoted by 802.11 followed by a non-capitalized letter, for example, IEEE 802.11a and IEEE 802.11b. Updating 802.11 is the responsibility of task group m. In order to create a new version, TGm combines the previous version of the standard and all published amendments. TGm also provides clarification and interpretation to industry on published documents. New versions of the IEEE 802.11 were published in 1999, 2007, 2012, and 2016.[57] Various terms in 802.11 are used to specify aspects of wireless local-area networking operation, and may be unfamiliar to some readers. For example, Time Unit (usually abbreviated TU) is used to indicate a unit of time equal to 1024 microseconds. Numerous time constants are defined in terms of TU (rather than the nearly equal millisecond). Also the term \"Portal\" is used to describe an entity that is similar to an 802.1H bridge. A Portal provides access to the WLAN by non-802.11 LAN STAs. With the proliferation of cable modems and DSL, there is an ever-increasing market of people who wish to establish small networks in their homes to share their broadband Internet connection. Many hotspot or free networks frequently allow anyone within range, including passersby outside, to connect to the Internet. There are also efforts by volunteer groups to establish wireless community networks to provide free wireless connectivity to the public. In 2001, a group from the University of California, Berkeley presented a paper describing weaknesses in the 802.11 Wired Equivalent Privacy (WEP) security mechanism defined in the original standard; they were followed by Fluhrer, Mantin, and Shamir's paper titled \"Weaknesses in the Key Scheduling Algorithm of RC4\". Not long after, Adam Stubblefield and AT&T publicly announced the first verification of the attack. In the attack, they were able to intercept transmissions and gain unauthorized access to wireless networks. The IEEE set up a dedicated task group to create a replacement security solution, 802.11i (previously this work was handled as part of a broader 802.11e effort to enhance the MAC layer). The Wi-Fi Alliance announced an interim specification called Wi-Fi Protected Access (WPA) based on a subset of the then current IEEE 802.11i draft. These started to appear in products in mid-2003. IEEE 802.11i (also known as WPA2) itself was ratified in June 2004, and uses the Advanced Encryption Standard AES, instead of RC4, which was used in WEP. The modern recommended encryption for the home/consumer space is WPA2 (AES Pre-Shared Key), and for the enterprise space is WPA2 along with a RADIUS authentication server (or another type of authentication server) and a strong authentication method such as EAP-TLS. In January 2005, the IEEE set up yet another task group \"w\" to protect management and broadcast frames, which previously were sent unsecured. Its standard was published in 2009.[58] In December 2011, a security flaw was revealed that affects some wireless routers with a specific implementation of the optional Wi-Fi Protected Setup (WPS) feature. While WPS is not a part of 802.11, the flaw allows an attacker within the range of the wireless router to recover the WPS PIN and, with it, the router's 802.11i password in a few hours.[59][60] In late 2014, Apple announced that its iOS\u00a08 mobile operating system would scramble MAC addresses[61] during the pre-association stage to thwart retail footfall tracking made possible by the regular transmission of uniquely identifiable probe requests. Many companies implement wireless networking equipment with non-IEEE standard 802.11 extensions either by implementing proprietary or draft features. These changes may lead to incompatibilities between these extensions.[citation needed]"}, "AAA": {"link": "https://en.wikipedia.org/wiki/AAA_protocol", "full_form": "Authentication Authorization, Accounting", "content": "The Diameter protocol refers to a security architecture for distributed systems for controlling which users are allowed access to which services, and tracking which resources they have used. Two network protocols providing this functionality are particularly popular: the RADIUS protocol,[1] and its newer Diameter counterpart.[2][3] Diameter is a protocol within the family of AAA Protocols. Diameter uses the URI scheme; AAA stands for Authentication, Authorization and Accounting, AAAS stands for Authentication, Authorization and Accounting with Secure Transport, which is a Diameter-based Protocol.[4] These protocols were defined by the Internet Engineering Task Force in RFC 6733 and are intended to provide an Authentication, Authorization, and Accounting (AAA) framework for applications such as network access or IP mobility in both local and roaming situations.[5]   AAA servers in CDMA data networks are entities that provide Internet Protocol (IP) functionality to support the functions of authentication, authorization and accounting. The AAA server in the CDMA wireless data network architecture is similar to the HLR in the CDMA wireless voice network architecture. Types of AAA servers\u00a0: Current AAA servers communicate using the RADIUS protocol. As such, TIA specifications refer to AAA servers as RADIUS servers. However, future AAA servers are expected to use a successor protocol to RADIUS known as Diameter.[citation needed] The behavior of AAA servers (radius servers) in the CDMA2000 wireless IP network is specified in TIA-835."}, "AABB": {"link": "https://en.wikipedia.org/wiki/Minimum_bounding_box", "full_form": "Axis Aligned Bounding Box", "content": "In geometry, the minimum or smallest bounding or enclosing box for a point set (S) in N dimensions is the box with the smallest measure (area, volume, or hypervolume in higher dimensions) within which all the points lie. When other kinds of measure are used, the minimum box is usually called accordingly, e.g., \"minimum-perimeter bounding box\". The minimum bounding box of a point set is the same as the minimum bounding box of its convex hull, a fact which may be used heuristically to speed up computation.[1] The term \"box\"/\"hyperrectangle\" comes from its usage in the Cartesian coordinate system, where it is indeed visualized as a rectangle (two-dimensional case), rectangular parallelepiped (three-dimensional case), etc. In the two-dimensional case it is called the minimum bounding rectangle.   The axis-aligned minimum bounding box (or AABB) for a given point set is its minimum bounding box subject to the constraint that the edges of the box are parallel to the (Cartesian) coordinate axes. It is simply the Cartesian product of N intervals each of which is defined by the minimal and maximal value of the corresponding coordinate for the points in S. Axis-aligned minimal bounding boxes are used to an approximate location of an object in question and as a very simple descriptor of its shape. For example, in computational geometry and its applications when it is required to find intersections in the set of objects, the initial check is the intersections between their MBBs. Since it is usually a much less expensive operation than the check of the actual intersection (because it only requires comparisons of coordinates), it allows to quickly exclude from checks the pairs that are far apart. The arbitrarily oriented minimum bounding box is the minimum bounding box, calculated subject to no constraints as to the orientation of the result. Minimum bounding box algorithms based on the rotating calipers method can be used to find the minimum-area or minimum-perimeter bounding box of a two-dimensional convex polygon in linear time, and of a two-dimensional point set in the time it takes to construct its convex hull followed by a linear-time computation.[1] A three-dimensional rotating calipers algorithm can find the minimum-volume arbitrarily-oriented bounding box of a three-dimensional point set in cubic time.[2] In the case where an object has its own local coordinate system, it can be useful to store a bounding box relative to these axes, which requires no transformation as the object's own transformation changes. In digital image processing, the bounding box is merely the coordinates of the rectangular border that fully encloses a digital image when it is placed over a page, a canvas, a screen or other similar bidimensional background."}, "AAC": {"link": "https://en.wikipedia.org/wiki/Advanced_Audio_Coding", "full_form": "Advanced Audio Coding", "content": "MPEG/3GPP container Apple container Raw stream Advanced Audio Coding (AAC) is a proprietary audio coding standard for lossy digital audio compression. Designed to be the successor of the MP3 format, AAC generally achieves better sound quality than MP3 at the same bit rate.[2] The confusingly named AAC+ (HE-AAC) does so only at low bit rates and less so at high ones. AAC has been standardized by ISO and IEC, as part of the MPEG-2 and MPEG-4 specifications.[3][4] Part of AAC, HE-AAC (AAC+), is part of MPEG-4 Audio and also adopted into digital radio standards DAB+ and Digital Radio Mondiale, as well as mobile television standards DVB-H and ATSC-M/H. AAC supports inclusion of 48 full-bandwidth (up to 96\u00a0kHz) audio channels in one stream plus 16 low frequency effects (LFE, limited to 120\u00a0Hz) channels, up to 16 \"coupling\" or dialog channels, and up to 16 data streams. The quality for stereo is satisfactory to modest requirements at 96\u00a0kbit/s in joint stereo mode; however, hi-fi transparency demands data rates of at least 128\u00a0kbit/s (VBR). Tests of MPEG-4 audio have shown that AAC meets the requirements referred to as \"transparent\" for the ITU at 128\u00a0kbit/s for stereo, and 320\u00a0kbit/s for 5.1 audio. AAC is the default or standard audio format for YouTube, iPhone, iPod, iPad, Nintendo DSi, Nintendo 3DS, iTunes, DivX Plus Web Player, PlayStation 3 and various Nokia Series 40 phones. It is supported on PlayStation Vita, Wii (with the Photo Channel 1.1 update installed), Sony Walkman MP3 series and later, Android and BlackBerry. AAC is also supported by manufacturers of in-dash car audio systems.[when?][vague]   AAC was developed with the cooperation and contributions of companies including AT&T Bell Laboratories, Fraunhofer IIS, Dolby Laboratories, Sony Corporation and Nokia. It was officially declared an international standard by the Moving Picture Experts Group in April 1997. It is specified both as Part 7 of the MPEG-2 standard, and Subpart 4 in Part 3 of the MPEG-4 standard.[5] In 1997, AAC was first introduced as MPEG-2 Part 7, formally known as ISO/IEC 13818-7:1997. This part of MPEG-2 was a new part, since MPEG-2 already included MPEG-2 Part 3, formally known as ISO/IEC 13818-3: MPEG-2 BC (Backwards Compatible).[6][7] Therefore, MPEG-2 Part 7 is also known as MPEG-2 NBC (Non-Backward Compatible), because it is not compatible with the MPEG-1 audio formats (MP1, MP2 and MP3).[6][8][9][10] MPEG-2 Part 7 defined three profiles: Low-Complexity profile (AAC-LC / LC-AAC), Main profile (AAC Main) and Scalable Sampling Rate profile (AAC-SSR). AAC-LC profile consists of a base format very much like AT&T's Perceptual Audio Coding (PAC) coding format,[11][12][13] with the addition of temporal noise shaping (TNS),[14] the Dolby Kaiser Window (described below), a nonuniform quantizer, and a reworking of the bitstream format to handle up to 16 stereo channels, 16 mono channels, 16 low-frequency effect (LFE) channels and 16 commentary channels in one bitstream. The Main profile adds a set of recursive predictors that are calculated on each tap of the filterbank. The SSR uses a 4-band PQMF filterbank, with four shorter filterbanks following, in order to allow for scalable sampling rates. In 1999, MPEG-2 Part 7 was updated and included in the MPEG-4 family of standards and became known as MPEG-4 Part 3, MPEG-4 Audio or ISO/IEC 14496-3:1999. This update included several improvements. One of these improvements was the addition of Audio Object Types which are used to allow interoperability with a diverse range of other audio formats such as TwinVQ, CELP, HVXC, Text-To-Speech Interface and MPEG-4 Structured Audio. Another notable addition in this version of the AAC standard is Perceptual Noise Substitution (PNS). In that regard, the AAC profiles (AAC-LC, AAC Main and AAC-SSR profiles) are combined with perceptual noise substitution and are defined in the MPEG-4 audio standard as Audio Object Types.[15] MPEG-4 Audio Object Types are combined in four MPEG-4 Audio profiles: Main (which includes most of the MPEG-4 Audio Object Types), Scalable (AAC LC, AAC LTP, CELP, HVXC, TwinVQ, Wavetable Synthesis, TTSI), Speech (CELP, HVXC, TTSI) and Low Rate Synthesis (Wavetable Synthesis, TTSI).[16][17] The reference software for MPEG-4 Part 3 is specified in MPEG-4 Part 5 and the conformance bit-streams are specified in MPEG-4 Part 4. MPEG-4 Audio remains backward-compatible with MPEG-2 Part 7.[18] The MPEG-4 Audio Version 2 (ISO/IEC 14496-3:1999/Amd 1:2000) defined new audio object types: the low delay AAC (AAC-LD) object type, bit-sliced arithmetic coding (BSAC) object type, parametric audio coding using harmonic and individual line plus noise and error resilient (ER) versions of object types.[19][20][21] It also defined four new audio profiles: High Quality Audio Profile, Low Delay Audio Profile, Natural Audio Profile and Mobile Audio Internetworking Profile.[22] The HE-AAC Profile (AAC LC with SBR) and AAC Profile (AAC LC) were first standardized in ISO/IEC 14496-3:2001/Amd 1:2003.[23] The HE-AAC v2 Profile (AAC LC with SBR and Parametric Stereo) was first specified in ISO/IEC 14496-3:2005/Amd 2:2006.[24][25][26] The Parametric Stereo audio object type used in HE-AAC v2 was first defined in ISO/IEC 14496-3:2001/Amd 2:2004.[27][28][29] The current version of the AAC standard is defined in ISO/IEC 14496-3:2009.[30] AAC+ v2 is also standardized by ETSI (European Telecommunications Standards Institute) as TS 102005.[27] The MPEG-4 Part 3 standard also contains other ways of compressing sound. These include lossless compression formats, synthetic audio and low bit-rate compression formats generally used for speech. Advanced Audio Coding is designed to be the successor of the MPEG-1 Audio Layer 3, known as MP3 format, which was specified by ISO/IEC in 11172-3 (MPEG-1 Audio) and 13818-3 (MPEG-2 Audio). Blind tests in the late 1990s showed that AAC demonstrated greater sound quality and transparency than MP3 for files coded at the same bit rate,[2] but since that time numerous codec listening tests have shown that the best encoders in each format are often of similar quality (statistically tied)[citation needed] and that the quality is often dependent on the encoder used even within the same format. As an approximation, when using the best encoders, AAC's advantage over MP3 tends to be evident below around 100 kbit/s, but certain AAC encoders are not as good as the best MP3 encoder as they do not take optimal advantage of the additional encoding tools that AAC makes available. Improvements include: Overall, the AAC format allows developers more flexibility to design codecs than MP3 does, and corrects many of the design choices made in the original MPEG-1 audio specification. This increased flexibility often leads to more concurrent encoding strategies and, as a result, to more efficient compression. However, in terms of whether AAC is better than MP3, the advantages of AAC are not entirely decisive, and the MP3 specification, although antiquated, has proven surprisingly robust in spite of considerable flaws. AAC and HE-AAC are better than MP3 at low bit rates (typically less than 128 kilobits per second.)[citation needed] This is especially true at very low bit rates where the superior stereo coding, pure MDCT, and better transform window sizes leave MP3 unable to compete. While the MP3 format has near-universal hardware and software support, primarily due to MP3 being the format of choice during the crucial first few years of widespread music file-sharing/distribution over the internet, AAC is a strong contender due to some unwavering industry support.[31] AAC is a wideband audio coding algorithm that exploits two primary coding strategies to dramatically reduce the amount of data needed to represent high-quality digital audio: The actual encoding process consists of the following steps: The MPEG-4 audio standard does not define a single or small set of highly efficient compression schemes but rather a complex toolbox to perform a wide range of operations from low bit rate speech coding to high-quality audio coding and music synthesis. AAC encoders can switch dynamically between a single MDCT block of length 1024 points or 8 blocks of 128 points (or between 960 points and 120 points, respectively). AAC takes a modular approach to encoding. Depending on the complexity of the bitstream to be encoded, the desired performance and the acceptable output, implementers may create profiles to define which of a specific set of tools they want to use for a particular application. The MPEG-2 Part 7 standard (Advanced Audio Coding) was first published in 1997 and offers three default profiles:[1][33] The MPEG-4 Part 3 standard (MPEG-4 Audio) defined various new compression tools (a.k.a. Audio Object Types) and their usage in brand new profiles. AAC is not used in some of the MPEG-4 Audio profiles. The MPEG-2 Part 7 AAC LC profile, AAC Main profile and AAC SSR profile are combined with Perceptual Noise Substitution and defined in the MPEG-4 Audio standard as Audio Object Types (under the name AAC LC, AAC Main and AAC SSR). These are combined with other Object Types in MPEG-4 Audio profiles.[15] Here is a list of some audio profiles defined in the MPEG-4 standard:[24][34] One of many improvements in MPEG-4 Audio is an Object Type called Long Term Prediction (LTP), which is an improvement of the Main profile using a forward predictor with lower computational complexity.[18] Applying error protection enables error correction up to a certain extent. Error correcting codes are usually applied equally to the whole payload. However, since different parts of an AAC payload show different sensitivity to transmission errors, this would not be a very efficient approach. The AAC payload can be subdivided into parts with different error sensitivities. Error Resilience (ER) techniques can be used to make the coding scheme itself more robust against errors. For AAC, three custom-tailored methods were developed and defined in MPEG-4 Audio The audio coding standards MPEG-4 Low Delay, Enhanced Low Delay and Enhanced Low Delay v2 (AAC-LD, AAC-ELD, AAC-ELDv2) as defined in ISO/IEC 14496-3:2009 and ISO/IEC 14496-3:2009/Amd 3 are designed to combine the advantages of perceptual audio coding with the low delay necessary for two-way communication. They are closely derived from the MPEG-2 Advanced Audio Coding (AAC) format. [36][37][38] AAC-ELD is recommended by GSMA as super-wideband voice codec in the IMS Profile for High Definition Video Conference (HDVC) Service.[39] No licenses or payments are required for a user to stream or distribute content in AAC format.[40] This reason alone can make AAC a much more attractive format to distribute content than its predecessor MP3, particularly for streaming content (such as Internet radio) depending on the use case. However, a patent license is required for all manufacturers or developers of AAC codecs.[41] For this reason, free and open source software implementations such as FFmpeg and FAAC may be distributed in source form only, in order to avoid patent infringement. (See below under Products that support AAC, Software.) Some extensions have been added to the first AAC standard (defined in MPEG-2 Part 7 in 1997): In addition to the MP4, 3GP and other container formats based on ISO base media file format for file storage, AAC audio data was first packaged in a file for the MPEG-2 standard using Audio Data Interchange Format (ADIF),[43] consisting of a single header followed by the raw AAC audio data blocks.[44] However, if the data is to be streamed within an MPEG-2 transport stream, a self-synchronizing format called an Audio Data Transport Stream (ADTS) is used, consisting of a series of frames, each frame having a header followed by the AAC audio data.[43] This file and streaming-based format are defined in MPEG-2 Part 7, but are only considered informative by MPEG-4, so an MPEG-4 decoder does not need to support either format.[43] These containers, as well as a raw AAC stream, may bear the .aac file extension. MPEG-4 Part 3 also defines its own self-synchronizing format called a Low Overhead Audio Stream (LOAS) that encapsulates not only AAC, but any MPEG-4 audio compression scheme such as TwinVQ and ALS. This format is what was defined for use in DVB transport streams when encoders use either SBR or parametric stereo AAC extensions. However, it is restricted to only a single non-multiplexed AAC stream. This format is also referred to as a Low Overhead Audio Transport Multiplex (LATM), which is just an interleaved multiple stream version of a LOAS.[43] In December 2003, Japan started broadcasting terrestrial DTV ISDB-T standard that implements MPEG-2 video and MPEG-2 AAC audio. In April 2006 Japan started broadcasting the ISDB-T mobile sub-program, called 1seg, that was the first implementation of video H.264/AVC with audio HE-AAC in Terrestrial HDTV broadcasting service on the planet. In December 2007, Brazil started broadcasting terrestrial DTV standard called International ISDB-Tb that implements video coding H.264/AVC with audio AAC-LC on main program (single or multi) and video H.264/AVC with audio HE-AACv2 in the 1seg mobile sub-program. The ETSI, the standards governing body for the DVB suite, supports AAC, HE-AAC and HE-AAC v2 audio coding in DVB applications since at least 2004.[45] DVB broadcasts which use the H.264 compression for video normally use HE-AAC for audio.[citation needed] In April 2003, Apple brought mainstream attention to AAC by announcing that its iTunes and iPod products would support songs in MPEG-4 AAC format (via a firmware update for older iPods). Customers could download music in a closed-source Digital Rights Management (DRM)-restricted form of AAC (see FairPlay) via the iTunes Store or create files without DRM from their own CDs using iTunes. In later years, Apple began offering music videos and movies, which also use AAC for audio encoding. On May 29, 2007, Apple began selling songs and music videos free of DRM from participating record labels. These files mostly adhere to the AAC standard and are playable on many non-Apple products but they do include custom iTunes information such as album artwork and a purchase receipt, so as to identify the customer in case the file is leaked out onto peer-to-peer networks. It is possible, however, to remove these custom tags to restore interoperability with players that conform strictly to the AAC specification.[citation needed] As of January 6, 2009, nearly all music on the USA regioned iTunes Store became DRM-free, with the remainder becoming DRM-free by the end of March 2009.[46] iTunes supports a \"Variable Bit Rate\" (VBR) encoding option which encodes AAC tracks in an \"Average Bit Rate\" (ABR) scheme.[citation needed] As of September 2009, Apple has added support for HE-AAC (which is fully part of the MP4 standard) only for radio streams, not file playback, and iTunes still lacks support for true VBR encoding. The underlying QuickTime API does offer a true VBR encoding profile however. For a number of years, many mobile phones from manufacturers such as Nokia, Motorola, Samsung, Sony Ericsson, BenQ-Siemens and Philips have supported AAC playback. The first such phone was the Nokia 5510 released in 2002 which also plays MP3s. However, this phone was a commercial failure[citation needed] and such phones with integrated music players did not gain mainstream popularity until 2005 when the trend of having AAC as well as MP3 support continued. Most new smartphones and music-themed phones support playback of these formats. Almost all current computer media players include built-in decoders for AAC, or can utilize a library to decode it. On Microsoft Windows, DirectShow can be used this way with the corresponding filters to enable AAC playback in any DirectShow based player. Mac OS X supports AAC via the QuickTime libraries. Adobe Flash Player, since version 9 update 3, can also play back AAC streams.[51][52] Since Flash Player is also a browser plugin, it can play AAC files through a browser as well. The Rockbox open source firmware (available for multiple portable players) also offers support for AAC to varying degrees, depending on the model of player and the AAC profile. Optional iPod support (playback of unprotected AAC files) for the Xbox 360 is available as a free download from Xbox Live.[53] The following is a non-comprehensive list of other software player applications: Some of these players (e.g., foobar2000, Winamp, and VLC) also support the decoding of ADTS (Audio Data Transport Stream) using the SHOUTcast protocol. Plug-ins for Winamp and foobar2000 enable the creation of such streams. In May 2006, Nero AG released an AAC encoding tool free of charge, Nero Digital Audio (the AAC codec portion has become Nero AAC Codec),[54] which is capable of encoding LC-AAC, HE-AAC and HE-AAC v2 streams. The tool is a Command Line Interface tool only. A separate utility is also included to decode to PCM WAV. Various tools including the foobar2000 audio player, MediaCoder, MeGUI encoding front end and dBpoweramp can provide a GUI for this encoder. FAAC and FAAD2 stand for Freeware Advanced Audio Coder and Decoder 2 respectively. FAAC supports audio object types LC, Main and LTP.[55] FAAD2 supports audio object types LC, Main, LTP, SBR and PS.[56] Although FAAD2 is free software, FAAC is not free software. A Fraunhofer-authored open-source encoder/decoder included in Android has been ported to other platforms. It is the recommended AAC encoder of FFmpeg. The native AAC encoder created in FFmpeg's libavcodec, and forked with Libav, was considered experimental and poor. A significant amount of work was done for the 3.0 release of FFmpeg (February 2016) to make its version usable and competitive with the rest of the AAC encoders.[57] Libav has not merged this work and continues to use the older version of the AAC encoder. These encoders are LGPL-licensed open-source and can be built for any platform that the FFmpeg or Libav frameworks can be built. Both FFmpeg and Libav can use the Fraunhofer FDK AAC library via libfdk-aac, and while the FFmpeg native encoder has become stable and good enough for common use, FDK is still considered the highest quality encoder available for use with FFmpeg.[58] Libav also recommends using FDK AAC if it is available.[59]"}, "AAL": {"link": "https://en.wikipedia.org/wiki/ATM_adaptation_layer", "full_form": "ATM Adaptation Layer", "content": "The use of Asynchronous Transfer Mode (ATM) technology and services creates the need for an adaptation layer in order to support information transfer protocols, which are not based on ATM. This adaptation layer defines how to segment and reassemble higher-layer packets into ATM cells, and how to handle various transmission aspects in the ATM layer. Examples of services that need adaptations are Gigabit Ethernet, IP, Frame Relay, SONET/SDH, UMTS/Wireless, etc. only optical fibers can be used. The main services provided by AAL (ATM Adaptation Layer) are: The following ATM Adaptation Layer protocols (AALs) have been defined by the ITU-T.[1] It is meant that these AALs will meet a variety of needs. The classification is based on whether a timing relationship must be maintained between source and destination, whether the application requires a constant bit rate, and whether the transfer is connection oriented or connectionless. AAL 5 was introduced to: The AAL 5 was designed to accommodate the same variable bit rate, connection-oriented asynchronous traffic or connectionless packet data supported by AAL 3/4, but without the segment tracking and error correction requirements."}, "AALC": {"link": "https://en.wikipedia.org/wiki/ATM_adaptation_layer", "full_form": "ATM Adaptation Layer Connection", "content": "The use of Asynchronous Transfer Mode (ATM) technology and services creates the need for an adaptation layer in order to support information transfer protocols, which are not based on ATM. This adaptation layer defines how to segment and reassemble higher-layer packets into ATM cells, and how to handle various transmission aspects in the ATM layer. Examples of services that need adaptations are Gigabit Ethernet, IP, Frame Relay, SONET/SDH, UMTS/Wireless, etc. only optical fibers can be used. The main services provided by AAL (ATM Adaptation Layer) are: The following ATM Adaptation Layer protocols (AALs) have been defined by the ITU-T.[1] It is meant that these AALs will meet a variety of needs. The classification is based on whether a timing relationship must be maintained between source and destination, whether the application requires a constant bit rate, and whether the transfer is connection oriented or connectionless. AAL 5 was introduced to: The AAL 5 was designed to accommodate the same variable bit rate, connection-oriented asynchronous traffic or connectionless packet data supported by AAL 3/4, but without the segment tracking and error correction requirements."}, "AARP": {"link": "https://en.wikipedia.org/wiki/AppleTalk", "full_form": "AppleTalk Address Resolution Protocol", "content": "AppleTalk was a proprietary suite of networking protocols developed by Apple Inc. for their Macintosh computers. AppleTalk includes a number of features that allow local area networks to be connected with no prior setup or the need for a centralized router or server of any sort. Connected AppleTalk-equipped systems automatically assign addresses, update the distributed namespace, and configure any required inter-networking routing. AppleTalk was released in 1985, and was the primary protocol used by Apple devices through the 1980s and 1990s. Versions were also released for the IBM PC and compatibles and the Apple IIGS. AppleTalk support was also available in most networked printers (especially laser printers), some file servers, and a number of routers. The rise of TCP/IP during the 1990s led to a reimplementation of most of these types of support on that protocol, and AppleTalk became unsupported as of the release of Mac OS X v10.6 in 2009. Many of AppleTalk's more advanced autoconfiguration features have since been introduced in Bonjour, while Universal Plug and Play serves similar needs.   After the release of the Apple Lisa computer in January 1983, Apple invested considerable effort in the development of a local area networking (LAN) system for the machines. Known as AppleNet, it was based on the seminal Xerox XNS protocol stack[1] but running on a custom 1\u00a0Mbit/s coaxial cable system rather than Xerox's 2.94 Mbit/s Ethernet. AppleNet was announced early in 1983 with a fall introduction at the target price of $500 for plug-in AppleNet cards for the Lisa and the Apple II.[2] At that time, early LAN systems were just coming to market, including Ethernet, Token Ring and ARCNET. This was a topic of major commercial effort at the time, dominating shows like the National Computer Conference (NCC) in Anaheim in May 1983. All of the systems were jockeying for position in the market, but even at this time Ethernet's widespread acceptance suggested it was to become a de facto standard.[3] It was at this show that Steve Jobs asked Gursharan Sidhu a seemingly innocuous question, \"Why has networking not caught on?\"[4] Four months later, in October, AppleNet was cancelled. At the time, they announced that \"Apple realized that it's not in the business to create a networking system. We built and used AppleNet in-house, but we realized that if we had shipped it, we would have seen new standards coming up.\"[5] In January, Jobs announced that they would instead be supporting IBM's Token Ring, which he expected to come out in a \"few months\".[5] Through this period, Apple was deep in development of the Macintosh computer. During development, engineers had made the decision to use the Zilog 8530 serial controller chip (SCC) instead of the lower cost and more common UART to provide serial port connections.[6] The SCC cost about $5 more than a UART, but offered much higher speeds up to 250\u00a0kilobits per second (or higher with additional hardware) and internally supported a number of basic networking-like protocols like IBM's Bisync.[7] The SCC was chosen because it would allow multiple devices to be attached to the port. Peripherals equipped with similar SCCs could communicate using the built-in protocols, interleaving their data with other peripherals on the same bus. This would eliminate the need for more ports on the back of the machine, and allowed for the elimination of expansion slots for supporting more complex devices. The initial concept was known as AppleBus, envisioning a system controlled by the host Macintosh polling \"dumb\" devices in a fashion similar to the modern Universal Serial Bus.[8] The Macintosh team had already begun work on what would become the LaserWriter, and had considered a number of other options of how to share these expensive machines and other resources. A series of memos from Bob Belleville clarified these concepts, outlining the Mac, LaserWriter and a file server system which would become Macintosh Office.[4] By late 1983 it was clear that IBM's Token Ring would not be ready in time for the launch of the Mac, and might miss the launch of these other products as well. In the end, Token Ring would not ship until October 1985.[9] Jobs' earlier question to Sidhu had already sparked a number of ideas. When AppleNet was cancelled in October, Sidhu led an effort to develop a new networking system based on the AppleBus hardware. This new system would not have to conform to any existing preconceptions, and was designed to be worthy of the Mac - a system that was user-installable, had zero-configuration, and no fixed network addresses - in short, a true plug-and-play network.[10][third-party source needed] Considerable effort was needed, but by the time the Mac was released, the basic concepts had been outlined, and some of the low-level protocols were on their way to completion. Sidhu mentioned the work to Belleville only two hours after the Mac was announced.[4] The \"new\" AppleBus was announced in early 1984,[N 1] allowing direct connection from the Mac or Lisa through a small box that plugged into the serial port and connected via cables to the next computer upstream and downstream. Adaptors for Apple II and Apple III were also announced.[11] Apple also announced that AppleBus networks could be attached to, and would appear to be a single node within, a Token Ring system.[5] Details of how this would work were sketchy.[5] Just prior to its release in early 1985, AppleBus was renamed AppleTalk. The system had a number of limitations, including a speed of only 230.4\u00a0kbit/s, a maximum distance of 1000\u00a0feet from end to end, and only 32 nodes per LAN.[12] But as the basic hardware was built into the Mac, adding nodes only cost about $50 for the adaptor box. In comparison, Ethernet or Token Ring cards cost hundreds or thousands of dollars. Additionally, the entire networking stack required only about 6\u00a0kB of RAM, allowing it to run on any Mac.[13] The relatively slow speed of AppleTalk allowed further reductions in cost. Instead of using RS-422's balanced transmit and receive circuits, the AppleTalk Personal Network cabling used a single common electrical ground, which limited speeds to about 500\u00a0kbit/s, but allowed one conductor to be removed. This meant that common three-conductor cables could be used for wiring. Additionally, the adaptors were designed to be \"self-terminating\", meaning that nodes at the end of the network could simply leave their last connector unconnected. There was no need for the wires to be connected back together into a loop, nor the need for hubs or other devices. The system was designed for future expansion; the addressing system allowed for expansion to 255 nodes in a LAN (although only 32 could be used at that time), and by using \"bridges\" (which came to be known as \"routers\", although technically not the same) one could interconnect LANs into larger collections. \"Zones\" allowed devices to be addressed within a bridge-connected internet. Additionally, AppleTalk was designed from the start to allow use with any potential underlying physical link.[14] The main advantage of AppleTalk was that it was completely maintenance-free. To join a device to a network, you simply plugged the adaptor into the machine, then connected a cable from it to any free port on any other adaptor. AppleTalk's internal protocols negotiated a working network address number, automatically gave the computer a human-readable name, and collected up a list of the names and types of other machines on the network so the user could browse the devices through the GUI-based Chooser. AppleTalk was so easy to use that ad-hoc networks tended to appear whenever multiple Macs were in the same room.[15] Apple would later use this in an advertisement showing a network being created between two seats in an airplane.[16] A thriving 3rd party market for AppleTalk devices developed over the next few years. One particularly notable example was an alternate adaptor designed by BMUG and commercialized by Farallon as PhoneNet in 1987.[17] This was essentially a replacement for Apple's connector that had conventional phone jacks instead of Apple's round connectors. PhoneNet allowed AppleTalk networks to be connected together using normal telephone wires, and with very little extra work, could run analog phones and AppleTalk on a single four-conductor phone cable. Other companies took advantage of the SCC's ability to read external clocks in order to support higher transmission speeds, up to 1\u00a0Mbit/s. In these systems the external adaptor also included its own clock, and used that to signal the SCC's clock input pins. The best known such system was Centram's FlashTalk, which ran at 768\u00a0kbit/s, and was intended to be used with their TOPS networking system.[18] A similar solution was the 850\u00a0kbit/s DaynaTalk, which used a separate box that plugged in between the computer and a normal LocalTalk/PhoneNet box. Dayna also offered a PC expansion card that ran up to 1.7\u00a0Mbit/s when talking to other Dayna PC cards.[19][20] Several other systems also existed with even higher performance, but these often required special cabling that was incompatible with LocalTalk/PhoneNet, and also required patches to the networking stack that often caused problems. By 1987 Ethernet was clearly winning the standards battle over Token Ring, and in the middle of that year Apple introduced EtherTalk 1.0 for the newly released Macintosh II computer. The package included both a NuBus card with Ethernet ports and a new Network control panel that allowed the user to select which physical connection to use for networking (from \"Built-in\" or \"EtherTalk\"). The release's new networking stack also expanded the system to allow a full 255 nodes per LAN. With its release, AppleTalk Personal Network was renamed LocalTalk.[21] Token Ring would eventually be supported with the similar TokenTalk product, which used the same Network control panel and underlying software. Many third party companies would introduce compatible Ethernet and Token Ring cards that used these same drivers. The appearance of EtherTalk also led to a problem: Networks with new and old Macs needed some way to communicate between each other. This could be as simple as a network of Ethernet Mac IIs trying to talk to a LaserWriter. Apple had considered the problem, and AppleTalk included the possibility for a low-cost LocalTalk-to-Ethernet bridge, but they felt it would be a low-volume product and left it to third parties.[22] A number of companies responded, both existing communications vendors like Hayes and Cisco Systems, as well as newly formed companies like Kinetics. Contrary to Apple's belief these would be low-volume, by the end of 1987, 130,000 such systems were in use. AppleTalk was at that time the most used networking system in the world, with over three times the installations of any other vendor.[23][third-party source needed] 1987 also marked the introduction of the AppleShare product, a dedicated file server that ran on any Mac with 512 kB of RAM or more. A common AppleShare machine was the Mac Plus with an external SCSI hard drive. AppleShare was the #3 network operating system in the late 1980s, behind Novell NetWare and Microsoft's MS-Net.[24] AppleShare was effectively the replacement for the failed Macintosh Office efforts, which had been based on a dedicated file server device. A significant re-design was released in 1989 as AppleTalk Phase II. In many ways, Phase II can be considered an effort to make the earlier version (never called Phase I) more generic. LANs could now support more than 255 nodes, and zones were no longer associated with physical networks, but were entirely virtual constructs used simply to organize nodes. For instance, one could now make a \"Printers\" zone that would list all the printers in an organization, or one might want to place that same device in the \"2nd Floor\" zone to indicate its physical location. Phase II also included changes to the underlying inter-networking protocols to make them less \"chatty\", which had previously been a serious problem on networks that bridged over wide-area networks.[25] By this point Apple had a wide variety of communications products under development, and many of these were announced along with AppleTalk Phase II. These included updates to EtherTalk and TokenTalk, AppleTalk software and LocalTalk hardware for the IBM PC, EtherTalk for Apple's A/UX operating system allowing it to use LaserPrinters and other network resources, and the Mac X.25 and MacX products. Ethernet had become almost universal by 1990, and it was time to build Ethernet into Macs direct from the factory. However, the physical wiring used by these networks was not yet completely standardized. Apple solved this problem using a single port on the back of the computer into which the user could plug an adaptor for any given cabling system. This FriendlyNet system was based on the industry-standard Attachment Unit Interface or AUI, but deliberately chose a non-standard connector that was smaller and easier to use, which they called \"Apple AUI\", or AAUI. FriendlyNet was first introduced on the Quadra 700 and Quadra 900 computers, and used across much of the Mac line for some time.[26] As with LocalTalk, a number of 3rd party FriendlyNet adaptors quickly appeared. As 10BASE-T became the de facto cabling system for Ethernet, second-generation Power Macintosh machines added a 10BASE-T port in addition to AAUI. The PowerBook 3400c and lower-end Power Macs also added 10BASE-T. The Power Macintosh 7300/8600/9600 were the final Macs to include AAUI, and 10BASE-T became universal starting with the Power Macintosh G3 and PowerBook G3. In 1988 Apple had released MacTCP, a system that allowed the Mac to support TCP/IP on machines with suitable Ethernet hardware. However, this left many universities with the problem of supporting IP on their many LocalTalk-equipped Macs. Stanford University pioneered development of MacIP, which allowed IP packets to be routed over LocalTalk networks with the support of a suitable \"gateway\" machine. These were initially custom devices, but it was soon common to include MacIP support in LocalTalk-to-Ethernet bridges.[26] MacTCP would not become a standard part of the Classic Mac OS until 1994,[27] by which time it also supported SNMP and PPP. For some time in the early 1990s, the Mac was a primary client on the rapidly expanding Internet.[citation needed] Among the better known programs in wide use were Fetch, Eudora, eXodus, NewsWatcher and the NCSA packages, especially NCSA Mosaic and its offspring, Netscape Navigator.[28] Additionally, a number of server products appeared that allowed the Mac to host Internet content. Through this period, Macs had about 2 to 3 times as many clients connected to the Internet as any other platform,[29][third-party source needed] despite the relatively small overall marketshare. As the world quickly moved to IP for both LAN and WAN uses, Apple was faced with maintaining two increasingly outdated code bases on an ever-wider group of machines as well as the introduction of the PowerPC based machines. This led to the Open Transport efforts, which re-implemented both MacTCP and AppleTalk on an entirely new code base adapted from the Unix standard STREAMS. Early versions had problems and did not become stable for some time.[30] By that point, Apple was deep in their ultimately doomed Copland efforts. With the purchase of NeXT and subsequent development of Mac OS X, AppleTalk was strictly a legacy system. Support was added to OS X in order to provide support for the large number of existing AppleTalk devices, notably laser printers and file shares, but alternate connection solutions common in this era, notably USB for printers, limited their demand. As Apple abandoned many of these product categories, and all new systems were based on IP, AppleTalk became less and less common. AppleTalk support was finally removed from the MacOS in Mac OS X v10.6 in 2009.[31] However, the loss of AppleTalk did not reduce the desire for networking solutions that combined its ease-of-use with IP routing. Apple has led development of many such efforts, from the introduction of the AirPort router to the development of the Zero configuration networking system and their implementation of it, Bonjour. The AppleTalk design rigorously followed the OSI model of protocol layering. Unlike most of the early LAN systems, AppleTalk was not built using the archetypal Xerox XNS system. The intended target was not Ethernet, and it did not have 48-bit addresses to route. Nevertheless, many portions of the AppleTalk system have direct analogs in XNS. One key differentiation for AppleTalk was it contained two protocols aimed at making the system completely self-configuring. The AppleTalk address resolution protocol (AARP) allowed AppleTalk hosts to automatically generate their own network addresses, and the Name Binding Protocol (NBP) was a dynamic system for mapping network addresses to user-readable names. Although systems similar to AARP existed in other systems, Banyan VINES for instance, nothing like NBP has existed until recently[citation needed]. Both AARP and NBP had defined ways to allow \"controller\" devices to override the default mechanisms. The concept was to allow routers to provide the information or \"hardwire\" the system to known addresses and names. On larger networks where AARP could cause problems as new nodes searched for free addresses, the addition of a router could reduce \"chattiness.\" Together AARP and NBP made AppleTalk an easy-to-use networking system. New machines were added to the network by plugging them and optionally giving them a name. The NBP lists were examined and displayed by a program known as the Chooser which would display a list of machines on the local network, divided into classes such as file-servers and printers. An AppleTalk address was a 4-byte quantity. This consisted of a two-byte network number, a one-byte node number, and a one-byte socket number. Of these, only the network number required any configuration, being obtained from a router. Each node dynamically chose its own node number, according to a protocol (originally the LocalTalk Link Access Protocol LLAP and later the AppleTalk Address Resolution Protocol, AARP)[32] which handled contention between different nodes accidentally choosing the same number. For socket numbers, a few well-known numbers were reserved for special purposes specific to the AppleTalk protocol itself. Apart from these, all application-level protocols were expected to use dynamically-assigned socket numbers at both the client and server end. Because of this dynamism, users could not be expected to access services by specifying their address. Instead, all services had names which, being chosen by humans, could be expected to be meaningful to users, and also could be sufficiently long to minimize the chance of conflicts. As NBP names translated to an address, which included a socket number as well as a node number, a name in AppleTalk mapped directly to a service being provided by a machine, which was entirely separate from the name of the machine itself. Thus, services could be moved to a different machine and, so long as they kept the same service name, there was no need for users to do anything different in order to continue accessing the service. And the same machine could host any number of instances of services of the same type, without any network connection conflicts. Contrast this with A records in the DNS, where a name translates to a machine's address, not including the port number that might be providing a service. Thus, if people are accustomed to using a particular machine name to access a particular service, their access will break when the service is moved to a different machine. This can be mitigated somewhat by insistence on using CNAME records indicating service rather than actual machine names to refer to the service, but there is no way of guaranteeing that users will follow such a convention. Some newer protocols, such as Kerberos and Active Directory use DNS SRV records to identify services by name, which is much closer to the AppleTalk model.[original research?] AARP resolves AppleTalk addresses to link layer, usually MAC, addresses. It is functionally equivalent to ARP. AARP is a fairly simple system. When powered on, an AppleTalk machine broadcasts an AARP probe packet asking for a network address, intending to hear back from controllers such as routers. If no address is provided, one is picked at random from the \"base subnet\", 0. It then broadcasts another packet saying \"I am selecting this address\", and then waits to see if anyone else on the network complains. If another machine has that address, it will pick another address, and keep trying until it finds a free one. On a network with many machines it may take several tries before a free address is found, so for performance purposes the successful address is \"written down\" in NVRAM and used as the default address in the future. This means that in most real-world setups where machines are added a few at a time, only one or two tries are needed before the address effectively become constant. This was a comparatively late addition to the AppleTalk protocol suite, done when it became clear that a TCP-style reliable connection-oriented transport was needed. Significant differences from TCP were: The Apple Filing Protocol (AFP), formerly AppleTalk Filing Protocol, is the protocol for communicating with AppleShare file servers. Built on top of AppleTalk Session Protocol (for legacy AFP over DDP) or the Data Stream Interface (for AFP over TCP), it provides services for authenticating users (extensible to different authentication methods including two-way random-number exchange) and for performing operations specific to the Macintosh HFS filesystem. AFP is still in use in macOS, even though most other AppleTalk protocols have been deprecated. ASP was an intermediate protocol, built on top of ATP, which in turn was the foundation of AFP. It provided basic services for requesting responses to arbitrary commands d performing out-of-band status queries. It also allowed the server to send asynchronous attention messages to the client. DDP was the lowest-level data-link-independent transport protocol. It provided a datagram service with no guarantees of delivery. All application-level protocols, including the infrastructure protocols NBP, RTMP and ZIP, were built on top of DDP. AppleTalk's DDP corresponds closely to the Network layer of the Open Systems Interconnection (OSI) communication model. Name Binding Protocol was a dynamic, distributed system for managing AppleTalk names. When a service started up on a machine, it registered a name for itself as chosen by a human administrator. At this point, NBP provided a system for checking that no other machine had already registered the same name. Later, when a client wanted to access that service, it used NBP to query machines to find that service. NBP provided browseability (\"what are the names of all the services available?\") as well as the ability to find a service with a particular name. Names were human readable, containing spaces, upper and lower case letters, and including support for searching. AEP (AppleTalk Echo Protocol) is a transport layer protocol designed to test the reachability of network nodes. AEP generates packets to be sent to the network node and is identified in the Type field of a packet as an AEP packet. The packet is first passed to the source DDP. After it is identified as an AEP packet, it is forwarded to the node where the packet is examined by the DDP at the destination. After the packet is identified as an AEP packet, the packet is then copied and a field in the packet is altered to create an AEP reply packet, and is then returned to the source node. PAP was the standard way of communicating with PostScript printers. It was built on top of ATP. When a PAP connection was opened, each end sent the other an ATP request which basically meant \"send me more data\". The client's response to the server was to send a block of PostScript code, while the server could respond with any diagnostic messages that might be generated as a result, after which another \"send-more-data\" request was sent. This use of ATP provided automatic flow control; each end could only send data to the other end if there was an outstanding ATP request to respond to. PAP also provided for out-of-band status queries, handled by separate ATP transactions. Even while it was busy servicing a print job from one client, a PAP server could continue to respond to status requests from any number of other clients. This allowed other Macintoshes on the LAN that were waiting to print to display status messages indicating that the printer was busy, and what the job was that it was busy with. RTMP was the protocol by which routers kept each other informed about the topology of the network. This was the only part of AppleTalk that required periodic unsolicited broadcasts: every 10 seconds, each router had to send out a list of all the network numbers it knew about and how far away it thought they were. ZIP was the protocol by which AppleTalk network numbers were associated with zone names. A zone was a subdivision of the network that made sense to humans (for example, \"Accounting Department\"); but while a network number had to be assigned to a topologically-contiguous section of the network, a zone could include several different discontiguous portions of the network. The initial default hardware implementation for AppleTalk was a high-speed serial protocol known as LocalTalk that used the Macintosh's built-in RS-422 ports at 230.4 kbit/s. LocalTalk used a splitter box in the RS-422 port to provide an upstream and downstream cable from a single port. The topology was a bus: cables were daisy-chained from each connected machine to the next, up to the maximum of 32 permitted on any LocalTalk segment. The system was slow by today's standards, but at the time the additional cost and complexity of networking on PC machines was such that it was common that Macs were the only networked personal computers in an office. Other larger computers, such as UNIX or VAX workstations, would commonly be networked via Ethernet. Other physical implementations were also available. One common replacement for LocalTalk was PhoneNet, a 3rd party solution (from a company called Farallon, now called Netopia, acquired by Motorola in 2007) that also used the RS-422 port and was indistinguishable from LocalTalk as far as Apple's LocalTalk port drivers were concerned, but ran over the two unused wires in standard four-wire phone cabling. PhoneNet was considerably less expensive to install and maintain. Ethernet and Token Ring was also supported, known as EtherTalk and TokenTalk respectively. EtherTalk in particular gradually became the dominant implementation method for AppleTalk as Ethernet became generally popular in the PC industry throughout the 1990s. Besides AppleTalk and TCP/IP, any Ethernet network could also simultaneously carry other protocols such as DECnet and IPX. When AppleTalk was first introduced, the dominant office computing platform was the PC compatible running MS-DOS. Apple introduced the AppleTalk PC Card in early 1987, allowing PCs to join AppleTalk networks and print to LaserWriter printers.[33] A year later AppleShare PC was released, allowing PCs to access AppleShare file servers.[34] The \"TOPS Teleconnector\"[35] MS-DOS networking system over AppleTalk system enabled MS-DOS PCs to communicate over AppleTalk network hardware; it comprised an AppleTalk interface card for the PC and a suite of networking software allowing such functions as file, drive and printer sharing. As well as allowing the construction of a PC-only AppleTalk network, it allowed communication between PCs and Macs with TOPS software installed. (Macs without TOPS installed could use the same network but only to communicate with other Apple machines.) The Mac TOPS software did not match the quality of Apple's own either in ease of use or in robustness and freedom from crashes, but the DOS software was relatively simple to use in DOS terms, and was robust. The BSD and Linux operating systems support AppleTalk through an open source project called Netatalk, which implements the complete protocol suite and allows them to both act as native file or print servers for Macintosh computers, and print to LocalTalk printers over the network. The Windows Server operating systems supported AppleTalk starting with Windows NT and ending after Windows Server 2003. Miramar included AppleTalk in its PC MacLAN product which was discontinued by CA in 2007. GroupLogic continues to bundle its AppleTalk protocol with its ExtremeZ-IP server software for Macintosh-Windows integration which supports Windows 2008 Server and Windows Vista as well prior versions. HELIOS Software GmbH offers a proprietary implementation of the AppleTalk protocol stack, as part of their HELIOS UB2 server. This is essentially a File and Print Server suite that runs on a whole range of different platforms. In addition, Columbia University released the Columbia AppleTalk Package (CAP) which implemented the protocol suite for various Unix flavors including Ultrix, SunOS, *BSD and IRIX. This package is no longer actively maintained."}, "ABAC": {"link": "https://en.wikipedia.org/wiki/Attribute-Based_Access_Control", "full_form": "Attribute-Based Access Control", "content": "Attribute-based access control (ABAC) defines an access control paradigm whereby access rights are granted to users through the use of policies which combine attributes together. The policies can use any type of attributes (user attributes, resource attributes, object, environment attributes etc.). This model supports Boolean logic, in which rules contain \"IF, THEN\" statements about who is making the request, the resource, and the action. For example: IF the requestor is a manager, THEN allow read/write access to sensitive data.[1] Unlike role-based access control (RBAC), which employs pre-defined roles that carry a specific set of privileges associated with them and to which subjects are assigned, the key difference with ABAC is the concept of policies that express a complex Boolean rule set that can evaluate many different attributes.[2] Attribute values can be set-valued or atomic-valued. Set-valued attributes contain more than one atomic value. Examples are role and project. Atomic-valued attributes contain only one atomic value. Examples are clearance and sensitivity. Attributes can be compared to static values or to one another, thus enabling relation-based access control. Although the concept itself existed for many years, ABAC is considered[3] \"next generation\" authorization model because it provides dynamic, context-aware and risk-intelligent access control to resources allowing access control policies that include specific attributes from many different information systems to be defined to resolve an authorization and achieve an efficient regulatory compliance, allowing enterprises flexibility in their implementations based on their existing infrastructures. Attribute-based access control is sometimes referred to as policy-based access control (PBAC) or claims-based access control (CBAC),[4] which is a Microsoft-specific term.[5]   ABAC can be seen as: ABAC comes with a recommended architecture which is as follows: Attributes can be about anything and anyone. They tend to fall into 4 different categories or functions (as in grammatical function) Policies are statements that bring together attributes to express what can happen and is not allowed. Policies in ABAC can be granting or denying policies. Policies can also be local or global and can be written in a way that they override other policies. Examples include: With ABAC you can have as many policies as you like that cater to many different scenarios and technologies.[8] Historically, access control models have included mandatory access control (MAC), discretionary access control (DAC), mandatory integrity control, and more recently role-based access control (RBAC). These access control models are user-centric and do not take into account additional parameters such as resource information, relationship between the user (the requesting entity) and the resource, and dynamic information e.g. time of the day or user IP. ABAC tries to address this by defining access control based on attributes which describe the requesting entity (the user), the targeted object or resource, the desired action (view, edit, delete...), and environmental or contextual information. This is why access control is said to be attribute-based. One standard that implements attribute- and policy-based access control is XACML, the eXtensible Access Control Markup Language. XACML defines an architecture, a policy language, and a request / response scheme. It does not handle attribute management (user attribute assignment, object attribute assignment, environment attribute assignment) which is left to traditional IAM tools, databases, and directories. The concept of ABAC can be applied at any level of the technology stack and an enterprise infrastructure. For example, ABAC can be used at the firewall, server, application, database, and data layer. The use of attributes bring additional context to evaluate the legitimacy of any request for access and inform the decision to grant or deny access. An important consideration when evaluating ABAC solutions is to understand its potential overhead on performance and its impact on the user experience. It is expected that the more granular the controls, the higher the overhead. ABAC can be used to apply attribute-based, fine-grained authorization to the API methods or functions. For instance, a banking API may expose an approveTransaction(transId) method. ABAC can be used to secure the call. With ABAC, a policy author can write the following: The flow would be as follows: One of the key benefits to ABAC is that the authorization policies and attributes can be defined in a technology neutral way. This means policies defined for APIs or databases can be reused in the application space. Common applications that can benefit from ABAC are: The same process and flow as the one described in the API section applies here too. Security for databases has long been specific to the database vendors: Oracle VPD, IBM FGAC, and Microsoft RLS are all means to achieve fine-grained ABAC-like security. Using ABAC, it is possible to define policies that apply across multiple databases. This is called dynamic data masking. An example would be: Data security typically goes one step further than database security and applies control directly to the data element. This is often referred to as Data-Centric Security. On traditional relational databases, ABAC policies can control access to data at the table, column, field, cell and sub-cell using logical controls with filtering conditions and masking based on attributes. Attributes can be data, user, session or tools based to deliver the greatest level of flexibility in dynamically granting/denying access to a specific data element. On big data, and distributed file systems such as Hadoop, ABAC applied at the data layer control access to folder, sub-folder, file, sub-file and other granular. Attribute-based access control can also be applied to Big Data systems like Hadoop. Policies similar to those used previously can be applied when retrieving data from data lakes.[9][10] As of Windows Server 2012, Microsoft has implemented an ABAC approach to controlling access to files and folders. This achieved through dynamic access control lists (DACL) and Security Descriptor Definition Language (SDDL). SDDL can be seen as an ABAC language as it uses metadata of the user (claims) and of the file / folder to control access."}, "ABCL": {"link": "https://en.wikipedia.org/wiki/Actor-Based_Concurrent_Language", "full_form": "Actor-Based Concurrent Language", "content": "Actor-Based Concurrent Language (ABCL) is a family of programming languages, developed in Japan in the 1980s and 1990s.   ABCL/1 (Actor-Based Concurrent Language) is a prototype-based concurrent programming language for the ABCL MIMD system, created in 1986 by Akinori Yonezawa, of the Department of Information Science at the University of Tokyo. ABCL/1 uses asynchronous message passing among objects to achieve concurrency. It requires Common Lisp. Implementations in Kyoto Common Lisp (KCL) and Symbolics Lisp are available from the author. ABCL/R is an object-oriented reflective subset of ABCL/1, written by Professor Akinori Yonezawa of Tokyo Institute of Technology in 1988. ABCL/R2 is a second generation version of ABCL/R, designed for the Hybrid Group Architecture. It was produced at the Tokyo Institute of Technology in 1992, and has almost all the functionality of ABCL/1. It is written in Common Lisp. As a reflective language, its programs can dynamically control their behavior, including scheduling policy, from within a user-process context. ABCL/c+ is an object-oriented concurrent language, a variant of ABCL/1 based on C instead of Lisp. This language is often referred to as C+, but must not be mistaken for C or C++. C+ was created by professor Akinori Yonezawa, winner of the Dahl-Nygaard Prize in 2008. The Dahl-Nygaard Prize is the world\u2019s most prestigious prize in the field of object-orientation."}, "ABI": {"link": "https://en.wikipedia.org/wiki/Application_binary_interface", "full_form": "Application Binary Interface", "content": "In computer software, an application binary interface (ABI) is the interface between two program modules, one of which is often a library and/or operating system and the other one is usually an application created by a regular programmer. In contrast to an API, which defines structures and methods one can use at software level, an ABI defines the structures and methods used to access external, already compiled libraries/code at the level of machine code. It does this by determining in which binary format information should be passed from one program component to the next, or to the operating system in the case of a system call. Thus it sets details such as the calling convention. Adhering to ABIs (which may or may not be officially standardized) is usually the job of the compiler, OS or library writer, but application programmers may have to deal with ABIs directly when writing programs in a mix of programming languages, using foreign function call interfaces between them. ABIs differ from application programming interfaces (APIs), which similarly define interfaces between program components, but at the source code level.   ABIs cover details such as: A complete ABI, such as the Intel Binary Compatibility Standard (iBCS),[1] allows a program from one operating system supporting that ABI to run without modifications on any other such system, provided that necessary shared libraries are present, and similar prerequisites are fulfilled. Other[which?] ABIs standardize details such as the C++ name mangling,[2] exception propagation,[3] and calling convention between compilers on the same platform, but do not require cross-platform compatibility. An embedded-application binary interface (EABI) specifies standard conventions for file formats, data types, register usage, stack frame organization, and function parameter passing of an embedded software program, for use with an embedded operating system. Compilers that support the EABI create object code that is compatible with code generated by other such compilers, allowing developers to link libraries generated with one compiler with object code generated with another compiler. Developers writing their own assembly language code may also interface with assembly generated by a compliant compiler. EABIs are designed to optimize for performance within the limited resources of an embedded system. Therefore, EABIs omit most abstractions that are made between kernel and user code in complex operating systems. For example, dynamic linking is avoided to allow smaller executables and faster loading, fixed register usage allows more compact stacks and kernel calls, and running the application in privileged mode allows direct access to custom hardware operation without the indirection of calling a device driver. [4] The choice of EABI can affect performance.[5][6] Widely used EABIs include PowerPC,[4] ARM EABI2[7] and MIPS EABI.[8]"}, "ABM": {"link": "https://en.wikipedia.org/wiki/Asynchronous_Balanced_Mode", "full_form": "Asynchronous Balanced Mode", "content": "High-Level Data Link Control (HDLC) is a bit-oriented code-transparent synchronous data link layer protocol developed by the International Organization for Standardization (ISO). The original ISO standards for HDLC are as follows: The current standard for HDLC is ISO 13239, which replaces all of those standards. HDLC provides both connection-oriented and connectionless service. HDLC can be used for point to multipoint connections, but is now used almost exclusively to connect one device to another, using what is known as Asynchronous Balanced Mode (ABM). The original master-slave modes Normal Response Mode (NRM) and Asynchronous Response Mode (ARM) are rarely used.   HDLC is based on IBM's SDLC protocol, which is the layer 2 protocol for IBM's Systems Network Architecture (SNA). It was extended and standardized by the ITU as LAP (Link Access Procedure), while ANSI named their essentially identical version ADCCP. Derivatives have since appeared in innumerable standards. It was adopted into the X.25 protocol stack as LAPB, into the V.42 protocol as LAPM, into the Frame Relay protocol stack as LAPF and into the ISDN protocol stack as LAPD. HDLC was the inspiration for the IEEE 802.2 LLC protocol, and it is the basis for the framing mechanism used with the PPP on synchronous lines, as used by many servers to connect to a WAN, most commonly the Internet. A mildly different version is also used as the control channel for E-carrier (E1) and SONET multichannel telephone lines. Cisco HDLC uses low-level HDLC framing techniques but adds a protocol field to the standard HDLC header. HDLC frames can be transmitted over synchronous or asynchronous serial communication links. Those links have no mechanism to mark the beginning or end of a frame, so the beginning and end of each frame has to be identified. This is done by using a frame delimiter, or flag, which is a unique sequence of bits that is guaranteed not to be seen inside a frame. This sequence is '01111110', or, in hexadecimal notation, 0x7E. Each frame begins and ends with a frame delimiter. A frame delimiter at the end of a frame may also mark the start of the next frame. A sequence of 7 or more consecutive 1-bits within a frame will cause the frame to be aborted. When no frames are being transmitted on a simplex or full-duplex synchronous link, a frame delimiter is continuously transmitted on the link. Using the standard NRZI encoding from bits to line levels (0 bit = transition, 1 bit = no transition), this generates one of two continuous waveforms, depending on the initial state:  This is used by modems to train and synchronize their clocks via phase-locked loops. Some protocols allow the 0-bit at the end of a frame delimiter to be shared with the start of the next frame delimiter, i.e. '011111101111110'. For half-duplex or multi-drop communication, where several transmitters share a line, a receiver on the line will see continuous idling 1-bits in the inter-frame period when no transmitter is active. Since the flag sequence could appear in user data, such sequences must be modified during transmission to keep the receiver from detecting a false frame delimiter. The receiver must also detect when this has occurred so that the original data stream can be restored before it is passed to higher layer protocols. This can be done using bit stuffing, in which a \"0\" is added after the occurrence of every \"11111\" in the data. When the receiver detects these \"11111\" in the data, it removes the \"0\" added by the transmitter. On synchronous links, this is done with bit stuffing. Any time that 5 consecutive 1-bits appear in the transmitted data, the data is paused and a 0-bit is transmitted. This ensures that no more than 5 consecutive 1-bits will be sent. The receiving device knows this is being done, and after seeing 5 1-bits in a row, a following 0-bit is stripped out of the received data. If, after 5 consecutive 1-bits, the following bit is also a 1-bit, the receiving device knows that either a flag has been found (if the sixth 1-bit is followed by a 0-bit) or an error has occurred (if the sixth 1-bit is followed by seventh 1-bit). In the latter case, the frame receive procedure, depending on state, is generally either aborted or restarted. This also (assuming NRZL with transition for 0 encoding of the output) provides a minimum of one transition per 6 bit times during transmission of data, and one transition per 7 bit times during transmission of flag, so the receiver can stay in sync with the transmitter. Note however, that for new protocols, newer encodings such as 8b/10b encoding are better suited. HDLC transmits bytes of data with the least significant bit first (not to be confused with little-endian order, which refers to byte ordering within a multi-byte field). When using asynchronous serial communication such as standard RS-232 serial ports, bits are sent in groups of 8, and bit-stuffing is inconvenient. Instead they use \"control-octet transparency\", also called \"byte stuffing\" or \"octet stuffing\". The frame boundary octet is 01111110, (7E in hexadecimal notation). A \"control escape octet\", has the bit sequence '01111101', (7D hexadecimal). If either of these two octets appears in the transmitted data, an escape octet is sent, followed by the original data octet with bit 5 inverted. For example, the data sequence \"01111110\" (7E hex) would be transmitted as \"01111101 01011110\" (\"7D 5E\" hex). Other reserved octet values (such as XON or XOFF) can be escaped in the same way if necessary. The contents of an HDLC frame are shown in the following table: Note that the end flag of one frame may be (but does not have to be) the beginning (start) flag of the next frame. Data is usually sent in multiples of 8 bits, but only some variants require this; others theoretically permit data alignments on other than 8-bit boundaries. The frame check sequence (FCS) is a 16-bit CRC-CCITT or a 32-bit CRC-32 computed over the Address, Control, and Information fields. It provides a means by which the receiver can detect errors that may have been induced during the transmission of the frame, such as lost bits, flipped bits, and extraneous bits. However, given that the algorithms used to calculate the FCS are such that the probability of certain types of transmission errors going undetected increases with the length of the data being checked for errors, the FCS can implicitly limit the practical size of the frame. If the receiver's calculation of the FCS does not match that of the sender's, indicating that the frame contains errors, the receiver can either send a negative acknowledge packet to the sender, or send nothing. After either receiving a negative acknowledge packet or timing out waiting for a positive acknowledge packet, the sender can retransmit the failed frame. The FCS was implemented because many early communication links had a relatively high bit error rate, and the FCS could readily be computed by simple, fast circuitry or software. More effective forward error correction schemes are now widely used by other protocols. Synchronous Data Link Control (SDLC) was originally designed to connect one computer with multiple peripherals. The original \"normal response mode\" is a master-slave mode where the computer (or primary terminal) gives each peripheral (secondary terminal) permission to speak in turn. Because all communication is either to or from the primary terminal, frames include only one address, that of the secondary terminal; the primary terminal is not assigned an address. There is no strong distinction between commands sent by the primary to a secondary, and responses sent by a secondary to the primary. Commands and responses are in fact indistinguishable; the only difference is the direction in which they are transmitted. Normal response mode allows operation over half-duplex communication links, as long as the primary is aware that it may not transmit when it has given permission to a secondary. Asynchronous response mode is an HDLC addition[1] for use over full-duplex links. While retaining the primary/secondary distinction, it allows the secondary to transmit at any time. Asynchronous balanced mode added the concept of a combined terminal which can act as both a primary and a secondary. There are some subtleties about this mode of operation; while many features of the protocol do not care whether they are in a command or response frame, some do, and the address field of a received frame must be examined to determine whether it contains a command (the address received is ours) or a response (the address received is that of the other terminal). Some HDLC variants extend the address field to include both source and destination addresses, or an explicit command/response bit. There are three fundamental types of HDLC frames. The general format of the control field is: There are also extended (2-byte) forms of I and S frames. Again, the least significant bit (rightmost in this table) is sent first. Poll/Final is a single bit with two names. It is called Poll when set by the primary station to obtain a response from a secondary station, and Final when set by the secondary station to indicate a response or the end of transmission. In all other cases, the bit is clear. The bit is used as a token that is passed back and forth between the stations. Only one token should exist at a time. The secondary only sends a Final when it has received a Poll from the primary. The primary only sends a Poll when it has received a Final back from the secondary, or after a timeout indicating that the bit has been lost. When operating as a combined station, it is important to maintain the distinction between P and F bits, because there may be two checkpoint cycles operating simultaneously. A P bit arriving in a command from the remote station is not in response to our P bit; only an F bit arriving in a response is. Both I and S frames contain a receive sequence number N(R). N(R) provides a positive acknowledgement for the receipt of I-frames from the other side of the link. Its value is always the first frame not received; it acknowledges that all frames with N(S) values up to N(R)-1 (modulo 8 or modulo 128) have been received and indicates the N(S) of the next frame it expects to receive. N(R) operates the same way whether it is part of a command or response. A combined station only has one sequence number space This is incremented for successive I-frames, modulo 8 or modulo 128. Depending on the number of bits in the sequence number, up to 7 or 127 I-frames may be awaiting acknowledgment at any time. Information frames, or I-frames, transport user data from the network layer. In addition they also include flow and error control information piggybacked on data. The sub-fields in the control field define these functions. The least significant bit (first transmitted) defines the frame type. 0 means an I-frame. Except for the interpretation of the P/F field, there is no difference between a command I frame and a response I frame; when P/F is 0, the two forms are exactly equivalent. Supervisory Frames, or 'S-frames', are used for flow and error control whenever piggybacking is impossible or inappropriate, such as when a station does not have data to send. S-frames do not have information fields. The S-frame control field includes a leading \"10\" indicating that it is an S-frame. This is followed by a 2-bit type, a poll/final bit, and a sequence number. If 7-bit sequence numbers are used, there is also a 4-bit padding field. The first 2 bits mean it is an S-frame. All S frames include a P/F bit and a receive sequence number as described above. Except for the interpretation of the P/F field, there is no difference between a command S frame and a response S frame; when P/F is 0, the two forms are exactly equivalent. 1|0 |S|S|P/F|N(R)| The 2-bit type field encodes the type of S frame. Unnumbered frames, or U-frames, are used for link management, and can also be used to transfer user data. They exchange session management and control information between connected devices, and some U-frames contain an information field, used for system management information or user data. The first 2 bits (11) mean it is a U-frame. The 5 type bits (2 before P/F bit and 3 bit after P/F bit) can create 32 different types of U-frame Link configurations can be categorized as being either: The three link configurations are: An additional link configuration is Disconnected mode. This is the mode that a secondary station is in before it is initialized by the primary, or when it is explicitly disconnected. In this mode, the secondary responds to almost every frame other than a mode set command with a \"Disconnected mode\" response. The purpose of this mode is to allow the primary to reliably detect a secondary being powered off or otherwise reset.. The HDLC module on the other end transmits (UA) frame when the request is accepted. And if the request is rejected it sends (DM) disconnect mode frame. Unnumbered frames are identified by the low two bits being 1. With the P/F flag, that leaves 5 bits as a frame type. Even though fewer than 32 values are in use, some types have different meanings depending on the direction they are sent: as a request or as a response. The relationship between the DISC (disconnect) command and the RD (request disconnect) response seems clear enough, but the reason for making SARM command numerically equal to the DM response is obscure. The UI, XID and TEST frames contain a payload, and can be used as both commands and responses. The FRMR frame contains a payload describing the unacceptable frame. The first 1 or 2 bytes are a copy of the rejected control field, the next 1 or 2 contain the current send and receive sequence numbers, and the following 4 or 5 bits indicate the reason for the rejection."}, "ABR": {"link": "https://en.wikipedia.org/wiki/Average_bitrate", "full_form": "Average Bitrate", "content": "Average bitrate (ABR) refers to the average amount of data transferred per unit of time, usually measured per second, commonly for digital music or video. An MP3 file, for example, that has an average bit rate of 128 kbit/s transfers, on average, 128,000 bits every second. It can have higher bitrate and lower bitrate parts, and the average bitrate for a certain timeframe is obtained by dividing the number of bits used during the timeframe by the number of seconds in the timeframe. Bitrate is not reliable as a standalone measure of audio/video quality, since more efficient compression methods use lower bitrates to encode material at a similar quality. Average bitrate can also refer to a form of variable bitrate (VBR) encoding in which the encoder will try to reach a target average bitrate or file size while allowing the bitrate to vary between different parts of the audio or video. As it is a form of variable bitrate, this allows more complex portions of the material to use more bits and less complex areas to use fewer bits. However, bitrate will not vary as much as in variable bitrate encoding.[1] At a given bitrate, VBR is usually higher quality than ABR, which is higher quality than CBR (constant bitrate).[2] ABR encoding is desirable for users who want the general benefits of VBR encoding (an optimum bitrate from frame to frame) but with a relatively predictable file size.[1] Two-pass encoding is usually needed for accurate ABR encoding, as on the first pass the encoder has no way of knowing what parts of the audio or video need the highest bitrates to be encoded.[3] "}, "AC": {"link": "https://en.wikipedia.org/wiki/Alternating_current", "full_form": "Alternating Current", "content": "Alternating current (AC) is an electric current which periodically reverses direction, in contrast to direct current (DC) which flows only in one direction. Alternating current is the form in which electric power is delivered to businesses and residences, and it is the form of electrical energy that consumers typically use when they plug kitchen appliances, televisions and electric lamps into a wall socket. A common source of DC power is a battery cell in a flashlight. The abbreviations AC and DC are often used to mean simply alternating and direct, as when they modify current or voltage.[1][2] The usual waveform of alternating current in most electric power circuits is a sine wave. In certain applications, different waveforms are used, such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. These types of alternating current carry information encoded (or modulated) onto the AC signal, such as sound (audio) or images (video). These currents typically alternate at higher frequencies than those used in power transmission. Electrical energy is distributed as alternating current because AC voltage may be increased or decreased with a transformer. This allows the power to be transmitted through power lines efficiently at high voltage, which reduces the energy lost as heat due to resistance of the wire, and transformed to a lower, safer, voltage for use. Use of a higher voltage leads to significantly more efficient transmission of power. The power losses (\n\n\n\n\nP\n\n\nL\n\n\n\n\n\n{\\displaystyle P_{\\rm {L}}}\n\n) in a conductor are a product of the square of the current (I) and the resistance (R) of the conductor, described by the formula This means that when transmitting a fixed power on a given wire, if the current is halved (i.e. the voltage is doubled), the power loss will be four times less. The power transmitted is equal to the product of the current and the voltage (assuming no phase difference); that is, Consequently, power transmitted at a higher voltage requires less loss-producing current than for the same power at a lower voltage. Power is often transmitted at hundreds of kilovolts, and transformed to 100\u2013240 volts for domestic use. High voltages have disadvantages, such as the increased insulation required, and generally increased difficulty in their safe handling. In a power plant, energy is generated at a convenient voltage for the design of a generator, and then stepped up to a high voltage for transmission. Near the loads, the transmission voltage is stepped down to the voltages used by equipment. Consumer voltages vary somewhat depending on the country and size of load, but generally motors and lighting are built to use up to a few hundred volts between phases. The voltage delivered to equipment such as lighting and motor loads is standardized, with an allowable range of voltage over which equipment is expected to operate. Standard power utilization voltages and percentage tolerance vary in the different mains power systems found in the world. High-voltage direct-current (HVDC) electric power transmission systems have become more viable as technology has provided efficient means of changing the voltage of DC power. HVDC systems, however, tend to be more expensive and less efficient over shorter distances than transformers.[citation needed] Transmission with high voltage direct current was not feasible in the early days of electric power transmission, as there was then no economically viable way to step down the voltage of DC for end user applications such as lighting incandescent bulbs. Three-phase electrical generation is very common. The simplest way is to use three separate coils in the generator stator, physically offset by an angle of 120\u00b0 (one-third of a complete 360\u00b0 phase) to each other. Three current waveforms are produced that are equal in magnitude and 120\u00b0 out of phase to each other. If coils are added opposite to these (60\u00b0 spacing), they generate the same phases with reverse polarity and so can be simply wired together. In practice, higher \"pole orders\" are commonly used. For example, a 12-pole machine would have 36 coils (10\u00b0 spacing). The advantage is that lower rotational speeds can be used to generate the same frequency. For example, a 2-pole machine running at 3600\u00a0rpm and a 12-pole machine running at 600\u00a0rpm produce the same frequency; the lower speed is preferable for larger machines. If the load on a three-phase system is balanced equally among the phases, no current flows through the neutral point. Even in the worst-case unbalanced (linear) load, the neutral current will not exceed the highest of the phase currents. Non-linear loads (e.g. the switch-mode power supplies widely used) may require an oversized neutral bus and neutral conductor in the upstream distribution panel to handle harmonics. Harmonics can cause neutral conductor current levels to exceed that of one or all phase conductors. For three-phase at utilization voltages a four-wire system is often used. When stepping down three-phase, a transformer with a Delta (3-wire) primary and a Star (4-wire, center-earthed) secondary is often used so there is no need for a neutral on the supply side. For smaller customers (just how small varies by country and age of the installation) only a single phase and neutral, or two phases and neutral, are taken to the property. For larger installations all three phases and neutral are taken to the main distribution panel. From the three-phase main panel, both single and three-phase circuits may lead off. Three-wire single-phase systems, with a single center-tapped transformer giving two live conductors, is a common distribution scheme for residential and small commercial buildings in North America. This arrangement is sometimes incorrectly referred to as \"two phase\". A similar method is used for a different reason on construction sites in the UK. Small power tools and lighting are supposed to be supplied by a local center-tapped transformer with a voltage of 55 V between each power conductor and earth. This significantly reduces the risk of electric shock in the event that one of the live conductors becomes exposed through an equipment fault whilst still allowing a reasonable voltage of 110 V between the two conductors for running the tools. A third wire, called the bond (or earth) wire, is often connected between non-current-carrying metal enclosures and earth ground. This conductor provides protection from electric shock due to accidental contact of circuit conductors with the metal chassis of portable appliances and tools. Bonding all non-current-carrying metal parts into one complete system ensures there is always a low electrical impedance path to ground sufficient to carry any fault current for as long as it takes for the system to clear the fault. This low impedance path allows the maximum amount of fault current, causing the overcurrent protection device (breakers, fuses) to trip or burn out as quickly as possible, bringing the electrical system to a safe state. All bond wires are bonded to ground at the main service panel, as is the neutral/identified conductor if present. The frequency of the electrical system varies by country and sometimes within a country; most electric power is generated at either 50 or 60\u00a0hertz. Some countries have a mixture of 50\u00a0Hz and 60\u00a0Hz supplies, notably electricity power transmission in Japan. A low frequency eases the design of electric motors, particularly for hoisting, crushing and rolling applications, and commutator-type traction motors for applications such as railways. However, low frequency also causes noticeable flicker in arc lamps and incandescent light bulbs. The use of lower frequencies also provided the advantage of lower impedance losses, which are proportional to frequency. The original Niagara Falls generators were built to produce 25\u00a0Hz power, as a compromise between low frequency for traction and heavy induction motors, while still allowing incandescent lighting to operate (although with noticeable flicker). Most of the 25\u00a0Hz residential and commercial customers for Niagara Falls power were converted to 60\u00a0Hz by the late 1950s, although some[which?] 25\u00a0Hz industrial customers still existed as of the start of the 21st century. 16.7\u00a0Hz power (formerly 16 2/3\u00a0Hz) is still used in some European rail systems, such as in Austria, Germany, Norway, Sweden and Switzerland. Off-shore, military, textile industry, marine, aircraft, and spacecraft applications sometimes use 400\u00a0Hz, for benefits of reduced weight of apparatus or higher motor speeds. Computer mainframe systems were often powered by 400 or 415\u00a0Hz for benefits of ripple reduction while using smaller internal AC to DC conversion units.[3] In any case, the input to the M-G set is the local customary voltage and frequency, variously 200 (Japan), 208, 240 (North America), 380, 400 or 415 (Europe) volts, and variously 50 or 60\u00a0Hz. A direct current flows uniformly throughout the cross-section of a uniform wire. An alternating current of any frequency is forced away from the wire's center, toward its outer surface. This is because the acceleration of an electric charge in an alternating current produces waves of electromagnetic radiation that cancel the propagation of electricity toward the center of materials with high conductivity. This phenomenon is called skin effect. At very high frequencies the current no longer flows in the wire, but effectively flows on the surface of the wire, within a thickness of a few skin depths. The skin depth is the thickness at which the current density is reduced by 63%. Even at relatively low frequencies used for power transmission (50\u201360\u00a0Hz), non-uniform distribution of current still occurs in sufficiently thick conductors. For example, the skin depth of a copper conductor is approximately 8.57\u00a0mm at 60\u00a0Hz, so high current conductors are usually hollow to reduce their mass and cost. Since the current tends to flow in the periphery of conductors, the effective cross-section of the conductor is reduced. This increases the effective AC resistance of the conductor, since resistance is inversely proportional to the cross-sectional area. The AC resistance often is many times higher than the DC resistance, causing a much higher energy loss due to ohmic heating (also called I2R loss). For low to medium frequencies, conductors can be divided into stranded wires, each insulated from one another, and the relative positions of individual strands specially arranged within the conductor bundle. Wire constructed using this technique is called Litz wire. This measure helps to partially mitigate skin effect by forcing more equal current throughout the total cross section of the stranded conductors. Litz wire is used for making high-Q inductors, reducing losses in flexible conductors carrying very high currents at lower frequencies, and in the windings of devices carrying higher radio frequency current (up to hundreds of kilohertz), such as switch-mode power supplies and radio frequency transformers. As written above, an alternating current is made of electric charge under periodic acceleration, which causes radiation of electromagnetic waves. Energy that is radiated is lost. Depending on the frequency, different techniques are used to minimize the loss due to radiation. At frequencies up to about 1\u00a0GHz, pairs of wires are twisted together in a cable, forming a twisted pair. This reduces losses from electromagnetic radiation and inductive coupling. A twisted pair must be used with a balanced signalling system, so that the two wires carry equal but opposite currents. Each wire in a twisted pair radiates a signal, but it is effectively cancelled by radiation from the other wire, resulting in almost no radiation loss. Coaxial cables are commonly used at audio frequencies and above for convenience. A coaxial cable has a conductive wire inside a conductive tube, separated by a dielectric layer. The current flowing on the surface of the inner conductor is equal and opposite to the current flowing on the inner surface of the outer tube. The electromagnetic field is thus completely contained within the tube, and (ideally) no energy is lost to radiation or coupling outside the tube. Coaxial cables have acceptably small losses for frequencies up to about 5\u00a0GHz. For microwave frequencies greater than 5\u00a0GHz, the losses (due mainly to the electrical resistance of the central conductor) become too large, making waveguides a more efficient medium for transmitting energy. Coaxial cables with an air rather than solid dielectric are preferred as they transmit power with lower loss. Waveguides are similar to coaxial cables, as both consist of tubes, with the biggest difference being that the waveguide has no inner conductor. Waveguides can have any arbitrary cross section, but rectangular cross sections are the most common. Because waveguides do not have an inner conductor to carry a return current, waveguides cannot deliver energy by means of an electric current, but rather by means of a guided electromagnetic field. Although surface currents do flow on the inner walls of the waveguides, those surface currents do not carry power. Power is carried by the guided electromagnetic fields. The surface currents are set up by the guided electromagnetic fields and have the effect of keeping the fields inside the waveguide and preventing leakage of the fields to the space outside the waveguide. Waveguides have dimensions comparable to the wavelength of the alternating current to be transmitted, so they are only feasible at microwave frequencies. In addition to this mechanical feasibility, electrical resistance of the non-ideal metals forming the walls of the waveguide cause dissipation of power (surface currents flowing on lossy conductors dissipate power). At higher frequencies, the power lost to this dissipation becomes unacceptably large. At frequencies greater than 200\u00a0GHz, waveguide dimensions become impractically small, and the ohmic losses in the waveguide walls become large. Instead, fiber optics, which are a form of dielectric waveguides, can be used. For such frequencies, the concepts of voltages and currents are no longer used. Alternating currents are accompanied (or caused) by alternating voltages. An AC voltage v can be described mathematically as a function of time by the following equation: where The peak-to-peak value of an AC voltage is defined as the difference between its positive peak and its negative peak. Since the maximum value of \n\n\n\nsin\n\u2061\n(\nx\n)\n\n\n{\\displaystyle \\sin(x)}\n\n is +1 and the minimum value is \u22121, an AC voltage swings between \n\n\n\n+\n\nV\n\n\np\ne\na\nk\n\n\n\n\n\n{\\displaystyle +V_{\\rm {peak}}}\n\n and \n\n\n\n\u2212\n\nV\n\n\np\ne\na\nk\n\n\n\n\n\n{\\displaystyle -V_{\\rm {peak}}}\n\n. The peak-to-peak voltage, usually written as \n\n\n\n\nV\n\n\np\np\n\n\n\n\n\n{\\displaystyle V_{\\rm {pp}}}\n\n or \n\n\n\n\nV\n\n\nP\n\u2212\nP\n\n\n\n\n\n{\\displaystyle V_{\\rm {P-P}}}\n\n, is therefore \n\n\n\n\nV\n\n\np\ne\na\nk\n\n\n\n\u2212\n(\n\u2212\n\nV\n\n\np\ne\na\nk\n\n\n\n)\n=\n2\n\nV\n\n\np\ne\na\nk\n\n\n\n\n\n{\\displaystyle V_{\\rm {peak}}-(-V_{\\rm {peak}})=2V_{\\rm {peak}}}\n\n. The relationship between voltage and the power delivered is Rather than using instantaneous power, \n\n\n\np\n(\nt\n)\n\n\n{\\displaystyle p(t)}\n\n, it is more practical to use a time averaged power (where the averaging is performed over any integer number of cycles). Therefore, AC voltage is often expressed as a root mean square (RMS) value, written as \n\n\n\n\nV\n\n\nr\nm\ns\n\n\n\n\n\n{\\displaystyle V_{\\rm {rms}}}\n\n, because Below it is assumed an AC waveform (with no DC component). To illustrate these concepts, consider a 230\u00a0V AC mains supply used in many countries around the world. It is so called because its root mean square value is 230\u00a0V. This means that the time-averaged power delivered is equivalent to the power delivered by a DC voltage of 230 V. To determine the peak voltage (amplitude), we can rearrange the above equation to: For 230\u00a0V AC, the peak voltage \n\n\n\n\nV\n\n\np\ne\na\nk\n\n\n\n\n\n{\\displaystyle V_{\\mathrm {peak} }}\n\n is therefore \n\n\n\n230\nV\n\u00d7\n\n\n2\n\n\n\n\n{\\displaystyle 230V\\times {\\sqrt {2}}}\n\n, which is about 325\u00a0V. During the course of one cycle the voltage rises from zero to 325\u00a0V, falls through zero to -325\u00a0V, and returns to zero. Alternating current is used to transmit information, as in the cases of telephone and cable television. Information signals are carried over a wide range of AC frequencies. POTS telephone signals have a frequency of about 3 kilohertz, close to the baseband audio frequency. Cable television and other cable-transmitted information currents may alternate at frequencies of tens to thousands of megahertz. These frequencies are similar to the electromagnetic wave frequencies often used to transmit the same types of information over the air. The first alternator to produce alternating current was a dynamo electric generator based on Michael Faraday's principles constructed by the French instrument maker Hippolyte Pixii in 1832.[4] Pixii later added a commutator to his device to produce the (then) more commonly used direct current. The earliest recorded practical application of alternating current is by Guillaume Duchenne, inventor and developer of electrotherapy. In 1855, he announced that AC was superior to direct current for electrotherapeutic triggering of muscle contractions.[5] Alternating current technology had first developed in Europe due to the work of Guillaume Duchenne (1850s), the Hungarian Ganz Works company (1870s), and in the 1880s: Sebastian Ziani de Ferranti, Lucien Gaulard, and Galileo Ferraris. In 1876, Russian engineer Pavel Yablochkov invented a lighting system where sets of induction coils were installed along a high voltage AC line. Instead of changing voltage, the primary windings transferred power to the secondary windings which were connected to one or several 'electric candles' (arc lamps) of his own design,[6][7] used to keep the failure of one lamp from disabling the entire circuit.[6] In 1878, the Ganz factory, Budapest, Hungary, began manufacturing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.[8] Alternating current systems can use transformers to change voltage from low to high level and back, allowing generation and consumption at low voltages but transmission, possibly over great distances, at high voltage, with savings in the cost of conductors and energy losses. A bipolar open-core power transformer developed by Lucien Gaulard and John Dixon Gibbs was demonstrated in London in 1881, and attracted the interest of Westinghouse. They also exhibited the invention in Turin in 1884. However these early induction coils with open magnetic circuits are inefficient at transferring power to loads. Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil.[9] The direct current systems did not have these drawbacks, giving it significant advantages over early AC systems. In the autumn of 1884, K\u00e1roly Zipernowsky, Ott\u00f3 Bl\u00e1thy and Miksa D\u00e9ri (ZBD), three engineers associated with the Ganz factory, determined that open-core devices were impractical, as they were incapable of reliably regulating voltage.[10] In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either a) wound around iron wire ring core or b)\u00a0surrounded by iron wire core.[9] In both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see toroidal cores). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs.[11] The Ganz factory in 1884 shipped the world's first five high-efficiency AC transformers.[12] This first unit had been manufactured to the following specifications: 1,400 W, 40\u00a0Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form.[12] The ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1,400 to 2,000 V) than the voltage of utilization loads (100 V initially preferred).[13][14] When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces.[15][16] The other essential milestone was the introduction of 'voltage source, voltage intensive' (VSVI) systems'[17] by the invention of constant voltage generators in 1885.[18] Ott\u00f3 Bl\u00e1thy also invented the first AC electricity meter.[19][20][21][22] The AC power systems was developed and adopted rapidly after 1886 due to its ability to distribute electricity efficiently over long distances, overcoming the limitations of the direct current system. In 1886, the ZBD engineers designed the world's first power station that used AC generators to power a parallel-connected common electrical network, the steam-powered Rome-Cerchi power plant.[23] The reliability of the AC technology received impetus after the Ganz Works electrified a large European metropolis: Rome in 1886.[23] In the UK, Sebastian de Ferranti, who had been developing AC generators and transformers in London since 1882, redesigned the AC system at the Grosvenor Gallery power station in 1886 for the London Electric Supply Corporation (LESCo) including alternators of his own design and transformer designs similar to Gaulard and Gibbs.[24] In 1890 he designed their power station at Deptford[25] and converted the Grosvenor Gallery station across the Thames into an electrical substation, showing the way to integrate older plants into a universal AC supply system.[26] In the US William Stanley, Jr. designed one of the first practical devices to transfer AC power efficiently between isolated circuits. Using pairs of coils wound on a common iron core, his design, called an induction coil, was an early (1885) transformer. Stanley also worked on engineering and adapting European designs such as the Gaulard and Gibbs transformer for US entrepreneur George Westinghouse who started building AC systems in 1886. The spread of Westinghouse and other AC systems triggered a push back in late 1887 by Edison (a proponent of direct current) who attempted to discredit alternating current as too dangerous in a public campaign called the \"War of Currents\". In 1888 alternating current systems gained further viability with introduction of a functional AC motor, something these systems had lacked up till then. The design, an induction motor, was independently invented by Galileo Ferraris and Nikola Tesla (with Tesla's design being licensed by Westinghouse in the US). This design was further developed into the modern practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.[27] The Ames Hydroelectric Generating Plant (spring of 1891) and the original Niagara Falls Adams Power Plant (August 25, 1895) were among the first hydroelectric alternating current power plants. The first long distance transmission of single-phase electricity was from a hydroelectric generating plant in Oregon at Willamette Falls which in 1890 sent power fourteen miles downriver to downtown Portland for street lighting.[28] In 1891, a second transmission system was installed in Telluride Colorado.[29] The San Antonio Canyon Generator was the third commercial single-phase hydroelectric AC power plant in the United States to provide long-distance electricity. It was completed on December 31, 1892 by Almarian William Decker to provide power to the city of Pomona, California which was 14 miles away. In 1893 he next designed the first commercial three-phase power plant in the United States using alternating current was the hydroelectric Mill Creek No. 1 Hydroelectric Plant near Redlands, California. Decker's design incorporated 10,000 V three-phase transmission and established the standards for the complete system of generation, transmission and motors used today. The Jaruga Hydroelectric Power Plant in Croatia was set in operation on 28 August 1895. The two generators (42\u00a0Hz, 550\u00a0kW each) and the transformers were produced and installed by the Hungarian company Ganz. The transmission line from the power plant to the City of \u0160ibenik was 11.5 kilometers (7.1\u00a0mi) long on wooden towers, and the municipal distribution grid 3000 V/110 V included six transforming stations. Alternating current circuit theory developed rapidly in the latter part of the 19th and early 20th century. Notable contributors to the theoretical basis of alternating current calculations include Charles Steinmetz, Oliver Heaviside, and many others.[30][31] Calculations in unbalanced three-phase systems were simplified by the symmetrical components methods discussed by Charles Legeyt Fortescue in 1918."}, "ACD": {"link": "https://en.wikipedia.org/wiki/Automatic_call_distributor", "full_form": "Automatic Call Distributor", "content": "An automated call distribution system, commonly known as, automatic call distributor (ACD), is a telephony device that answers and distributes incoming calls to a specific group of terminals or agents within an organization. ACDs often use a voice menu to direct callers based on the customer's selection, telephone number, selected incoming line to the system or time of day the call was processed. Computer telephony integration (CTI) and computer-supported telecommunications applications (CSTA) are intermediate software that can produce advanced ACD systems. Experts claim that \"the invention of ACD technology made the concept of a call centre possible.\"[1][2][3]   Private Branch Exchange (PBX) was a telephone exchange device that acted as a mini-switchboard to route phone calls. The closed nature of PBXs limited flexibility, and a system was designed to enable common computing devices to make routing decisions. The automated form of this technology developed into the automated call distribution system, where issued information about incoming calls would direct a response.[4][5] Although ACDs appeared in the 1950s, one of the first large and separate ACDs was a modified 5XB switch used by the New York Telephone Company in the early 1970s to distribute calls among hundreds of 4-1-1 information operators. Robert Hirvela developed and received a patent for technology that was used to create the Rockwell Galaxy Automatic Call Distributor, which was used by Continental Airlines for more than 20 years. Since then, ACDs have integrated incoming call management and voice messaging software into its capabilities.[6][7] ACDs route incoming calls to the most qualified employee or employees within a company that can address a caller's needs. The technology can also use rule-based instructions such as caller ID, automatic number identification, interactive voice response or dialed number identification services to determine how calls are handled. ACD systems are often found in offices that handle large volumes of incoming phone calls from callers who have no need to talk to a specific person, but require assistance from any of multiple persons (e.g., customer service representatives) at the earliest opportunity. There are several contact routing strategies that can be set up within an algorithm based on a company's needs. Skills-based routing is determined by an operator's knowledge to handle a caller's inquiry. Virtual contact centers can also be used to aggregate the skill sets of agents to help multiple vendors, where all real-time and statistical information can be shared amongst the contact center sites. An additional function for these external routing applications is to enable Computer telephony integration (CTI), which improves efficiency for call center agents by matching incoming phone calls with relevant data via screen pop.[8][9] The system has been met with criticism for making small improvements based on customer feedback.[10] In Florida, ACD technology was installed in several counties for 9-1-1 operators to aid in unanswered phone calls. However, call-takers are not familiar with the geography of the entire county due to the automated system sending calls to the first available responder.[11] There are multiple choices for distributing incoming calls from a queue"}, "ACE": {"link": "https://en.wikipedia.org/wiki/Advanced_Computing_Environment", "full_form": "Advanced Computing Environment", "content": "The Advanced Computing Environment (ACE) was defined by an industry consortium in the early 1990s to be the next generation commodity computing platform, the successor to personal computers based on Intel's 32-bit instruction set architecture. The effort found little support in the market and dissolved due to a lack of sales and infighting within the group.   The consortium was announced on the 9th of April 1991 by Compaq, Microsoft, MIPS Computer Systems, Digital Equipment Corporation, and the Santa Cruz Operation.[1][2] At the time it was widely believed that RISC-based systems would maintain a price/performance advantage over the ad hoc Wintel systems. However, it was also widely believed that Windows NT would quickly displace many other operating systems through the combined effects of a wide selection of software and the ease of building Wintel machines that supported it. ACE was formed to provide an alternative platform to Wintel, providing a viable alternative with the same advantages in terms of software support, and greater advantages in terms of performance. The environment standardized on the MIPS architecture and two operating systems: SCO UNIX with Open Desktop and what would become Windows NT (originally named OS/2 3.0). The Advanced RISC Computing (ARC) document was produced to give hardware and firmware specifications for the platform. Other members of the consortium included Acer, Control Data Corporation, Kubota, NEC Corporation, NKK, Olivetti, Prime Computer, Pyramid Technology, Siemens, Silicon Graphics, Sony, Sumitomo, Tandem Computers, Wang Laboratories, and Zenith Data Systems. Besides these large companies, several start-up companies built ACE-compliant systems as well. Each of the companies involved had their own reasons for joining the ACE effort. The initiative was used by microprocessor companies as an attempt to take market share away from Intel. System companies used the initiative as an attempt to take market share away from the workstation leader, Sun Microsystems. Because of these different goals, the effort was doomed from the start. Soon after the initiative was announced, a dissenting faction of seven ACE members declared that the decision to support only little-endian architectures was short-sighted. This subgroup, known as the Apache Group, promoted a big-endian alternative. The group, whose name was conceived as a pun on \"Big Indian\", was unrelated to the later Apache Software Foundation. It later adopted the name MIPS/Open. A rift within the ACE consortium was averted when it was decided to add support for big-endian SVR4.[citation needed] Even so, the ACE initiative (and consortium) began to fall apart little more than a year after it started, as it became apparent that there was not a mass market for an alternative to the Wintel computing platform. The upstart platforms did not offer enough performance improvement from the incumbent PC and there were major cost disadvantages of such systems due to the low volume production. When the initiative started, RISC based systems (running at 100-200 MHz at the time) had substantial performance advantage over Intel 80486 and original Pentium chips (running at approximately 60 MHz at the time).[clarification needed] Intel quickly migrated the Pentium design to newer semiconductor process generations and that performance (and operating frequency) advantage slipped away.[citation needed] Compaq was the first company to leave the consortium, stating that with the departure of CEO Rod Canion, one of the primary backers behind the formation of ACE, they were shifting priorities away from higher-end systems.[3] This was followed in short order by SCO announcing that they were suspending all work on moving their version of Unix to the MIPS platform. There were other potential conflicts: earlier that year, MIPS had been purchased by SGI, which may have also contributed to concerns about the neutrality of the target platform. DEC had released their Alpha processor and were less interested in promoting a competing architecture. And finally, the significant improvements in Intel x86 performance made abandoning it less attractive, and although ACE supported x86 for a time, Intel was never a member. The main product of the ACE group is the Advanced RISC Computing specification, or ARC. It was initially based on MIPS-based computer hardware and firmware environment. Although ACE went defunct, and no computer was ever manufactured which fully complied with the ARC standard, the ARC system still exerts a widespread legacy in that all Microsoft Windows NT-based operating systems (such as Windows XP) used ARC conventions for naming boot devices before Windows Vista. Further, SGI used a modified version of the ARC firmware (which it calls ARCS) in its systems. All SGI computers which run IRIX 6.1 or later (such as the Indy, Octane, etc.) boot from an ARCS console (which uses the same drive naming conventions as Windows, accordingly). In addition, most of the various RISC-based computers designed to run Windows NT used versions of the ARC boot console to boot NT. Among these computers were: It was also predicted that Intel IA-32-based computers would adopt the ARC console, although only SGI ever marketed such IA-32-based machines with ARC firmware (namely, the SGI Visual Workstation series, which went on sale in 1999). Products complying (to some degree) with the ARC standard include:"}, "ACID": {"link": "https://en.wikipedia.org/wiki/ACID", "full_form": "Atomicity Consistency Isolation Durability", "content": "In computer science, ACID (Atomicity, Consistency, Isolation, Durability) is a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, etc. In the context of databases, a sequence of database operations that satisfies the ACID properties and, thus, can be perceived as single logical operation on the data, is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction. In 1983,[1] Andreas Reuter and Theo H\u00e4rder coined the acronym ACID as shorthand for Atomicity, Consistency, Isolation, and Durability, building on earlier work[2] by Jim Gray who enumerated Atomicity, Consistency, and Durability but left out Isolation when characterizing the transaction concept. These four properties describe the major guarantees of the transaction paradigm, which has influenced many aspects of development in database systems. According to Gray and Reuter, IMS supported ACID transactions as early as 1973 (although the term ACID came later).[3]   The characteristics of these four properties as defined by Reuter and H\u00e4rder are as follows: Atomicity requires that each transaction be \"all or nothing\": if one part of the transaction fails, then the entire transaction fails, and the database state is left unchanged. An atomic system must guarantee atomicity in each and every situation, including power failures, errors and crashes. To the outside world, a committed transaction appears (by its effects on the database) to be indivisible (\"atomic\"), and an aborted transaction does not happen. The consistency property ensures that any transaction will bring the database from one valid state to another. Any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof. This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code), but merely that any programming errors cannot result in the violation of any defined rules. The isolation property ensures that the concurrent execution of transactions results in a system state that would be obtained if transactions were executed sequentially, i.e., one after the other. Providing isolation is the main goal of concurrency control. Depending on the concurrency control method (i.e., if it uses strict - as opposed to relaxed - serializability), the effects of an incomplete transaction might not even be visible to another transaction. The durability property ensures that once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors. In a relational database, for instance, once a group of SQL statements execute, the results need to be stored permanently (even if the database crashes immediately thereafter). To defend against power loss, transactions (or their effects) must be recorded in a non-volatile memory. The following examples further illustrate the ACID properties. In these examples, the database table has two columns, A and B. An integrity constraint requires that the value in A and the value in B must sum to 100. The following SQL code creates a table as described above: In database systems, atomicity (or atomicness; from Greek a-tomos, undividable) is one of the ACID transaction properties. A series of database operations in an atomic transaction will either all occur, or none will occur. The series of operations cannot be separated with only some of them being executed, which makes the series of operations \"indivisible\". A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. In other words, atomicity means indivisibility and irreducibility.[4] Consistency is a very general term, which demands that the data must meet all validation rules. In the previous example, the validation is a requirement that A + B = 100. All validation rules must be checked to ensure consistency. Assume that a transaction attempts to subtract 10 from A without altering B. Because consistency is checked after each transaction, it is known that A + B = 100 before the transaction begins. If the transaction removes 10 from A successfully, atomicity will be achieved. However, a validation check will show that A + B = 90, which is inconsistent with the rules of the database. The entire transaction must be cancelled and the affected rows rolled back to their pre-transaction state. If there had been other constraints, triggers, or cascades, every single change operation would have been checked in the same way as above before the transaction was committed. To demonstrate isolation, we assume two transactions execute at the same time, each attempting to modify the same data. One of the two must wait until the other completes in order to maintain isolation. Consider two transactions. T1 transfers 10 from A to B. T2 transfers 10 from B to A. Combined, there are four actions: If these operations are performed in order, isolation is maintained, although T2 must wait. Consider what happens if T1 fails halfway through. The database eliminates T1's effects, and T2 sees only valid data. By interleaving the transactions, the actual order of actions might be: Again, consider what happens if T1 fails halfway through. By the time T1 fails, T2 has already modified A; it cannot be restored to the value it had before T1 without leaving an invalid database. This is known as a write-write failure,[citation needed] because two transactions attempted to write to the same data field. In a typical system, the problem would be resolved by reverting to the last known good state, canceling the failed transaction T1, and restarting the interrupted transaction T2 from the good state. Consider a transaction that transfers 10 from A to B. First it removes 10 from A, then it adds 10 to B. At this point, the user is told the transaction was a success, however the changes are still queued in the disk buffer waiting to be committed to disk. Power fails and the changes are lost. The user assumes (understandably) that the changes persist. Processing a transaction often requires a sequence of operations that is subject to failure for a number of reasons. For instance, the system may have no room left on its disk drives, or it may have used up its allocated CPU time. There are two popular families of techniques: write-ahead logging and shadow paging. In both cases, locks must be acquired on all information to be updated, and depending on the level of isolation, possibly on all data that be read as well. In write ahead logging, atomicity is guaranteed by copying the original (unchanged) data to a log before changing the database.[dubious \u2013 discuss] That allows the database to return to a consistent state in the event of a crash. In shadowing, updates are applied to a partial copy of the database, and the new copy is activated when the transaction commits. Many databases rely upon locking to provide ACID capabilities. Locking means that the transaction marks the data that it accesses so that the DBMS knows not to allow other transactions to modify it until the first transaction succeeds or fails. The lock must always be acquired before processing data, including data that is read but not modified. Non-trivial transactions typically require a large number of locks, resulting in substantial overhead as well as blocking other transactions. For example, if user A is running a transaction that has to read a row of data that user B wants to modify, user B must wait until user A's transaction completes. Two phase locking is often applied to guarantee full isolation. An alternative to locking is multiversion concurrency control, in which the database provides each reading transaction the prior, unmodified version of data that is being modified by another active transaction. This allows readers to operate without acquiring locks, i.e. writing transactions do not block reading transactions, and readers do not block writers. Going back to the example, when user A's transaction requests data that user B is modifying, the database provides A with the version of that data that existed when user B started his transaction. User A gets a consistent view of the database even if other users are changing data. One implementation, namely snapshot isolation, relaxes the isolation property. Guaranteeing ACID properties in a distributed transaction across a distributed database, where no single node is responsible for all data affecting a transaction, presents additional complications. Network connections might fail, or one node might successfully complete its part of the transaction and then be required to roll back its changes because of a failure on another node. The two-phase commit protocol (not to be confused with two-phase locking) provides atomicity for distributed transactions to ensure that each participant in the transaction agrees on whether the transaction should be committed or not.[citation needed] Briefly, in the first phase, one node (the coordinator) interrogates the other nodes (the participants) and only when all reply that they are prepared does the coordinator, in the second phase, formalize the transaction."}, "ACK": {"link": "https://en.wikipedia.org/wiki/Amsterdam_Compiler_Kit", "full_form": "Amsterdam Compiler Kit", "content": "The Amsterdam Compiler Kit (ACK) is a retargetable compiler suite and toolchain written by Andrew Tanenbaum and Ceriel Jacobs, and was MINIX's native toolchain until the MINIX userland was largely replaced by that of NetBSD (MINIX 3.2.0) and clang was adopted as the system compiler. The ACK was originally closed-source software (that allowed binaries to be distributed for MINIX as a special case), but in April 2003 it was released under an open source BSD license. It has frontends for programming languages C, Pascal, Modula-2, Occam, and BASIC. The ACK's notability stems from the fact that in the early 1980s it was one of the first portable compilation systems designed to support multiple source languages and target platforms.[1][2] The ACK achieves maximum portability by using an intermediate language using bytecode, called EM. Each language front-end produces EM object files, which are then processed through several generic optimisers before being translated by a back-end into native machine code. ACK comes with a generic linker and librarian capable of manipulating files in the ACK's own a.out-based format; it will work on files containing EM code as well as native machine code. However, EM code cannot be linked to native machine code without translating the EM binary first.   * Version 6.0"}, "ACL": {"link": "https://en.wikipedia.org/wiki/Active_current_loop", "full_form": "Active Current Loop", "content": "In electrical signalling an analog current loop is used where a device must be monitored or controlled remotely over a pair of conductors. Only one current level can be present at any time. A major application of current loops is the industry standard 4-20 mA current loop for process control applications, where they are extensively used to carry signals from process instrumentation to PID controllers, SCADA systems, and Programmable logic controllers. They are also used to transmit controller outputs to the modulating field devices such as control valves. These loops have the advantages of simplicity and noise immunity, and have a large international user and equipment supplier base. Some 4-20 mA field devices can be powered by the current loop itself, removing the need for separate power supplies, and the \"smart\" HART Protocol uses the loop for communications between field devices and controllers. Various Automation Protocols may replace analog current loops, but 4-20 mA is still a principal industrial standard.   In industrial process control, analog 4\u201320\u00a0mA current loops are commonly used for electronic signalling, with the two values of 4 & 20\u00a0mA representing 0-100% of the range of measurement or control. These loops are used both for carrying sensor information from field instrumentation, and carrying control signals to the process modulating devices, such as a valve. The key advantages of the current loop are: Field instrumentation measurements are such as pressure, temperature, level, flow, pH or other process variables. A current loop can also be used to control a valve positioner or other output actuator. Since input terminals of instruments may have one side of the current loop input tied to the chassis ground (earth), analog isolators may be required when connecting several instruments in series. The relationship between current value and process variable measurement is set by calibration, which assigns different ranges of engineering units to the span between 4 and 20\u00a0mA. The mapping between engineering units and current can be inverted, so that 4\u00a0mA represents the maximum and 20\u00a0mA the minimum. Depending on the source of current for the loop, devices may be classified as active (supplying or \"sourcing\" power) or passive (relying on or \"sinking\" loop power). For example, a chart recorder may provide loop power to a pressure transmitter. The pressure transmitter modulates the current on the loop to send the signal to the strip chart recorder, but does not in itself supply power to the loop and so is passive. Another loop may contain two passive chart recorders, a passive pressure transmitter, and a 24 V battery. (The battery is the active device). Note that a 4-wire instrument has a power supply input separate from the current loop. Panel mount displays and chart recorders are commonly termed 'indicator devices' or 'process monitors'. Several passive indicator devices may be connected in series, but a loop must have only one transmitter device and only one power source (active device). The 4-20 mA convention was born in the 1950s out of the earlier highly successful 3-15 psi pneumatic control signal standard, when electronics became cheap and reliable enough to emulate the older standard electrically. The 3-15 psi standard had the same features of being able to power some remote devices, and have a \"live\" zero. However the 4-20 mA standard was better suited to the electronic controllers then being developed. The transition was gradual and has extended into the 21st century, due to the huge installed base of 3-15 psi devices. As the operation of pneumatic valves over motorised valves has many cost and reliability advantages, pneumatic actuation is still an industry standard. To allow the construction of hybrid systems, where the 4-20 mA is generated by the controller, but allows the use of pneumatic valves, a range of current to pressure (I to P) converters are available from manufacturers. These are usually located locally to the control valve and convert 4-20 mA to 3-15 psi (or 0.2 - 1.0 bar). This signal is then fed to the valve actuator or more commonly, a pneumatic positioner. The positioner is a dedicated controller which has a mechanical linkage to the actuator movement. This ensures that problems of friction are overcome and the valve control element moves to the desired position. It also allows the use of higher air pressures for valve actuation. With the development of cheap industrial micro-processors, \"smart\" valve positioners have become available since the mid-1980s and are very popular for new installations. These include an I to P converter, plus valve position and condition monitoring. These latter are fed back over the current loop to the controller, using such as the HART protocol. Analog current loops were historically occasionally carried between buildings by dry pairs in telephone cables leased from the local telephone company. 4\u201320\u00a0mA loops were more common in the days of analog telephony. These circuits require end-to-end direct current (DC) continuity, and unless a dedicated wire pair was hardwired, their use ceased with the introduction of semiconductor switching. DC continuity is not available over a microwave radio, optical fibre, or a multiplexed telephone circuit connection. Basic DC circuit theory shows that the current is the same all along the line. It was common to see 4\u201320\u00a0mA circuits that had loop lengths in miles or circuits working over telephone cable pairs that were longer than ten thousand feet end-to-end. There are still legacy systems in place using this technology. In Bell System circuits, voltages up to 125\u00a0VDC were employed. Discrete control functions can be represented by discrete levels of current sent over a loop. This would allow multiple control functions to be operated over a single pair of wires. Currents required for a specific function vary from one application or manufacturer to another. There is no specific current that is tied to a single meaning. It is almost universal that 0\u00a0mA indicates the circuit has failed. In the case of a fire alarm, 6\u00a0mA could be normal, 15\u00a0mA could mean a fire has been detected, and 0\u00a0mA would produce a trouble indication, telling the monitoring site the alarm circuit had failed. Some devices, such as two-way radio remote control consoles, can reverse the polarity of currents and can multiplex audio onto a DC current. These devices can be employed for any remote control need a designer might imagine. For example, a current loop could actuate an evacuation siren or command synchronized traffic signals. Current loop circuits are one possible way used to control radio base stations at distant sites. The two-way radio industry calls this type of remote control DC remote. This name comes from the need for DC circuit continuity between the control point and the radio base station. A current loop remote control saves the cost of extra pairs of wires between the operating point and the radio transceiver. Some equipment, such as the Motorola MSF-5000 base station, uses currents below 4\u00a0mA for some functions. An alternative type, the tone remote, is more complex but requires only an audio path between control point and base station.[2] For example, a taxi dispatch base station might be physically located on the rooftop of an eight-story building. The taxi company office might be in the basement of a different building nearby. The office would have a remote control unit that would operate the taxi company base station over a current loop circuit. The circuit would normally be over a telephone line or similar wiring. Control function currents come from the remote control console at the dispatch office end of a circuit. In two-way radio use, an idle circuit would normally have no current present. In two-way radio use, radio manufacturers use different currents for specific functions. Polarities are changed to get more possible functions over a single circuit. For example, imagine one possible scheme where the presence of these currents cause the base station to change state: This circuit is polarity-sensitive. If a telephone company cable splicer accidentally reversed the conductors, selecting channel 2 would lock the transmitter on. Each current level could close a set of contacts, or operate solid-state logic, at the other end of the circuit. That contact closure caused a change of state on the controlled device. Some remote control equipment could have options set to allow compatibility between manufacturers. That is, a base station that was configured to transmit with a +18\u00a0mA current could have options changed to (instead) make it transmit when +6\u00a0mA was present. In two-way radio use, AC signals were also present on the circuit pair. If the base station were idle, receive audio would be sent over the line from the base station to the dispatch office. In the presence of a transmit command current, the remote control console would send audio to be transmitted. The voice of the user in the dispatch office would be modulated and superimposed over the DC current that caused the transmitter to operate."}, "ACM": {"link": "https://en.wikipedia.org/wiki/Association_for_Computing_Machinery", "full_form": "Association for Computing Machinery", "content": "The Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947 and is the world's largest[1] scientific and educational computing society. It is a not-for-profit professional membership group.[2] Its membership is more than 100,000 as of 2011. Its headquarters are in New York City. The ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is \"Advancing Computing as a Science & Profession\".   ACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette. Many of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters. ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer. ACM publishes over 50 journals[3] including the prestigious[4] Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include: Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages. ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members. In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry. The ACM Portal is an online service of the ACM.[6] Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.[7] The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.[6] The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature. ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.[8] ACM was a \"green\" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record. All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription. There is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a centralized peer\u2013review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology. In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".[9] The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are presently[when?] about 958 Fellows[10] out of about 75,000 professional members. In 2006 ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". Note that in 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles. Also in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership. ACM has three kinds of chapters: Special Interest Groups,[11] Professional Chapters, and Student Chapters.[12] As of 2011, ACM has professional & SIG Chapters in 56 countries.[13] As of 2014, there exist ACM student chapters in 41 different countries.[14] ACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005. The ACM is a co\u2013presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.[21] There are some conferences hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.[citation needed] . In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended. For additional non-ACM conferences, see this list of computer science conferences. The ACM presents or co\u2013presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.[22][23][24] Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.[27] The President of ACM for 2016\u20132018 is Vicki L. Hanson, Distinguished Professor in the Department of Information Sciences and Technologies at the Rochester Institute of Technology and Professor and Chair of Inclusive Technologies at the University of Dundee, UK. She is successor of Alexander L. Wolf (2014\u20132016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012\u20132014), an American computer scientist who is recognized as one of \"the fathers of the Internet\"; Alain Chesnais (2010\u20132012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008\u20132010).[28] ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members\u2013At\u2013Large. This institution is often referred to simply as \"Council\" in Communications of the ACM. ACM has five \"Boards\" that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows: ACM-W,[29] the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM\u2013W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women and IT, CRA-W. The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science.[30] This program began in 2006. Speakers are nominated by SIG officers.[31] ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards.[33] ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE.[34] Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS.[35] They occasionally cooperate on projects like developing computing curricula.[36] ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).[37]"}, "ACME": {"link": "https://en.wikipedia.org/wiki/Automated_Classification_of_Medical_Entities", "full_form": "Automated Classification of Medical Entities", "content": "The Mortality Medical Data System (MMDS) is used to automate the entry, classification, and retrieval of cause-of-death information reported on death certificates throughout the United States and in many other countries. The National Center for Health Statistics (NCHS) began the system's development in 1967. The system has facilitated the standardization of mortality information within the United States, and ACME has become the de facto international standard for the automated selection of the underlying cause of death from multiple conditions listed on a death certificate. (Johansson & Westerling 2002:302)   The MMDS system consists of the following components, and is itself part of the National Vital Statistics System. There are two Mortality Medical Indexing, Classification, and Retrieval components. The Automated Classification of Medical Entities program automates the underlying cause-of-death coding rules. The input to ACME is the multiple cause-of-death codes (ICD) assigned to each entity (e.g., disease condition, accident, or injury) listed on cause-of-death certifications, preserving the location and order as reported by the certifier. ACME then applies the World Health Organization (WHO) rules to the ICD codes and selects an underlying cause of death. ACME has become the de facto international standard for the automated selection of the underlying cause of death. (Johansson & Westerling 2002:302) The TRANSlation of Axis program converts the ACME output data into fixed format and translates the data into a more desirable statistical form using the linkage provisions of the ICD. TRANSAX creates the data necessary for person-based tabulations by translating the axis of classification from an entity basis to a record basis."}, "ACP": {"link": "https://en.wikipedia.org/wiki/IBM_Airline_Control_Program", "full_form": "Airline Control Program", "content": "IBM Airline Control Program, or ACP, is a discontinued operating system developed by IBM beginning about 1965. In contrast to previous airline transaction processing systems, the most notable aspect of ACP is that it was designed to run on most models of the IBM System/360 mainframe computer family. This departed from the earlier model in which each airline had a different, machine-specific transaction system. Development began with SABRE (Semi-Automatic Business Research Environment), Deltamatic, and PANAMAC. From these Programmed Airline Reservations System (PARS) was developed. In 1969 the control program, ACP was separated from PARS; PARS keeping the functions for processing airline reservations and related data. In December 1979, ACP became known as ACP/TPF and then just TPF (Transaction Processing Facility)[1] as the transaction operating system became more widely implemented by businesses other than the major airlines, such as online credit card processing, hotel and rental car reservations, police emergency response systems, and package delivery systems. The last \"free\" release of ACP, 9.2.1, was intended for use in bank card and similar applications. It was shipped on a \"mini-reel\" which contained a complete ACP system, and its libraries for restoration to IBM 3340 DASD packs. From that complete system one could easily create derivative works. A hypervisor was included, which allowed OS/370 VS1 or VS2 (SVS or MVS) to be run as a \"guest\" OS under ACP itself. The end-user documentation, which was shipped with the tape, took almost 60 linear inches of shelf space. See also IBM Airline Control System (ALCS), a variant of TPF specially designed to provide all the benefits of TPF (very high speed, high volume, high availability transaction processing) but with the advantages such as easier integration into the data center offered by running on a standard IBM operating system platform."}, "ACPI": {"link": "https://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface", "full_form": "Advanced Configuration and Power Interface", "content": "In a computer, the Advanced Configuration and Power Interface (ACPI) provides an open standard that operating systems can use to discover and configure computer hardware components, to perform power management by (for example) putting unused components to sleep, and to perform status monitoring. First released in December 1996, ACPI aims to replace Advanced Power Management (APM), the MultiProcessor Specification, and the Plug and Play BIOS (PnP) Specification.[1] ACPI brings the power management under the control of the operating system, as opposed to the previous BIOS-centric system that relied on platform-specific firmware to determine power management and configuration policies.[2] The specification is central to the Operating System-directed configuration and Power Management (OSPM) system, an implementation for ACPI which removes device management responsibilities from legacy firmware interfaces via a UI. Internally, ACPI advertises the available components and their functions to the operating system kernel using instruction lists (\"methods\") provided through the system firmware (Unified Extensible Firmware Interface (UEFI) or BIOS), which the kernel parses. ACPI then executes the desired operations (such as the initialization of hardware components) using an embedded minimal virtual machine. Intel, Microsoft and Toshiba originally developed the standard, while HP, Huawei and Phoenix also participated later. In October 2013 the original developers of the ACPI standard agreed to transfer all assets to the UEFI Forum, in which all future development will take place.[3] The UEFI Forum published the latest version[update] of the standard, \"Revision\u00a06.2\", in May 2017.[4]   The firmware-level ACPI has three main components: the ACPI tables, the ACPI BIOS, and the ACPI registers. Unlike its predecessors, such as the APM or PnP BIOS, the ACPI implements little of its functionality in the ACPI BIOS code, whose main role is to load the ACPI tables in system memory. Instead, most of the firmware ACPI functionality is provided in ACPI Machine Language (AML) bytecode stored in the ACPI tables. To make use of these tables, the operating system must have an interpreter for the AML bytecode. A reference AML interpreter implementation is provided by the ACPI Component Architecture (ACPICA). At the BIOS development time, AML bytecode is compiled from the ASL (ACPI Source Language) code.[5][6] As ACPI also replaces PnP BIOS, it also provides a hardware enumerator, mostly implemented in the Differentiated System Description Table (DSDT) ACPI table. The advantage of a bytecode approach is that unlike PnP BIOS code (which was 16-bit), the ACPI bytecode may be used in any operating system, even in 64-bit long mode.[6] Overall design decision was not without criticism. In November 2003, Linus Torvalds\u2014author of the Linux kernel\u2014described ACPI as \"a complete design disaster in every way\".[7][8] In 2001, other senior Linux software developers like Alan Cox expressed concerns about the requirements that bytecode from an external source must be run by the kernel with full privileges, as well as the overall complexity of the ACPI specification.[9] In 2014, Mark Shuttleworth, founder of the Ubuntu Linux distribution, compared ACPI with Trojan horses.[10] The ACPI Component Architecture (ACPICA), mainly written by Intel's engineers, provides an open-source platform-independent reference implementation of the operating system\u2013related ACPI code.[11] The ACPICA code is used by Linux, Haiku and FreeBSD,[5] which supplement it with their operating-system specific code. The first revision of the ACPI specification was released in December 1996, supporting 16 and 32-bit addressing spaces. It was not until August 2000 that ACPI received 64-bit address support as well as support for multiprocessor workstations and servers with revision 2.0. In September 2004, revision 3.0 was released, bringing to the ACPI specification support for SATA controllers, PCI Express bus, multiprocessor support for more than 256 processors, ambient light sensors and user-presence devices, as well as extending the thermal model beyond the previous processor-centric support. Released in June 2009, revision 4.0 of the ACPI specification added various new features to the design; most notable are the USB\u00a03.0 support, logical processor idling support, and x2APIC support. Revision 5.0 of the ACPI specification was released in December 2011,[12] followed by the revision 5.1 that was released in July 2014.[13] The latest specification revision is 6.2, which was released in May 2017.[4] Microsoft's Windows 98 was the first operating system to implement ACPI,[14][15] but its implementation was somewhat buggy or incomplete,[16][17] although some of the problems associated with it were caused by the first-generation ACPI hardware.[18] Windows 98 first edition disabled ACPI by default except on a whitelist of systems. Other operating systems, including later versions of Windows, eComStation, FreeBSD, NetBSD, OpenBSD, HP-UX, OpenVMS, Linux, and PC versions of Solaris, have at least some support for ACPI.[19] Some newer operating systems like Windows Vista require ACPI-compliant BIOS to work at all[20] The 2.4 series of the Linux kernel had only minimal support for ACPI, with better support implemented (and enabled by default) from kernel version 2.6.0 onwards.[21] Old ACPI BIOS implementations tend to be quite buggy, and consequently are not supported by later operating systems. For example, Windows 2000, Windows XP, and Windows Server 2003 only use ACPI if the BIOS date is after January 1, 1999, and for Windows 98 Second Edition this date is December 1, 1999.[22] Similarly, Linux kernel 2.6 blacklisted any ACPI BIOS from before January 1, 2001.[21] Once an OSPM-compatible operating system activates ACPI, it takes exclusive control of all aspects of power management and device configuration. The OSPM implementation must expose an ACPI-compatible environment to device drivers, which exposes certain system, device and processor states. The ACPI specification defines the following four global \"Gx\" states and six sleep \"Sx\" states for an ACPI-compliant computer system:[23][24] The specification also defines a Legacy state: the state on an operating system which does not support ACPI. In this state, the hardware and power are not managed via ACPI, effectively disabling ACPI. The device states D0\u2013D3 are device dependent: The CPU power states C0\u2013C3 are defined as follows: While a device or processor operates (D0 and C0, respectively), it can be in one of several power-performance states. These states are implementation-dependent. Though, P0 is always the highest-performance state; with P1 to Pn being successively lower-performance states, up to an implementation-specific limit of n no greater than 16.[27] P-states have become known as SpeedStep in Intel processors, as PowerNow! or Cool'n'Quiet in AMD processors, and as PowerSaver in VIA processors. ACPI-compliant systems interact with hardware through either a \"Function Fixed Hardware (FFH) Interface\", or a platform-independent hardware programming model which relies on platform-specific ACPI Machine Language (AML) provided by the original equipment manufacturer (OEM). Function Fixed Hardware interfaces are platform-specific features, provided by platform manufacturers for the purposes of performance and failure recovery. Standard Intel-based PCs have a fixed function interface defined by Intel,[28] which provides a set of core functionality that reduces an ACPI-compliant system's need for full driver stacks for providing basic functionality during boot time or in the case of major system failure. ACPI Platform Error Interface (APEI) is a specification for reporting of hardware errors, e.g. from the chipset, to the operating system. ACPI defines many tables that provide the interface between an ACPI-compliant operating system, and system firmware. This includes Differentiated System Description Table (DSDT), Secondary System Description Table (SSDT), and Static Resource Affinity Table (SRAT), for example.[29] The tables allow description of system hardware in a platform-independent manner, and are presented as either fixed-formatted data structures or in AML. The main AML table is the DSDT (differentiated system description table). The Root System Description Pointer is located in a platform-dependent manner, and describes the rest of the tables. Ubuntu Linux founder Mark Shuttleworth has likened ACPI to Trojan horses.[30] He has described proprietary firmware (ACPI-related or any other firmware) as a security risk, saying that \"firmware on your device is the NSA's best friend\" and calling firmware (ACPI or non-ACPI) \"a Trojan horse of monumental proportions\". He has pointed out that low quality, closed source firmware is a major threat to system security:[7] \"Your biggest mistake is to assume that the NSA is the only institution abusing this position of trust \u2014 in fact, it's reasonable to assume that all firmware is a cesspool of insecurity, courtesy of incompetence of the highest degree from manufacturers, and competence of the highest degree from a very wide range of such agencies\". As a solution to this problem, he has called for declarative firmware (ACPI or non-ACPI).[7] Firmware should be open-source so that the code can be checked and verified. Firmware should be declarative, meaning that it should describe \"hardware linkage and dependencies\" and should not include executable code. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "ACR": {"link": "https://en.wikipedia.org/wiki/Attenuation_to_crosstalk_ratio", "full_form": "Attenuation to Crosstalk Ratio", "content": "Attenuation-to-crosstalk ratio (ACR) is a parameter that is measured when testing a communication link, which represents the overall performance of the cable.[1] ACR is a mathematical formula that calculates the ratio of attenuation to near-end crosstalk for each combination of cable pairs.[2] ACR is expressed as a figure in decibels (dB), between the signal attenuation produced by a wire or cable transmission medium and the near-end crosstalk (NEXT).[3] In order for a signal to be received with an acceptable bit error rate, the attenuation and the crosstalk must both be minimized.[3] Crosstalk can be reduced by ensuring that twisted-pair wiring is tightly twisted and is not crushed, and by ensuring that connectors between wire and cable media are properly rated and installed.[3] Positive ACR calculations mean that transmitted signal strength is stronger than that of near-end crosstalk.[4] ACR can be used to define a signal bandwidth (e.g. 250\u00a0MHz for category 6 cable) where signal to noise ratios are sufficient to support certain applications.[4] The maximum frequency for which positive ACR is assured provides a benchmark to assess the usable bandwidth of twisted-pair cabling systems.[4] EIA/TIA specifies specific values for ACR in order to meet the various categories of cable.[5] "}, "AD": {"link": "https://en.wikipedia.org/wiki/Administrative_domain", "full_form": "Administrative Domain", "content": "An administrative domain is a service provider holding a security repository permitting to easily authenticate and authorize clients with credentials. This particularly applies to computer network security. This concept is captured by the 'AdminDomain' class of the GLUE information model.[1] An administrative domain is mainly used in intranet environments. It may be implemented as a collection of hosts and routers, and the interconnecting network(s), managed by a single administrative authority. Interoperation between different administrative domains having different security repositories, different security software or different security policies is notoriously difficult. Therefore, administrative domains wishing ad hoc interoperation or full interoperability have to build a federation. This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later. "}, "ADC": {"link": "https://en.wikipedia.org/wiki/Apple_Display_Connector", "full_form": "Apple Display Connector", "content": "The Apple Display Connector (ADC) is a proprietary modification of the DVI connector that combines analog and digital video signals, USB, and power all in one cable. Apple used ADC for its LCD-based Apple Cinema Displays and their final CRT displays, before deciding to use standard DVI connectors on later models. First implemented in the July 2000 Power Mac G4 and G4 Cube, ADC disappeared from displays in June 2004 when Apple introduced the aluminum-clad 20\" (51 cm), 23\" (58 cm), and 30\" (76 cm) Apple Cinema Displays, which feature separate DVI, USB and FireWire connectors, and their own power supplies. The ADC was still standard on the Power Mac G5 until April 2005, when new models meant the only remaining Apple product with an ADC interface was the single processor Power Mac G5 introduced in October 2004. This single processor Power Mac G5 was discontinued soon after in June 2005.   The Apple Display Connector, debuted in the Apple Studio Display, is physically incompatible with a standard DVI connector, as was previously used in the PowerBook G4 and the older Power Mac G4. The Apple DVI to ADC Adapter,[1] which cost $149US at launch but was in 2002 available for $99US,[2] takes USB and DVI connections from the computer, together with power, and combines them into an ADC connection, allowing ADC monitors to be used with DVI-based machines. The initial implementation of ADC on some models of Power Mac G4s involved the removal of DVI connectors from these computers. This change necessitated a passive ADC to DVI adapter to use a DVI monitor. The ADC carries up to 100 W of power, an insufficient amount to run most 19-inch (48 cm) or bigger CRTs widely available during ADC's debut, nor can it run contemporary flat panels marketed for home entertainment (many of which support DVI or VGA connections) without an adapter. The power limit was an important factor for Apple to abandon ADC when it launched the 30-inch (76 cm) Apple Cinema HD Display. On newer DVI-based displays lacking ADC, Apple still opted for a single \"ganged cable\" that connects the separate signal cables to each other so they cannot tangle. Such cables, however, employ standard DVI, power, USB and FireWire connectors, avoiding drawbacks to ADC. Beginning in 2008, Apple began transitioning away from DVI, adopting the increasingly common DisplayPort signalling standard, and developed their own Mini DisplayPort connector beginning with the first LED-backlit Cinema Displays. As of 2013, Apple no longer uses a DVI-based interface for any of its displays. Apple no longer supports ADC adapters, or monitors. Power is supplied to the ADC port by an additional finger connector on the video card, which plugs into a slot on the motherboard between the AGP slot and the back panel of the computer; on G4 Macs, some power is also sent through AGP pins 3 and 11. When ADC was introduced, AGP pins 3 and 11 were unassigned. In AGP 8x, pins 3 and 11 were assigned, so because of that, G4s are not directly compatible with AGP 8x. G5 Macs draw all power from the finger connector and therefore are 8x compatible. To use an AGP 8x card in a G4, pins 3 and 11 must be somehow disabled; this can be done by placing tape over the conductive part of the pin, slicing the PCB traces, or on some cards, desoldering surface mount resistors."}, "ADB": {"link": "https://en.wikipedia.org/wiki/Apple_Desktop_Bus", "full_form": "Apple Desktop Bus", "content": "Apple Desktop Bus (ADB) is a proprietary[1] bit-serial peripheral bus connecting low-speed devices to computers. It was introduced on the Apple IIGS in 1986 as a way to support low-cost devices like keyboards and mice, allowing them to be connected together in a daisy chain without the need for hubs or other devices. ADB was quickly introduced on later Macintosh models, on later models of NeXT computers, and saw some other 3rd party use as well. Like the similar PS/2 connector used in many PC-compatibles at the time, ADB was rapidly replaced by USB as that system became popular in the late 1990s; the last external ADB port on an Apple product was in 1999, though it remained as an internal-only bus on some Mac models into the 2000s.   Early during the creation of the Macintosh computer, the engineering team had selected the fairly sophisticated Zilog 8530 to supply serial communications. This was initially done to allow multiple devices to be plugged into a single port, using simple networking protocols implemented inside the 8530 to allow them to send and receive data with the host computer.[2] During development of the AppleBus system, computer networking became a vitally important feature of any computer system. With no card slots, the Macintosh was unable to easily add support for Ethernet or similar local area networking standards. Work on AppleBus was re-directed to networking purposes, and was released in 1985 as the AppleTalk system. This left the Mac with the original single-purpose mouse and keyboard ports, and no general purpose system for low-speed devices to use.[2] ADB was created by Steve Wozniak, who had been looking for a project to work on in the mid-1980s.[citation needed] Someone suggested that he should create a new connection system for devices like mice and keyboards, one that would require only a single daisy-chained cable, and be inexpensive to implement. The first system to use ADB is the Apple IIGS in 1986. It is used on all Apple Macintosh machines starting with the Macintosh II and Macintosh SE. ADB is also used on later models of NeXT computers.[3] The vast majority of ADB devices are for input, including track balls, joysticks, graphics tablets and similar devices. Special-purpose uses include software protection dongles and even the Teleport modem. The first Macintosh to move from ADB is the iMac in 1998, which features USB in its place. The last Apple computer to have an ADB port is the \"Yosemite\" Power Macintosh G3 in 1999. No machines being built today use ADB for device interconnection, but up to February 2005, PowerBooks and iBooks still use the ADB protocol in the internal interface with the built-in keyboard and touchpad. Subsequent models use a USB-based trackpad. In keeping with Apple's general philosophy of industrial design, ADB was intended to be as simple to use as possible, while still being inexpensive to implement. A suitable connector was found in the form of the 4 pin mini-DIN connector, which is also used for S-Video. The connectors are small, widely available, and can only be inserted the \"correct way\". They do not lock into position, but even with a friction fit they are firm enough for light duties like those intended for ADB. ADB's protocol requires only a single pin for data, labeled ADB. Two of the other pins are used for +5 V power supply and ground. The +5 V pin guarantees at least 500 mA, and requires devices to use only 100 mA each. ADB also includes the PSW pin which is attached directly to the power supply of the host computer. This is included to allow a key on the keyboard to start up the machine without needing the ADB software to interpret the signal. In more modern designs, an auxiliary microcontroller is always kept running, so it is economical to use a power-up command over the standard USB channel. Most serial digital interfaces use a separate clock pin to signal the arrival of individual bits of data. However, Wozniak decided that a separate wire for a clock signal was not necessary; and as ADB was designed to be low-cost, it made economical sense to leave it out. Like modems, the system locks onto the signal rise and fall times to recreate a clock signal. The decoding transceiver ASIC as well as associated patents were controlled by Apple; this required vendors to work more closely with Apple. In the Macintosh SE, the ADB is implemented in an Apple-branded Microchip PIC16CR54 Microcontroller. The ADB system is based around the devices having the ability to decode a single number (the address) and being able to hold several small bits of data (their registers). All traffic on the bus is driven by the host computer, which sends out commands to read or write data: devices are not allowed to use the bus unless the computer first requests it. These requests take the form of single-byte strings. The upper four bits contain the address, the ID of one of the devices on the chain. The four bits allow for up to 16 devices on a single bus. The next two bits specify one of four commands, and the final two bits indicate one of four registers. The commands are: For instance, if the mouse is known to be at address $D, the computer will periodically send out a message on the bus that looks something like... 1101 11 00 This says that device $D (1101) should talk (11) and return the contents of register zero (00). To a mouse this means \"tell me the latest position changes\". Registers can contain between two and eight bytes. Register zero is generally the primary communications channel. Registers one and two are undefined, and are generally intended to allow 3rd party developers to store configuration information. Register three always contains device identification information. The addresses and enumeration of the devices are set to default values when reset. For instance, all keyboards are set to $2, and all mice to $3. When the machine is first powered on, the ADB device driver will send out talk commands asking each of these known default addresses, in turn, for the contents of register three. If no response comes from a particular address, the computer marks it dead and doesn't bother polling it later. If a device does respond, it does so by saying it is moving to a new randomly selected higher address. The computer then responds by sending another command to that new address, asking the device to move to yet another new address. Once this completes, that device is marked live, and the system continues polling it in the future. Once all of the devices are enumerated in this fashion, the bus is ready to be used. Although it was not common, it is possible for the ADB bus to have more than one device of the same sort plugged in \u2014 two graphics tablets or software copy protection dongles, for instance. In this case when it asks for devices on that default address, both will respond and a collision could occur. The devices include a small bit of timing that allows them to avoid this problem. After receiving a message from the host, the devices wait a short random time before responding, and then only do so after \"snooping\" the bus to make sure it was not busy. With two dongles plugged in, for instance, when the bus is first setting up and queries that address, one of them will be the first to respond due to the random wait timer. The other will notice the bus was busy and not respond. The host will then send out another message to that original address, but since one device has moved to a new address, only the other will then respond. This process continues until no one responds to the request on the original address, meaning there are no more devices of that type to enumerate. Data rates on the bus are theoretically as high as 125 kbit/s. However, the actual speed is at best half that, due to there being only one pin being shared between the computer and devices, and in practice, throughput is even less, as the entire system was driven by how fast the computer polls the bus. The classic Mac OS is not particularly well suited to this task, and the bus often gets bogged down at about 10 kbit/s. Early Teleport modems running at 2400\u00a0bit/s have no problems using ADB, but later models were forced to move to the more expensive RS422 ports as speeds moved to 14.4\u00a0kbit/s and higher. One peculiarity of ADB is that in spite of being electrically unsafe for hot-swapping on all but a few machines, it has all of the basic capabilities needed for hot-swapping implemented in its software and support hardware. On practically all original ADB systems it is not safe to plug in or unplug a device once the system is powered on (unlike modern day busses designed with hot-swap in mind). This can cause the opening of a soldered-in fuse on the motherboard. If brought to an authorised dealer, this can result in a motherboard swap at a significant expense. A simple alternative is to obtain a fuse at a nominal cost and wire it in parallel across the open motherboard fuse (not even requiring soldering if done appropriately). The mini-DIN connector is only rated for 400 insertions and it is easy to bend a pin if not inserted with caution; in addition, the socket can become loose, resulting in intermittent function. Presaging the disappearance of the second port on newer FireWire devices, some ADB devices lack a pass-through connector, making it impossible to daisy-chain more than one such device at a time without obscure splitter units. Keyboards, software dongles, graphics tablets, game pads and joysticks typically have pass-through connectors, while few mice or trackballs have them. While Mini-DIN connectors cannot be plugged in the \"wrong way\", it is possible to have trouble finding the right way without looking inside the circular connector's shroud. Apple attempted to help by using U-shaped soft plastic grips around the connectors to key both plugs and sockets so the flat side has a specific relation to the shell keyway, but this feature was ignored by some 3rd-party manufacturers. Additionally, there are four ways to orient the receiving socket on a device such as a keyboard; various Apple keyboards use at least three of these possible orientations."}, "ADCCP": {"link": "https://en.wikipedia.org/wiki/Advanced_Data_Communication_Control_Procedures", "full_form": "Advanced Data Communications Control Procedures", "content": "In telecommunication, Advanced Data Communication Control Procedures (or Protocol) (ADCCP) is a bit-oriented data link layer protocol used to provide point-to-point and point-to-multipoint transmission of data frames that contain error control information. It places data on a network and ensures proper delivery to a destination. ADCCP is based on the IBM's SDLC protocol. The HDLC by ISO and LAPB by ITU/CCITT are based on ADCCP.[citation needed] ADCCP is an ANSI standard, X3.66, derived from IBM's Synchronous Data Link Control (SDLC) protocol, and is functionally equivalent to the ISO High-Level Data Link Control (HDLC) standard.[1] ADCCP has 3 main modes \u2013 NRM (Normal Response mode akin to SDLC), ABM (Asynchronous Balanced mode - akin to HDLC) and ARM (Asynchronous Response mode) \u00a0This article incorporates\u00a0public domain material from the General Services Administration document \"Federal Standard 1037C\"."}, "ADO": {"link": "https://en.wikipedia.org/wiki/ActiveX_Data_Objects", "full_form": "ActiveX Data Objects", "content": "In computing, Microsoft's ActiveX Data Objects (ADO) comprises a set of Component Object Model (COM) objects for accessing data sources. A part of MDAC (Microsoft Data Access Components), it provides a middleware layer between programming languages and OLE DB (a means of accessing data stores, whether databases or not, in a uniform manner). ADO allows a developer to write programs that access data without knowing how the database is implemented; developers must be aware of the database for connection only. No knowledge of SQL is required to access a database when using ADO, although one can use ADO to execute SQL commands directly (with the disadvantage of introducing a dependency upon the type of database used). Microsoft introduced ADO in October 1996, positioning the software as a successor to Microsoft's earlier object layers for accessing data sources, including RDO (Remote Data Objects) and DAO (Data Access Objects). ADO is made up of four collections and twelve objects.   Some basic steps are required in order to be able to access and manipulate data using ADO\u00a0: Here is an ASP example using ADO to select the \"Name\" field, from a table called \"Phonebook\", where a \"PhoneNumber\" was equal to \"555-5555\". This is equivalent to the following ASP code, which uses plain SQL instead of the functionality of the Recordset object: ADO is supported in any development language that supports binding to binary COM interfaces. These languages include ASP, Delphi, PowerBuilder, and Visual Basic for Applications (VBA). ADO support has now been added to dBase Plus 8 (With ADO) ADO.NET has replaced ADO in the same way that C#/.NET replaced C/Win32 as the primary mode for targeting Windows application development. ADO.NET follows the same design pattern as ADO, enabling an ADO developer an easy path forward when moving to the .NET framework."}, "ADSL": {"link": "https://en.wikipedia.org/wiki/Asymmetric_digital_subscriber_line", "full_form": "Asymmetric Digital Subscriber Line", "content": "Asymmetric digital subscriber line (ADSL) is a type of digital subscriber line (DSL) technology, a data communications technology that enables faster data transmission over copper telephone lines than a conventional voiceband modem can provide. ADSL differs from the less common symmetric digital subscriber line (SDSL). In ADSL, Bandwidth and bit rate are said to be asymmetric, meaning greater toward the customer premises (downstream) than the reverse (upstream). Providers usually market ADSL as a service for consumers for Internet access for primarily downloading content from the Internet, but not serving content accessed by others.   ADSL works by using the frequency spectrum above the band used by voice telephone calls.[1] With a DSL filter, often called splitter, the frequency bands are isolated, permitting a single telephone line to be used for both ADSL service and telephone calls at the same time. ADSL is generally only installed for short distances from the telephone exchange (the last mile), typically less than 4 kilometres (2\u00a0mi),[2] but has been known to exceed 8 kilometres (5\u00a0mi) if the originally laid wire gauge allows for further[clarification needed] distribution. At the telephone exchange, the line generally terminates at a digital subscriber line access multiplexer (DSLAM) where another frequency splitter separates the voice band signal for the conventional phone network. Data carried by the ADSL are typically routed over the telephone company's data network and eventually reach a conventional Internet Protocol network. There are both technical and marketing reasons why ADSL is in many places the most common type offered to home users. On the technical side, there is likely to be more crosstalk from other circuits at the DSLAM end (where the wires from many local loops are close to each other) than at the customer premises. Thus the upload signal is weakest at the noisiest part of the local loop, while the download signal is strongest at the noisiest part of the local loop. It therefore makes technical sense to have the DSLAM transmit at a higher bit rate than does the modem on the customer end. Since the typical home user in fact does prefer a higher download speed, the telephone companies chose to make a virtue out of necessity, hence ADSL. The marketing reasons for an asymmetric connection are that, firstly, most users of internet traffic will require less data to be uploaded than downloaded. For example, in normal web browsing, a user will visit a number of web sites and will need to download the data that comprises the web pages from the site, images, text, sound files etc. but they will only upload a small amount of data, as the only uploaded data is that used for the purpose of verifying the receipt of the downloaded data or any data inputted by the user into forms etc. This provides a justification for internet service providers to offer a more expensive service aimed at commercial users who host websites, and who therefore need a service which allows for as much data to be uploaded as downloaded. File sharing applications are an obvious exception to this situation. Secondly internet service providers, seeking to avoid overloading of their backbone connections, have traditionally tried to limit uses such as file sharing which generate a lot of uploads. Currently, most ADSL communication is full-duplex. Full-duplex ADSL communication is usually achieved on a wire pair by either frequency-division duplex (FDD), echo-cancelling duplex (ECD), or time-division duplex (TDD). FDD uses two separate frequency bands, referred to as the upstream and downstream bands. The upstream band is used for communication from the end user to the telephone central office. The downstream band is used for communicating from the central office to the end user. With commonly deployed ADSL over POTS (Annex A), the band from 26.075\u00a0kHz to 137.825\u00a0kHz is used for upstream communication, while 138\u20131104\u00a0kHz is used for downstream communication. Under the usual DMT scheme, each of these is further divided into smaller frequency channels of 4.3125\u00a0kHz. These frequency channels are sometimes termed bins. During initial training to optimize transmission quality and speed, the ADSL modem tests each of the bins to determine the signal-to-noise ratio at each bin's frequency. Distance from the telephone exchange, cable characteristics, interference from AM radio stations, and local interference and electrical noise at the modem's location can adversely affect the signal-to-noise ratio at particular frequencies. Bins for frequencies exhibiting a reduced signal-to-noise ratio will be used at a lower throughput rate or not at all; this reduces the maximum link capacity but allows the modem to maintain an adequate connection. The DSL modem will make a plan on how to exploit each of the bins, sometimes termed \"bits per bin\" allocation. Those bins that have a good signal-to-noise ratio (SNR) will be chosen to transmit signals chosen from a greater number of possible encoded values (this range of possibilities equating to more bits of data sent) in each main clock cycle. The number of possibilities must not be so large that the receiver might incorrectly decode which one was intended in the presence of noise. Noisy bins may only be required to carry as few as two bits, a choice from only one of four possible patterns, or only one bit per bin in the case of ADSL2+, and very noisy bins are not used at all. If the pattern of noise versus frequencies heard in the bins changes, the DSL modem can alter the bits-per-bin allocations, in a process called \"bitswap\", where bins that have become more noisy are only required to carry fewer bits and other channels will be chosen to be given a higher burden. The data transfer capacity the DSL modem therefore reports is determined by the total of the bits-per-bin allocations of all the bins combined. Higher signal-to-noise ratios and more bins being in use gives a higher total link capacity, while lower signal-to-noise ratios or fewer bins being used gives a low link capacity. The total maximum capacity derived from summing the bits-per-bin is reported by DSL modems and is sometimes termed sync rate. This will always be rather misleading, as the true maximum link capacity for user data transfer rate will be significantly lower; because extra data are transmitted that are termed protocol overhead, reduced figures for PPPoA connections of around 84-87 percent, at most, being common. In addition, some ISPs will have traffic policies that limit maximum transfer rates further in the networks beyond the exchange, and traffic congestion on the Internet, heavy loading on servers and slowness or inefficiency in customers' computers may all contribute to reductions below the maximum attainable. When a wireless access point is used, low or unstable wireless signal quality can also cause reduction or fluctuation of actual speed. In fixed-rate mode, the sync rate is predefined by the operator and the DSL modem chooses a bits-per-bin allocation that yields an approximately equal error rate in each bin.[3] In variable-rate mode, the bits-per-bin are chosen to maximize the sync rate, subject to a tolerable error risk.[3] These choices can either be conservative, where the modem chooses to allocate fewer bits per bin than it possibly could, a choice which makes for a slower connection, or less conservative in which more bits per bin are chosen in which case there is a greater risk case of error should future signal-to-noise ratios deteriorate to the point where the bits-per-bin allocations chosen are too high to cope with the greater noise present. This conservatism, involving a choice of using fewer bits per bin as a safeguard against future noise increases, is reported as the signal-to-noise ratio margin or SNR margin. The telephone exchange can indicate a suggested SNR margin to the customer's DSL modem when it initially connects, and the modem may make its bits-per-bin allocation plan accordingly. A high SNR margin will mean a reduced maximum throughput, but greater reliability and stability of the connection. A low SNR margin will mean high speeds, provided the noise level does not increase too much; otherwise, the connection will have to be dropped and renegotiated (resynced). ADSL2+ can better accommodate such circumstances, offering a feature termed seamless rate adaptation (SRA), which can accommodate changes in total link capacity with less disruption to communications. Vendors may support usage of higher frequencies as a proprietary extension to the standard. However, this requires matching vendor-supplied equipment on both ends of the line, and will likely result in crosstalk problems that affect other lines in the same bundle. There is a direct relationship between the number of channels available and the throughput capacity of the ADSL connection. The exact data capacity per channel depends on the modulation method used. ADSL initially existed in two versions (similar to VDSL), namely CAP and DMT. CAP was the de facto standard for ADSL deployments up until 1996, deployed in 90 percent of ADSL installations at the time. However, DMT was chosen for the first ITU-T ADSL standards, G.992.1 and G.992.2 (also called G.dmt and G.lite respectively). Therefore, all modern installations of ADSL are based on the DMT modulation scheme. ISPs (rarely, users apart from Australia where its default[4]) have the option to use interleaving of packets to counter the effects of burst noise on the telephone line. An interleaved line has a depth, usually 8 to 64, which describes how many Reed\u2013Solomon codewords are accumulated before they are sent. As they can all be sent together, their forward error correction codes can be made more resilient. Interleaving adds latency as all the packets have to first be gathered (or replaced by empty packets) and they, of course, all take time to transmit. 8 frame interleaving adds 5 ms round-trip-time, while 64 deep interleaving adds 25 ms. Other possible depths are 16 and 32. \"Fastpath\" connections have an interleaving depth of 1, that is one packet is sent at a time. This has a low latency, usually around 10 ms (interleaving adds to it, this is not greater than interleaved) but it is extremely prone to errors, as any burst of noise can take out the entire packet and so require it all to be retransmitted. Such a burst on a large interleaved packet only blanks part of the packet, it can be recovered from error correction information in the rest of the packet. A \"fastpath\" connection will result in extremely high latency on a poor line, as each packet will take many retries. ADSL deployment on an existing plain old telephone service (POTS) telephone line presents some problems because the DSL is within a frequency band that might interact unfavourably with existing equipment connected to the line. It is therefore necessary to install appropriate frequency filters at the customer's premises to avoid interference between the DSL, voice services, and any other connections to the line (for example intruder alarms). This is desirable for the voice service and essential for a reliable ADSL connection. In the early days of DSL, installation required a technician to visit the premises. A splitter or microfilter was installed near the demarcation point, from which a dedicated data line was installed. This way, the DSL signal is separated as close as possible to the central office and is not attenuated inside the customer's premises. However, this procedure was costly, and also caused problems with customers complaining about having to wait for the technician to perform the installation. So, many DSL providers started offering a \"self-install\" option, in which the provider provided equipment and instructions to the customer. Instead of separating the DSL signal at the demarcation point, the DSL signal is filtered at each telephone outlet by use of a low-pass filter for voice and a high-pass filter for data, usually enclosed in what is known as a microfilter. This microfilter can be plugged by an end user into any telephone jack: it does not require any rewiring at the customer's premises. Commonly, microfilters are only low-pass filters, so beyond them only low frequencies (voice signals) can pass. In the data section, a microfilter is not used because digital devices that are intended to extract data from the DSL signal will, themselves, filter out low frequencies. Voice telephone devices will pick up the entire spectrum so high frequencies, including the ADSL signal, will be \"heard\" as noise in telephone terminals, and will affect and often degrade the service in fax, dataphones and modems. From the point of view of DSL devices, any acceptance of their signal by POTS devices mean that there is a degradation of the DSL signal to the devices, and this is the central reason why these filters are required. A side effect of the move to the self-install model is that the DSL signal can be degraded, especially if more than 5 voiceband (that is, POTS telephone-like) devices are connected to the line. Once a line has had DSL enabled, the DSL signal is present on all telephone wiring in the building, causing attenuation and echo. A way to circumvent this is to go back to the original model, and install one filter upstream from all telephone jacks in the building, except for the jack to which the DSL modem will be connected. Since this requires wiring changes by the customer, and may not work on some household telephone wiring, it is rarely done. It is usually much easier to install filters at each telephone jack that is in use. DSL signals may be degraded by older telephone lines, surge protectors, poorly designed microfilters, Repetitive Electrical Impulse Noise, and by long telephone extension cords. Telephone extension cords are typically made with small-gauge, multi-strand copper conductors which do not maintain a noise-reducing pair twist. Such cable is more susceptible to electromagnetic interference and has more attenuation than solid twisted-pair copper wires typically wired to telephone jacks. These effects are especially significant where the customer's phone line is more than 4\u00a0km from the DSLAM in the telephone exchange, which causes the signal levels to be lower relative to any local noise and attenuation. This will have the effect of reducing speeds or causing connection failures. ADSL defines three \"Transmission protocol-specific transmission convergence (TPS-TC)\" layers:[5] In home installation, the prevalent transport protocol is ATM. On top of ATM, there are multiple possibilities of additional layers of protocols (two of them are abbreviated in a simplified manner as \"PPPoA\" or \"PPPoE\"), with the all-important TCP/IP at layers 4 and 3 respectively of the OSI model providing the connection to the Internet. The theoretical maximum download speed reachable by ADSL 2+ is dependant of distance from user modem to the DSLAM. Real download speeds are a little lower."}, "ADT": {"link": "https://en.wikipedia.org/wiki/Abstract_data_type", "full_form": "Abstract Data Type", "content": "In computer science, an abstract data type (ADT) is a mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user. Formally, an ADT may be defined as a \"class of objects whose logical behavior is defined by a set of values and a set of operations\";[1] this is analogous to an algebraic structure in mathematics. What is meant by \"behavior\" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model;[2] these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (\"cost\"), both in terms of time (for computing operations) and space (for representing values). In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded. ADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages\u2014mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.[3]   For example, integers are an ADT, defined as the values \u2026, \u22122, \u22121, 0, 1, 2, \u2026, and by the operations of addition, subtraction, multiplication, and division, together with greater than, less than, etc., which behave according to familiar mathematics (with care for integer division), independently of how the integers are represented by the computer.[a] Explicitly, \"behavior\" includes obeying various axioms (associativity and commutativity of addition etc.), and preconditions on operations (cannot divide by zero). Typically integers are represented in a data structure as binary numbers, most often as two's complement, but might be binary-coded decimal or in ones' complement, but the user is abstracted from the concrete choice of representation, and can simply use the data as integers. An ADT consists not only of operations, but also of values of the underlying data and of constraints on the operations. An \"interface\" typically refers only to the operations, and perhaps some of the constraints on the operations, notably pre-conditions and post-conditions, but not other constraints, such as relations between the operations. For example, an abstract stack, which is a last-in-first-out structure, could be defined by three operations: push, that inserts a data item onto the stack; pop, that removes a data item from it; and peek or top, that accesses a data item on top of the stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a data item into the queue; dequeue, that removes the first data item from it; and front, that accesses and serves the first data item in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many data items have been pushed into the stack, and that the stack uses a constant amount of storage for each element. Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages. However, an ADT may be implemented by specific data types or data structures, in many ways and in many programming languages; or described in a formal specification language. ADTs are often implemented as modules: the module's interface declares procedures that correspond to the ADT operations, sometimes with comments that describe the constraints. This information hiding strategy allows the implementation of the module to be changed without disturbing the client programs. The term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings.[4] The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development.[5] An abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects. There are no standard conventions for defining them. A broad division may be drawn between \"imperative\" and \"functional\" definition styles. In the philosophy of imperative programming languages, an abstract data structure is conceived as an entity that is mutable\u2014meaning that it may be in different states at different times. Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times\u2014just like the instructions of a computer, or the commands and procedures of an imperative language. To underscore this view, it is customary to say that the operations are executed or applied, rather than evaluated. The imperative style is often used when describing abstract algorithms. (See The Art of Computer Programming by Donald Knuth for more details) Imperative-style definitions of ADT often depend on the concept of an abstract variable, which may be regarded as the simplest non-trivial ADT. An abstract variable V is a mutable entity that admits two operations: with the constraint that As in so many programming languages, the operation store(V, x) is often written V \u2190 x (or some similar notation), and fetch(V) is implied whenever a variable V is used in a context where a value is required. Thus, for example, V \u2190 V + 1 is commonly understood to be a shorthand for store(V,fetch(V) + 1). In this definition, it is implicitly assumed that storing a value into a variable U has no effect on the state of a distinct variable V. To make this assumption explicit, one could add the constraint that More generally, ADT definitions often assume that any operation that changes the state of one ADT instance has no effect on the state of any other instance (including other instances of the same ADT) \u2014 unless the ADT axioms imply that the two instances are connected (aliased) in that sense. For example, when extending the definition of abstract variable to include abstract records, the operation that selects a field from a record variable R must yield a variable V that is aliased to that part of R. The definition of an abstract variable V may also restrict the stored values x to members of a specific set X, called the range or type of V. As in programming languages, such restrictions may simplify the description and analysis of algorithms, and improve their readability. Note that this definition does not imply anything about the result of evaluating fetch(V) when V is un-initialized, that is, before performing any store operation on V. An algorithm that does so is usually considered invalid, because its effect is not defined. (However, there are some important algorithms whose efficiency strongly depends on the assumption that such a fetch is legal, and returns some arbitrary value in the variable's range.[citation needed]) Some algorithms need to create new instances of some ADT (such as new variables, or new stacks). To describe such algorithms, one usually includes in the ADT definition a create() operation that yields an instance of the ADT, usually with axioms equivalent to This axiom may be strengthened to exclude also partial aliasing with other instances. On the other hand, this axiom still allows implementations of create() to yield a previously created instance that has become inaccessible to the program. As another example, an imperative-style definition of an abstract stack could specify that the state of a stack S can be modified only by the operations with the constraint that Since the assignment V \u2190 x, by definition, cannot change the state of S, this condition implies that V \u2190 pop(S) restores S to the state it had before the push(S, x). From this condition and from the properties of abstract variables, it follows, for example, that the sequence where x, y, and z are any values, and U, V, W are pairwise distinct variables, is equivalent to Here it is implicitly assumed that operations on a stack instance do not modify the state of any other ADT instance, including other stacks; that is, An abstract stack definition usually includes also a Boolean-valued function empty(S) and a create() operation that returns a stack instance, with axioms equivalent to Sometimes an ADT is defined as if only one instance of it existed during the execution of the algorithm, and all operations were applied to that instance, which is not explicitly notated. For example, the abstract stack above could have been defined with operations push(x) and pop(), that operate on the only existing stack. ADT definitions in this style can be easily rewritten to admit multiple coexisting instances of the ADT, by adding an explicit instance parameter (like S in the previous example) to every operation that uses or modifies the implicit instance. On the other hand, some ADTs cannot be meaningfully defined without assuming multiple instances. This is the case when a single operation takes two distinct instances of the ADT as parameters. For an example, consider augmenting the definition of the abstract stack with an operation compare(S, T) that checks whether the stacks S and T contain the same items in the same order. Another way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. In this view, any operation that modifies the ADT is modeled as a mathematical function that takes the old state as an argument, and returns the new state as part of the result. Unlike the imperative operations, these functions have no side effects. Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states). In the functional view, in particular, there is no way (or need) to define an \"abstract variable\" with the semantics of imperative variables (namely, with fetch and store operations). Instead of storing values into variables, one passes them as arguments to functions. For example, a complete functional-style definition of an abstract stack could use the three operations: In a functional-style definition there is no need for a create operation. Indeed, there is no notion of \"stack instance\". The stack states can be thought of as being potential states of a single stack structure, and two stack states that contain the same values in the same order are considered to be identical states. This view actually mirrors the behavior of some concrete implementations, such as linked lists with hash cons. Instead of create(), a functional-style definition of an abstract stack may assume the existence of a special stack state, the empty stack, designated by a special symbol like \u039b or \"()\"; or define a bottom() operation that takes no arguments and returns this special stack state. Note that the axioms imply that In a functional-style definition of a stack one does not need an empty predicate: instead, one can test whether a stack is empty by testing whether it is equal to \u039b. Note that these axioms do not define the effect of top(s) or pop(s), unless s is a stack state returned by a push. Since push leaves the stack non-empty, those two operations are undefined (hence invalid) when s = \u039b. On the other hand, the axioms (and the lack of side effects) imply that push(s, x) = push(t, y) if and only if x = y and s = t. As in some other branches of mathematics, it is customary to assume also that the stack states are only those whose existence can be proved from the axioms in a finite number of steps. In the abstract stack example above, this rule means that every stack is a finite sequence of values, that becomes the empty stack (\u039b) after a finite number of pops. By themselves, the axioms above do not exclude the existence of infinite stacks (that can be poped forever, each time yielding a different state) or circular stacks (that return to the same state after a finite number of pops). In particular, they do not exclude states s such that pop(s) = s or push(s, x) = s for some x. However, since one cannot obtain such stack states with the given operations, they are assumed \"not to exist\". Aside from the behavior in terms of axioms, it is also possible to include, in the definition of an ADT operation, their algorithmic complexity. Alexander Stepanov, designer of the C++ Standard Template Library, included complexity guarantees in the STL specification, arguing: The reason for introducing the notion of abstract data types was to allow interchangeable software modules. You cannot have interchangeable modules unless these modules share similar complexity behavior. If I replace one module with another module with the same functional behavior but with different complexity tradeoffs, the user of this code will be unpleasantly surprised. I could tell him anything I like about data abstraction, and he still would not want to use the code. Complexity assertions have to be part of the interface. Abstraction provides a promise that any implementation of the ADT has certain properties and abilities; knowing these is all that is required to make use of an ADT object. The user does not need any technical knowledge of how the implementation works to use the ADT. In this way, the implementation may be complex but will be encapsulated in a simple interface when it is actually used. Code that uses an ADT object will not need to be edited if the implementation of the ADT is changed. Since any changes to the implementation must still comply with the interface, and since code using an ADT object may only refer to properties and abilities specified in the interface, changes may be made to the implementation without requiring any changes in code where the ADT is used. Different implementations of the ADT, having all the same properties and abilities, are equivalent and may be used somewhat interchangeably in code that uses the ADT. This gives a great deal of flexibility when using ADT objects in different situations. For example, different implementations of the ADT may be more efficient in different situations; it is possible to use each in the situation where they are preferable, thus increasing overall efficiency. Some operations that are often specified for ADTs (possibly under other names) are In imperative-style ADT definitions, one often finds also The free operation is not normally relevant or meaningful, since ADTs are theoretical entities that do not \"use memory\". However, it may be necessary when one needs to analyze the storage used by an algorithm that uses the ADT. In that case one needs additional axioms that specify how much memory each ADT instance uses, as a function of its state, and how much of it is returned to the pool by free. Some common ADTs, which have proved useful in a great variety of applications, are Each of these ADTs may be defined in many ways and variants, not necessarily equivalent. For example, an abstract stack may or may not have a count operation that tells how many items have been pushed and not yet popped. This choice makes a difference not only for its clients but also for the implementation. An extension of ADT for computer graphics was proposed in 1979:[7] an abstract graphical data type (AGDT). It was introduced by Nadia Magnenat Thalmann, and Daniel Thalmann. AGDTs provide the advantages of ADTs with facilities to build graphical objects in a structured way. Implementing an ADT means providing one procedure or function for each abstract operation. The ADT instances are represented by some concrete data structure that is manipulated by those procedures, according to the ADT's specifications. Usually there are many ways to implement the same ADT, using several different concrete data structures. Thus, for example, an abstract stack can be implemented by a linked list or by an array. In order to prevent clients from depending on the implementation, an ADT is often packaged as an opaque data type in one or more modules, whose interface contains only the signature (number and types of the parameters and results) of the operations. The implementation of the module\u2014namely, the bodies of the procedures and the concrete data structure used\u2014can then be hidden from most clients of the module. This makes it possible to change the implementation without affecting the clients. If the implementation is exposed, it is known instead as a transparent data type. When implementing an ADT, each instance (in imperative-style definitions) or each state (in functional-style definitions) is usually represented by a handle of some sort.[8] Modern object-oriented languages, such as C++ and Java, support a form of abstract data types. When a class is used as a type, it is an abstract type that refers to a hidden representation. In this model an ADT is typically implemented as a class, and each instance of the ADT is usually an object of that class. The module's interface typically declares the constructors as ordinary procedures, and most of the other ADT operations as methods of that class. However, such an approach does not easily encapsulate multiple representational variants found in an ADT. It also can undermine the extensibility of object-oriented programs. In a pure object-oriented program that uses interfaces as types, types refer to behaviors not representations. As an example, here is an implementation of the abstract stack above in the C programming language. An imperative-style interface might be: This interface could be used in the following manner: This interface can be implemented in many ways. The implementation may be arbitrarily inefficient, since the formal definition of the ADT, above, does not specify how much space the stack may use, nor how long each operation should take. It also does not specify whether the stack state s continues to exist after a call x \u2190 pop(s). In practice the formal definition should specify that the space is proportional to the number of items pushed and not yet popped; and that every one of the operations above must finish in a constant amount of time, independently of that number. To comply with these additional specifications, the implementation could use a linked list, or an array (with dynamic resizing) together with two integers (an item count and the array size). Functional-style ADT definitions are more appropriate for functional programming languages, and vice versa. However, one can provide a functional-style interface even in an imperative language like C. For example: Many modern programming languages, such as C++ and Java, come with standard libraries that implement several common ADTs, such as those listed above. The specification of some programming languages is intentionally vague about the representation of certain built-in data types, defining only the operations that can be done on them. Therefore, those types can be viewed as \"built-in ADTs\". Examples are the arrays in many scripting languages, such as Awk, Lua, and Perl, which can be regarded as an implementation of the abstract list."}, "AE": {"link": "https://en.wikipedia.org/wiki/Adaptive_equalizer", "full_form": "Adaptive Equalizer", "content": "An adaptive equalizer is an equalizer that automatically adapts to time-varying properties of the communication channel.[1] It is frequently used with coherent modulations such as phase shift keying, mitigating the effects of multipath propagation and Doppler spreading. Many adaptation strategies exist. They include: A well-known example is the decision feedback equalizer,[3][4] a filter that uses feedback of detected symbols in addition to conventional equalization of future symbols.[5] Some systems use predefined training sequences to provide reference points for the adaptation process. "}, "AES": {"link": "https://en.wikipedia.org/wiki/Advanced_Encryption_Standard", "full_form": "Advanced Encryption Standard", "content": "Attacks have been published that are computationally faster than a full brute-force attack, though none as of 2013 are computationally feasible.[3] The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: [\u02c8r\u025binda\u02d0l]),[5][6] is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.[7] AES is a subset of the Rijndael cipher[6] developed by two Belgian cryptographers, Vincent Rijmen and Joan Daemen, who submitted a proposal to NIST during the AES selection process.[8] Rijndael is a family of ciphers with different key and block sizes. For AES, NIST selected three members of the Rijndael family, each with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits. AES has been adopted by the U.S. government and is now used worldwide. It supersedes the Data Encryption Standard (DES),[9] which was published in 1977. The algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data. In the United States, AES was announced by the NIST as U.S. FIPS PUB 197 (FIPS 197) on November 26, 2001.[7] This announcement followed a five-year standardization process in which fifteen competing designs were presented and evaluated, before the Rijndael cipher was selected as the most suitable (see Advanced Encryption Standard process for more details). AES became effective as a federal government standard on May 26, 2002, after approval by the Secretary of Commerce. AES is included in the ISO/IEC 18033-3 standard. AES is available in many different encryption packages, and is the first (and only) publicly accessible cipher approved by the National Security Agency (NSA) for top secret information when used in an NSA approved cryptographic module (see Security of AES, below).   The Advanced Encryption Standard (AES) is defined in each of: AES is based on a design principle known as a substitution-permutation network, a combination of both substitution and permutation, and is fast in both software and hardware.[11] Unlike its predecessor DES, AES does not use a Feistel network. AES is a variant of Rijndael which has a fixed block size of 128 bits, and a key size of 128, 192, or 256 bits. By contrast, the Rijndael specification per se is specified with block and key sizes that may be any multiple of 32 bits, both with a minimum of 128 and a maximum of 256 bits. AES operates on a 4 \u00d7 4 column-major order matrix of bytes, termed the state, although some versions of Rijndael have a larger block size and have additional columns in the state. Most AES calculations are done in a particular finite field. For instance, if there are 16 bytes, \n\n\n\n\nb\n\n0\n\n\n,\n\nb\n\n1\n\n\n,\n.\n.\n.\n,\n\nb\n\n15\n\n\n\n\n{\\displaystyle b_{0},b_{1},...,b_{15}}\n\n, these bytes are represented as this matrix: The key size used for an AES cipher specifies the number of repetitions of transformation rounds that convert the input, called the plaintext, into the final output, called the ciphertext. The number of cycles of repetition are as follows: Each round consists of several processing steps, each containing four similar but different stages, including one that depends on the encryption key itself. A set of reverse rounds are applied to transform ciphertext back into the original plaintext using the same encryption key. In the SubBytes step, each byte \n\n\n\n\na\n\ni\n,\nj\n\n\n\n\n{\\displaystyle a_{i,j}}\n\n in the state matrix is replaced with a SubByte \n\n\n\nS\n(\n\na\n\ni\n,\nj\n\n\n)\n\n\n{\\displaystyle S(a_{i,j})}\n\n using an 8-bit substitution box, the Rijndael S-box. This operation provides the non-linearity in the cipher. The S-box used is derived from the multiplicative inverse over GF(28), known to have good non-linearity properties. To avoid attacks based on simple algebraic properties, the S-box is constructed by combining the inverse function with an invertible affine transformation. The S-box is also chosen to avoid any fixed points (and so is a derangement), i.e., \n\n\n\nS\n(\n\na\n\ni\n,\nj\n\n\n)\n\u2260\n\na\n\ni\n,\nj\n\n\n\n\n{\\displaystyle S(a_{i,j})\\neq a_{i,j}}\n\n, and also any opposite fixed points, i.e., \n\n\n\nS\n(\n\na\n\ni\n,\nj\n\n\n)\n\u2295\n\na\n\ni\n,\nj\n\n\n\u2260\n\n\nFF\n\n\n16\n\n\n\n\n{\\displaystyle S(a_{i,j})\\oplus a_{i,j}\\neq {\\text{FF}}_{16}}\n\n. While performing the decryption, the InvSubBytes step (the inverse of SubBytes) is used, which requires first taking the inverse of the affine transformation and then finding the multiplicative inverse. The ShiftRows step operates on the rows of the state; it cyclically shifts the bytes in each row by a certain offset. For AES, the first row is left unchanged. Each byte of the second row is shifted one to the left. Similarly, the third and fourth rows are shifted by offsets of two and three respectively. For blocks of sizes 128 bits and 192 bits, the shifting pattern is the same. Row \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is shifted left circular by \n\n\n\nn\n\u2212\n1\n\n\n{\\displaystyle n-1}\n\n bytes. In this way, each column of the output state of the ShiftRows step is composed of bytes from each column of the input state. (Rijndael variants with a larger block size have slightly different offsets). For a 256-bit block, the first row is unchanged and the shifting for the second, third and fourth row is 1 byte, 3 bytes and 4 bytes respectively\u2014this change only applies for the Rijndael cipher when used with a 256-bit block, as AES does not use 256-bit blocks. The importance of this step is to avoid the columns being encrypted independently, in which case AES degenerates into four independent block ciphers. In the MixColumns step, the four bytes of each column of the state are combined using an invertible linear transformation. The MixColumns function takes four bytes as input and outputs four bytes, where each input byte affects all four output bytes. Together with ShiftRows, MixColumns provides diffusion in the cipher. During this operation, each column is transformed using a fixed matrix (matrix left-multiplied by column gives new value of column in the state): Matrix multiplication is composed of multiplication and addition of the entries. Entries are 8 bit bytes treated as coefficients of polynomial of order \n\n\n\n\nx\n\n7\n\n\n\n\n{\\displaystyle x^{7}}\n\n. Addition is simply XOR. Multiplication is modulo irreducible polynomial \n\n\n\n\nx\n\n8\n\n\n+\n\nx\n\n4\n\n\n+\n\nx\n\n3\n\n\n+\nx\n+\n1\n\n\n{\\displaystyle x^{8}+x^{4}+x^{3}+x+1}\n\n. If processed bit by bit then after shifting a conditional XOR with 1B16 should be performed if the shifted value is larger than FF16 (overflow must be corrected by subtraction of generating polynomial). These are special cases of the usual multiplication in \n\n\n\nGF\n\u2061\n(\n\n2\n\n8\n\n\n)\n\n\n{\\displaystyle \\operatorname {GF} (2^{8})}\n\n. In more general sense, each column is treated as a polynomial over \n\n\n\nGF\n\u2061\n(\n\n2\n\n8\n\n\n)\n\n\n{\\displaystyle \\operatorname {GF} (2^{8})}\n\n and is then multiplied modulo \n\n\n\n\nz\n\n4\n\n\n+\n1\n\n\n{\\displaystyle z^{4}+1}\n\n with a fixed polynomial \n\n\n\nc\n(\nz\n)\n=\n\n\n03\n\n\n16\n\n\n\u22c5\n\nz\n\n3\n\n\n+\n\nz\n\n2\n\n\n+\nz\n+\n\n\n02\n\n\n16\n\n\n\n\n{\\displaystyle c(z)={03}_{16}\\cdot z^{3}+z^{2}+z+{02}_{16}}\n\n. The coefficients are displayed in their hexadecimal equivalent of the binary representation of bit polynomials from \n\n\n\nGF\n\u2061\n(\n2\n)\n[\nx\n]\n\n\n{\\displaystyle \\operatorname {GF} (2)[x]}\n\n. The MixColumns step can also be viewed as a multiplication by the shown particular MDS matrix in the finite field \n\n\n\nGF\n\u2061\n(\n\n2\n\n8\n\n\n)\n\n\n{\\displaystyle \\operatorname {GF} (2^{8})}\n\n. This process is described further in the article Rijndael MixColumns. In the AddRoundKey step, the subkey is combined with the state. For each round, a subkey is derived from the main key using Rijndael's key schedule; each subkey is the same size as the state. The subkey is added by combining each byte of the state with the corresponding byte of the subkey using bitwise XOR. On systems with 32-bit or larger words, it is possible to speed up execution of this cipher by combining the SubBytes and ShiftRows steps with the MixColumns step by transforming them into a sequence of table lookups. This requires four 256-entry 32-bit tables, and utilizes a total of four kilobytes (4096 bytes) of memory\u2014one kilobyte for each table. A round can then be done with 16 table lookups and 12 32-bit exclusive-or operations, followed by four 32-bit exclusive-or operations in the AddRoundKey step.[12] If the resulting four-kilobyte table size is too large for a given target platform, the table lookup operation can be performed with a single 256-entry 32-bit (i.e. 1 kilobyte) table by the use of circular rotates. Using a byte-oriented approach, it is possible to combine the SubBytes, ShiftRows, and MixColumns steps into a single round operation.[13] Until May 2009, the only successful published attacks against the full AES were side-channel attacks on some specific implementations. The National Security Agency (NSA) reviewed all the AES finalists, including Rijndael, and stated that all of them were secure enough for U.S. Government non-classified data. In June 2003, the U.S. Government announced that AES could be used to protect classified information: The design and strength of all key lengths of the AES algorithm (i.e., 128, 192 and 256) are sufficient to protect classified information up to the SECRET level. TOP SECRET information will require use of either the 192 or 256 key lengths. The implementation of AES in products intended to protect national security systems and/or information must be reviewed and certified by NSA prior to their acquisition and use.[14] AES has 10 rounds for 128-bit keys, 12 rounds for 192-bit keys, and 14 rounds for 256-bit keys. By 2006, the best known attacks were on 7 rounds for 128-bit keys, 8 rounds for 192-bit keys, and 9 rounds for 256-bit keys.[15] For cryptographers, a cryptographic \"break\" is anything faster than a brute-force attack \u2013 i.e., performing one trial decryption for each possible key in sequence (see Cryptanalysis). A break can thus include results that are infeasible with current technology. Despite being impractical, theoretical breaks can sometimes provide insight into vulnerability patterns. The largest successful publicly known brute-force attack against a widely implemented block-cipher encryption algorithm was against a 64-bit RC5 key by distributed.net in 2006.[16] The key space increases by a factor of 2 for each additional bit of key length, and if every possible value of the key is equiprobable, this translates into a doubling of the average brute-force key search time. This implies that the effort of a brute-force search increases exponentially with key length. Key length in itself does not imply security against attacks, since there are ciphers with very long keys that have been found to be vulnerable. AES has a fairly simple algebraic framework.[17] In 2002, a theoretical attack, named the \"XSL attack\", was announced by Nicolas Courtois and Josef Pieprzyk, purporting to show a weakness in the AES algorithm, partially due to the low complexity of its nonlinear components.[18] Since then, other papers have shown that the attack, as originally presented, is unworkable; see XSL attack on block ciphers. During the AES selection process, developers of competing algorithms wrote of Rijndael's algorithm \"...we are concerned about [its] use ... in security-critical applications.\"[19] In October 2000, however, at the end of the AES selection process, Bruce Schneier, a developer of the competing algorithm Twofish, wrote that while he thought successful academic attacks on Rijndael would be developed someday, \"he does not \"believe that anyone will ever discover an attack that will allow someone to read Rijndael traffic.\"[20] In 2009, a new attack was discovered that exploits the simplicity of AES's key schedule and has a complexity of 2119. In December 2009 it was improved to 299.5.[4] This is a follow-up to an attack discovered earlier in 2009 by Alex Biryukov, Dmitry Khovratovich, and Ivica Nikoli\u0107, with a complexity of 296 for one out of every 235 keys.[21] However, related-key attacks are not of concern in any properly designed cryptographic protocol, as a properly designed protocol (i.e., implementational software) will take care not to allow related-keys, forcing key choice to be as random as possible. Another attack was blogged by Bruce Schneier[22] on July 30, 2009, and released as a preprint[23] on August 3, 2009. This new attack, by Alex Biryukov, Orr Dunkelman, Nathan Keller, Dmitry Khovratovich, and Adi Shamir, is against AES-256 that uses only two related keys and 239 time to recover the complete 256-bit key of a 9-round version, or 245 time for a 10-round version with a stronger type of related subkey attack, or 270 time for an 11-round version. 256-bit AES uses 14 rounds, so these attacks aren't effective against full AES. The practicality of these attacks with stronger related keys has been criticized,[24] for instance, by the paper on \"chosen-key-relations-in-the-middle\" attacks on AES-128 authored by Vincent Rijmen in 2010.[25] In November 2009, the first known-key distinguishing attack against a reduced 8-round version of AES-128 was released as a preprint.[26] This known-key distinguishing attack is an improvement of the rebound, or the start-from-the-middle attack, against AES-like permutations, which view two consecutive rounds of permutation as the application of a so-called Super-Sbox. It works on the 8-round version of AES-128, with a time complexity of 248, and a memory complexity of 232. 128-bit AES uses 10 rounds, so this attack isn't effective against full AES-128. The first key-recovery attacks on full AES were due to Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger, and were published in 2011.[27] The attack is a biclique attack and is faster than brute force by a factor of about four. It requires 2126.2 operations to recover an AES-128 key. For AES-192 and AES-256, 2190.2 and 2254.6 operations are needed, respectively. This result has been further improved to 2126.0 for AES-128, 2189.9 for AES-192 and 2254.3 for AES-256,[28] which are the current best results in key recovery attack against AES. This is a very small gain, as a 126-bit key (instead of 128-bits) would still take billions of years to brute force on current and foreseeable hardware. Also, the authors calculate the best attack using their technique on AES with a 128 bit key requires storing 288 bits of data (though this has later been improved to 256,[28] which is 9 petabytes). That works out to about 38 trillion terabytes of data, which is more than all the data stored on all the computers on the planet in 2016. As such this is a seriously impractical attack which has no practical implication on AES security.[29] According to the Snowden documents, the NSA is doing research on whether a cryptographic attack based on tau statistic may help to break AES.[30] At present, there is no known practical attack that would allow someone without knowledge of the key to read data encrypted by AES when correctly implemented. Side-channel attacks do not attack the cipher as a black box, and thus are not related to cipher security as defined in the classical context, but are important in practice. They attack implementations of the cipher on hardware or software systems that inadvertently leak data. There are several such known attacks on various implementations of AES. In April 2005, D.J. Bernstein announced a cache-timing attack that he used to break a custom server that used OpenSSL's AES encryption.[31] The attack required over 200 million chosen plaintexts.[32] The custom server was designed to give out as much timing information as possible (the server reports back the number of machine cycles taken by the encryption operation); however, as Bernstein pointed out, \"reducing the precision of the server's timestamps, or eliminating them from the server's responses, does not stop the attack: the client simply uses round-trip timings based on its local clock, and compensates for the increased noise by averaging over a larger number of samples.\"[31] In October 2005, Dag Arne Osvik, Adi Shamir and Eran Tromer presented a paper demonstrating several cache-timing attacks against AES.[33] One attack was able to obtain an entire AES key after only 800 operations triggering encryptions, in a total of 65 milliseconds. This attack requires the attacker to be able to run programs on the same system or platform that is performing AES. In December 2009 an attack on some hardware implementations was published that used differential fault analysis and allows recovery of a key with a complexity of 232.[34] In November 2010 Endre Bangerter, David Gullasch and Stephan Krenn published a paper which described a practical approach to a \"near real time\" recovery of secret keys from AES-128 without the need for either cipher text or plaintext. The approach also works on AES-128 implementations that use compression tables, such as OpenSSL.[35] Like some earlier attacks this one requires the ability to run unprivileged code on the system performing the AES encryption, which may be achieved by malware infection far more easily than commandeering the root account.[36] In March 2016, Ashokkumar C., Ravi Prakash Giri and Bernard Menezes presented a very efficient side-channel attack on AES that can recover the complete 128-bit AES key in just 6\u20137 blocks of plaintext/ciphertext which is a substantial improvement over previous works that require between 100 and a million encryptions.[37] The proposed attack require standard user privilege as previous attacks and key-retrieval algorithms run under a minute. Many modern CPUs have built-in hardware instructions for AES, which would protect against timing-related side-channel attacks.[38][39] The Cryptographic Module Validation Program (CMVP) is operated jointly by the United States Government's National Institute of Standards and Technology (NIST) Computer Security Division and the Communications Security Establishment (CSE) of the Government of Canada. The use of cryptographic modules validated to NIST FIPS 140-2 is required by the United States Government for encryption of all data that has a classification of Sensitive but Unclassified (SBU) or above. From NSTISSP #11, National Policy Governing the Acquisition of Information Assurance: \"Encryption products for protecting classified information will be certified by NSA, and encryption products intended for protecting sensitive information will be certified in accordance with NIST FIPS 140-2.\"[40] The Government of Canada also recommends the use of FIPS 140 validated cryptographic modules in unclassified applications of its departments. Although NIST publication 197 (\"FIPS 197\") is the unique document that covers the AES algorithm, vendors typically approach the CMVP under FIPS 140 and ask to have several algorithms (such as Triple\u00a0DES or SHA1) validated at the same time. Therefore, it is rare to find cryptographic modules that are uniquely FIPS 197 validated and NIST itself does not generally take the time to list FIPS 197 validated modules separately on its public web site. Instead, FIPS 197 validation is typically just listed as an \"FIPS approved: AES\" notation (with a specific FIPS 197 certificate number) in the current list of FIPS 140 validated cryptographic modules. The Cryptographic Algorithm Validation Program (CAVP)[41] allows for independent validation of the correct implementation of the AES algorithm at a reasonable cost[citation needed]. Successful validation results in being listed on the NIST validations page.[42] This testing is a pre-requisite for the FIPS 140-2 module validation described below. However, successful CAVP validation in no way implies that the cryptographic module implementing the algorithm is secure. A cryptographic module lacking FIPS 140-2 validation or specific approval by the NSA is not deemed secure by the US Government and cannot be used to protect government data.[40] FIPS 140-2 validation is challenging to achieve both technically and fiscally.[43] There is a standardized battery of tests as well as an element of source code review that must be passed over a period of a few weeks. The cost to perform these tests through an approved laboratory can be significant (e.g., well over $30,000 US)[43] and does not include the time it takes to write, test, document and prepare a module for validation. After validation, modules must be re-submitted and re-evaluated if they are changed in any way. This can vary from simple paperwork updates if the security functionality did not change to a more substantial set of re-testing if the security functionality was impacted by the change. Test vectors are a set of known ciphers for a given input and key. NIST distributes the reference of AES test vectors as AES Known Answer Test (KAT) Vectors (in ZIP format). High speed and low RAM requirements were criteria of the AES selection process. As the chosen algorithm, AES performed well on a wide variety of hardware, from 8-bit smart cards to high-performance computers. On a Pentium Pro, AES encryption requires 18 clock cycles per byte,[44] equivalent to a throughput of about 11\u00a0MB/s for a 200\u00a0MHz processor. On a 1.7\u00a0GHz Pentium M throughput is about 60\u00a0MB/s. On Intel Core i3/i5/i7 and AMD APU and FX CPUs supporting AES-NI instruction set extensions, throughput can be over 700\u00a0MB/s.[45]"}, "AF": {"link": "https://en.wikipedia.org/wiki/Anisotropic_filtering", "full_form": "Anisotropic Filtering", "content": "In 3D computer graphics, anisotropic filtering (abbreviated AF) is a method of enhancing the image quality of textures on surfaces of computer graphics that are at oblique viewing angles with respect to the camera where the projection of the texture (not the polygon or other primitive on which it is rendered) appears to be non-orthogonal (thus the origin of the word: \"an\" for not, \"iso\" for same, and \"tropic\" from tropism, relating to direction; anisotropic filtering does not filter the same in every direction). Like bilinear and trilinear filtering, anisotropic filtering eliminates aliasing effects,[1][2] but improves on these other techniques by reducing blur and preserving detail at extreme viewing angles. Anisotropic filtering is relatively intensive (primarily memory bandwidth and to some degree computationally, though the standard space\u2013time tradeoff rules apply) and only became a standard feature of consumer-level graphics cards in the late 1990s.[3] Anisotropic filtering is now common in modern graphics hardware (and video driver software) and is enabled either by users through driver settings or by graphics applications and video games through programming interfaces.   From this point forth, it is assumed the reader is familiar with MIP mapping. If we were to explore a more approximate anisotropic algorithm, RIP mapping, as an extension from MIP mapping, we can understand how anisotropic filtering gains so much texture mapping quality.[4] If we need to texture a horizontal plane which is at an oblique angle to the camera, traditional MIP map minification would give us insufficient horizontal resolution due to the reduction of image frequency in the vertical axis. This is because in MIP mapping each MIP level is isotropic, so a 256 \u00d7 256 texture is downsized to a 128 \u00d7 128 image, then a 64 \u00d7 64 image and so on, so resolution halves on each axis simultaneously, so a MIP map texture probe to an image will always sample an image that is of equal frequency in each axis. Thus, when sampling to avoid aliasing on a high-frequency axis, the other texture axes will be similarly downsampled and therefore potentially blurred. With RIP map anisotropic filtering, in addition to downsampling to 128 \u00d7 128, images are also sampled to 256 \u00d7 128 and 32 \u00d7 128 etc. These anisotropically downsampled images can be probed when the texture-mapped image frequency is different for each texture axis. Therefore, one axis need not blur due to the screen frequency of another axis, and aliasing is still avoided. Unlike more general anisotropic filtering, the RIP mapping described for illustration is limited by only supporting anisotropic probes that are axis-aligned in texture space, so diagonal anisotropy still presents a problem, even though real-use cases of anisotropic texture commonly have such screenspace mappings. Although implementations are free to vary their methods, RIP mapping and the associated axis aligned constraints mean it is suboptimal for true anisotropic filtering and is used here for illustrative purposes only. Fully anisotropic implementation is described below. In layman's terms, anisotropic filtering retains the \"sharpness\" of a texture normally lost by MIP map texture's attempts to avoid aliasing. Anisotropic filtering can therefore be said to maintain crisp texture detail at all viewing orientations while providing fast anti-aliased texture filtering. Different degrees or ratios of anisotropic filtering can be applied during rendering and current hardware rendering implementations set an upper bound on this ratio.[5] This degree refers to the maximum ratio of anisotropy supported by the filtering process. For example, 4:1 (pronounced \u201c4-to-1\u201d) anisotropic filtering will continue to sharpen more oblique textures beyond the range sharpened by 2:1.[6] In practice what this means is that in highly oblique texturing situations a 4:1 filter will be twice as sharp as a 2:1 filter (it will display frequencies double that of the 2:1 filter). However, most of the scene will not require the 4:1 filter; only the more oblique and usually more distant pixels will require the sharper filtering. This means that as the degree of anisotropic filtering continues to double there are diminishing returns in terms of visible quality with fewer and fewer rendered pixels affected, and the results become less obvious to the viewer. When one compares the rendered results of an 8:1 anisotropically filtered scene to a 16:1 filtered scene, only a relatively few highly oblique pixels, mostly on more distant geometry, will display visibly sharper textures in the scene with the higher degree of anisotropic filtering, and the frequency information on these few 16:1 filtered pixels will only be double that of the 8:1 filter. The performance penalty also diminishes because fewer pixels require the data fetches of greater anisotropy. In the end it is the additional hardware complexity vs. these diminishing returns, which causes an upper bound to be set on the anisotropic quality in a hardware design. Applications and users are then free to adjust this trade-off through driver and software settings up to this threshold. True anisotropic filtering probes the texture anisotropically on the fly on a per-pixel basis for any orientation of anisotropy. In graphics hardware, typically when the texture is sampled anisotropically, several probes (texel samples) of the texture around the center point are taken, but on a sample pattern mapped according to the projected shape of the texture at that pixel,[7] although earlier software methods have used summed area tables.[8] Each anisotropic filtering probe is often in itself a filtered MIP map sample, which adds more sampling to the process. Sixteen trilinear anisotropic samples might require 128 samples from the stored texture, as trilinear MIP map filtering needs to take four samples times two MIP levels and then anisotropic sampling (at 16-tap) needs to take sixteen of these trilinear filtered probes. However, this level of filtering complexity is not required all the time. There are commonly available methods to reduce the amount of work the video rendering hardware must do. The anisotropic filtering method most commonly implemented on graphics hardware is the composition of the filtered pixel values from only one line of MIP map samples. In general the method of building a texture filter result from multiple probes filling a projected pixel sampling into texture space is referred to as \"footprint assembly\", even where implementation details vary.[9][10][11] The sample count required can make anisotropic filtering extremely bandwidth-intensive. Multiple textures are common; each texture sample could be four bytes or more, so each anisotropic pixel could require 512 bytes from texture memory, although texture compression is commonly used to reduce this.[12] A video display device can easily contain over two million pixels, and desired application framerates are often upwards of 60 frames per second. As a result, the required texture memory bandwidth may grow to large values. Ranges of hundreds of gigabytes per second of pipeline bandwidth for texture rendering operations is not unusual where anisotropic filtering operations are involved.[13] Fortunately, several factors mitigate in favor of better performance:"}, "AFP": {"link": "https://en.wikipedia.org/wiki/Apple_Filing_Protocol", "full_form": "Apple Filing Protocol", "content": "The Apple Filing Protocol (AFP), formerly AppleTalk Filing Protocol, is a proprietary network protocol, and part of the Apple File Service (AFS), that offers file services for macOS and the classic Mac OS. In macOS, AFP is one of several file services supported, with others including Server Message Block (SMB), Network File System (NFS), File Transfer Protocol (FTP), and WebDAV. AFP currently supports Unicode file names, POSIX and access control list permissions, resource forks, named extended attributes, and advanced file locking. In Mac OS 9 and earlier, AFP was the primary protocol for file services.   AFP versions 3.0 and greater rely exclusively on TCP/IP (port 548 or 427) for establishing communication, supporting AppleTalk only as a service discovery protocol. The AFP 2.x family supports both TCP/IP (using Data Stream Interface) and AppleTalk for communication and service discovery. Many third-party AFP implementations use AFP 2.x, thereby supporting AppleTalk as a connection method. Still earlier versions rely exclusively on AppleTalk. For this reason, some older literature refers to AFP as \"AppleTalk Filing Protocol\". Other literature may refer to AFP as \"AppleShare\", the name of the Mac OS 9 (and earlier) AFP client. Notable current compatibility topics are: Early implementations of AFP server software were available in Mac OS starting with System 6, in AppleShare and AppleShare IP, and in early \"1.x\" releases of Mac OS X Server. In client operating systems, AFP was called \"Personal File Sharing\", and supported up to ten simultaneous connections.[1] These AFP implementations relied on version 1.x or 2.x of the protocol. AppleShare IP 5.x, 6.x, and the \"1.x\" releases of Mac OS X Server introduced AFP version 2.2. This was the first version to offer transport connections using TCP/IP as well as AppleTalk. It also increased the maximum share point size from four gibibytes to two tebibytes,[1] although the maximum file size that could be stored remained at two gibibytes due to limitations in the original Mac OS.[2] Changes made in AFP since version 3.0 represent major advances in the protocol, introducing features designed specifically for Mac OS X clients. However, like the AppleShare client in original Mac OS, the AFP client in Mac OS X continues to support type and creator codes, along with filename extensions. AFP 3.0 was introduced in Mac OS X Server 10.0.3, and was used through Mac OS X Server 10.1.5. It was the first version to use the UNIX-style POSIX permissions model and Unicode UTF-8 file name encodings. Version 3.0 supported a maximum share point and file size of two tebibytes, the maximum file size and volume size for Mac OS X until version 10.2.[3] (Note that the maximum file size changed from version 2.2, described above.) Before AFP 3.0, 31 bytes was the maximum length of a filename sent over AFP. AFP 3.1 was introduced in Mac OS X Server version 10.2. Notable changes included support for Kerberos authentication, automatic client reconnect, NFS resharing, and secure AFP connections via Secure Shell (SSH). The maximum share point and file size increased to 8 tebibytes with Mac OS X Server 10.2,[3][4] and then to 16 tebibytes with Mac OS X Server 10.3.[3][5] AFP 3.2 adds support for Access Control Lists and extended attributes in Mac OS X Server 10.4. Maximum share point size is at least 16 tebibytes, although Apple has not published a limits document for Mac OS X Server 10.4. AFP 3.2+ was introduced in Mac OS X Leopard and adds case sensitivity support and improves support for Time Machine (synchronization, lock stealing, and sleep notifications). AFP 3.3 mandates support for Replay Cache functionality (required for Time Machine). AFP 3.4, introduced in OS X Mountain Lion, includes a minor change in the mapping of POSIX errors to AFP errors. See Apple's Developer documentation on AFP Version Differences.[6] In Mac OS X Tiger, users can connect to AFP servers by browsing for them in the Network globe or entering an AFP Uniform Resource Locator (URL) into the Connect to Server dialog. In Mac OS X Leopard and later releases, AFP shares are displayed in the Finder side-bar. AFP URLs take the form: afp://<server>/<share>, where <server> is the server's IP address, Domain Name System (DNS) name, or Bonjour name, and <share> is the name of the share point. In Snow Leopard and later, a URL of the form afp://<server>/<share>/<path> can be used to mount a subdirectory underneath a share point. macOS also offers Personal File Sharing, a \"light\" implementation of the current version of AFP. In Mac OS X 10.4, users can share the contents of their Public folders by checking Personal File Sharing in the Sharing section of System Preferences. AFP URLs for AppleTalk servers took the form: afp://at/<AppleTalk name>:<AppleTalk zone>. For networks without AppleTalk zones, an asterisk (*) would be substituted for the zone name. Third party server implementations of AFP are available from a number of companies."}, "AGP": {"link": "https://en.wikipedia.org/wiki/Accelerated_Graphics_Port", "full_form": "Accelerated Graphics Port", "content": "The Accelerated Graphics Port (AGP) is a high-speed point-to-point channel for attaching a video card to a computer system, primarily to assist in the acceleration of 3D computer graphics. It was originally designed as a successor to PCI-type connections for video cards. Since 2004, AGP has been progressively phased out in favor of PCI Express (PCIe); by mid-2008, PCI Express cards dominated the market and only a few AGP models were available.[1]   As computers increasingly became graphically oriented, successive generations of graphics adapters began to push the limits of PCI, a bus with shared bandwidth. This led to the development of AGP, a \"bus\" dedicated to graphics adapters. AGP is heavily based on PCI, and in fact the AGP bus is a superset of the conventional PCI bus, and AGP cards must act as PCI cards. The primary advantage of AGP over PCI is that it provides a dedicated pathway between the slot and the processor rather than sharing the PCI bus. In addition to a lack of contention for the bus, the direct connection allows for higher clock speeds. The second major change is that AGP uses split transactions, where the address and data phases of a PCI transaction are separated. The card may send many address phases, and the host processes them in order. This avoids long delays, with the bus idle, during read operations. Third, PCI bus handshaking is simplified. Unlike PCI bus transactions whose length is negotiated on a cycle-by-cycle basis using the FRAME# and STOP# signals, AGP transfers are always a multiple of 8 bytes long, and the total length is included in the request. Further, rather than using the IRDY# and TRDY# signals for each word, data is transferred in blocks of four clock cycles (32 words at AGP 8\u00d7 speed), and pauses are allowed only between blocks. Finally, AGP allows (optional in AGP 1.0 and 2.0, mandatory in AGP 3.0) sideband addressing, meaning that the address and data buses are separated so the address phase does not use the main address/data (AD) lines at all. This is done by adding an extra 8-bit \"SideBand Address\" bus over which the graphics controller can issue new AGP requests while other AGP data is flowing over the main 32 address/data (AD) lines. This results in improved overall AGP data throughput. This great improvement in memory read performance makes it practical for an AGP card to read textures directly from system RAM, while a PCI graphics card must copy it from system RAM to the card's video memory. System memory is made available using the graphics address remapping table (GART), which apportions main memory as needed for texture storage.[2] The maximum amount of system memory available to AGP is defined as the AGP aperture. The AGP slot first appeared on x86-compatible system boards based on Socket 7 Intel P5 Pentium and Slot 1 P6 Pentium II processors. Intel introduced AGP support with the i440LX Slot 1 chipset on August 26, 1997, and a flood of products followed from all the major system board vendors.[3] The first Socket 7 chipsets to support AGP were the VIA Apollo VP3, SiS 5591/5592, and the ALI Aladdin V. Intel never released an AGP-equipped Socket 7 chipset. FIC demonstrated the first Socket 7 AGP system board in November 1997 as the FIC PA-2012 based on the VIA Apollo VP3 chipset, followed very quickly by the EPoX P55-VP3 also based on the VIA VP3 chipset which was first to market.[4] Early video chipsets featuring AGP support included the Rendition V\u00e9rit\u00e9 V2200, 3dfx Voodoo Banshee, Nvidia RIVA 128, 3Dlabs PERMEDIA 2, Intel i740, ATI Rage series, Matrox Millennium II, and S3 ViRGE GX/2. Some early AGP boards used graphics processors built around PCI and were simply bridged to AGP. This resulted in the cards benefiting little from the new bus, with the only improvement used being the 66\u00a0MHz bus clock, with its resulting doubled bandwidth over PCI, and bus exclusivity. Examples of such cards were the Voodoo Banshee, V\u00e9rit\u00e9 V2200, Millennium II, and S3 ViRGE GX/2. Intel's i740 was explicitly designed to exploit the new AGP feature set. In fact it was designed to texture only from AGP memory, making PCI versions of the board difficult to implement (local board RAM had to emulate AGP memory.) Microsoft first introduced AGP support into Windows 95 OEM Service Release 2 (OSR2 version 1111 or 950B) via the USB SUPPLEMENT to OSR2 patch.[5] After applying the patch the Windows 95 system became Windows 95 version 4.00.950 B. The first Windows NT-based operating system to receive AGP support was Windows NT 4.0 with Service Pack 3, introduced in 1997. Linux support for AGP enhanced fast data transfers was first added in 1999 with the implementation of the AGPgart kernel module. Intel released \"AGP specification 1.0\" in 1997.[6] It specified 3.3\u00a0V signals and 1\u00d7 and 2\u00d7 speeds.[3] Specification 2.0 documented 1.5\u00a0V signaling, which could be used at 1\u00d7, 2\u00d7 and the additional 4\u00d7 speed[7][8] and 3.0 added 0.8\u00a0V signaling, which could be operated at 4\u00d7 and 8\u00d7 speeds.[9] (1\u00d7 and 2\u00d7 speeds are physically possible, but were not specified.) Available versions are listed in the table on the right. AGP version 3.5 is only publicly mentioned by Microsoft under Universal Accelerated Graphics Port (UAGP), which specifies mandatory supports of extra registers once marked optional under AGP 3.0. Upgraded registers include PCISTS, CAPPTR, NCAPID, AGPSTAT, AGPCMD, NISTAT, NICMD. New required registers include APBASELO, APBASEHI, AGPCTRL, APSIZE, NEPG, GARTLO, GARTHI. There are various physical interfaces (connectors); see the Compatibility section. An official extension for cards that required more electrical power, with a longer slot with additional pins for that purpose. AGP Pro cards were usually workstation-class cards used to accelerate professional computer-aided design applications employed in the fields of architecture, machining, engineering, simulations, and similar fields.[10] A 64-bit channel was once proposed as an optional standard for AGP 3.0 in draft documents,[11] but it was dropped in the final version of the standard. The standard allows 64-bit transfer for AGP8\u00d7 reads, writes, and fast writes; 32-bit transfer for PCI operations. A number of non-standard variations of the AGP interface have been produced by manufacturers. AGP cards are backward and forward compatible within limits. 1.5 V-only keyed cards will not go into 3.3 V slots and vice versa, though \"Universal\" cards exist which will fit into either type of slot. There are also unkeyed \"Universal\" slots that will accept either type of card. When an AGP Universal card is plugged-into an AGP Universal slot, only the 1.5 V portion of the card is used. Some cards, like Nvidia's GeForce 6 series (except the 6200) or ATI's Radeon X800 series, only have keys for 1.5 V to prevent them from being installed in older mainboards without 1.5 V support. Some of the last modern cards with 3.3 V support were the Nvidia GeForce FX series (FX 5200, FX 5500, FX 5700, some FX 5800, FX 5900 and some FX 5950), Geforce 6 Series (6200, 6600/6600 LE/6600 GT only) and the ATI Radeon 9500/9700/9800(R350) (but not 9600/9800(R360)). Some Geforce 6200 and Geforce 6600 cards will function with AGP 1.0 (3.3v) slots. AGP Pro cards will not fit into standard slots, but standard AGP cards will work in a Pro slot. Motherboards equipped with a Universal AGP Pro slot will accept a 1.5 V or 3.3 V card in either the AGP Pro or standard AGP configuration, a Universal AGP card, or a Universal AGP Pro card. Some cards incorrectly have dual notches, and some motherboards incorrectly have fully open slots, allowing a card to be plugged into a slot that does not support the correct signaling voltage, which may damage card or motherboard. Some incorrectly designed older 3.3 V cards have the 1.5 V key. There are some proprietary systems incompatible with standard AGP; for example, Apple Power Macintosh computers with the Apple Display Connector (ADC) have an extra connector which delivers power to the attached display. Some cards designed to work with a specific CPU architecture (e.g., PC, Apple) may not work with others due to firmware issues. Mark Allen of Playtools.com made the following comments regarding Practical AGP Compatibility for AGP 3.0 and AGP 2.0:[15] \"...nobody makes AGP 3.0 cards, and nobody makes AGP 3.0 motherboards. At least not any manufacturers I can find. Every single video card I could find which claimed to be an AGP 3.0 card was actually a universal 1.5V AGP 3.0 card. And every motherboard which claimed to be an AGP 3.0 motherboard turned out to be a universal 1.5V AGP 3.0 motherboard. It makes sense, if you think about it, because if anyone actually shipped a consumer-oriented product which supported only 0.8 volts, they would end up with lots of confused customers and a support nightmare. In the consumer market, you'd have to be crazy to ship a 0.8 volt only product.\" Actual power supplied by an AGP slot depends upon the card used. The maximum current drawn from the various rails is given in the specifications for the various versions. For example, if maximum current is drawn from all supplies and all voltages are at their specified upper limits,[9]:95 an AGP\u00a03.0 slot can supply up to 48.25\u00a0watts; this figure can be used to specify a power supply conservatively, but in practice a card is unlikely ever to draw more than 40\u00a0W from the slot, with many using less. AGP Pro provides additional power up to 110\u00a0W. Many AGP cards had additional power connectors to supply them with more power than the slot could provide. By 2010 few new motherboards had AGP slots. No new motherboard chipsets were equipped with AGP support, but motherboards continued to be produced with older chipsets with support for AGP. Graphics processors of this period use PCI-Express, a general-purpose (not restricted to graphics) standard that supports higher data transfer rates and full-duplex. To create AGP-compatible graphics cards, those chips require an additional PCIe-to-AGP bridge-chip to convert PCIe signals to and from AGP signals. This incurs additional board costs due to the need for the additional bridge chip and for a separate AGP-designed circuit board. Various manufacturers of graphics cards continued to produce AGP cards for the shrinking AGP user-base. The first bridged cards were the GeForce 6600 and ATI Radeon X800 XL boards, released during 2004-5.[16][17] In 2009 AGP cards from Nvidia had a ceiling of the GeForce 7 Series. In 2011 DirectX 10-capable AGP cards from AMD vendors (Club 3D, HIS, Sapphire, Jaton, Visiontek, Diamond, etc.) included the Radeon HD 2400, 3450, 3650, 3850, 4350, 4650, and 4670. The HD 5000 AGP series mentioned in the catalyst software was never available. There were many problems with the AMD Catalyst 11.2 - 11.6 AGP hotfix drivers under Windows 7 with the HD 4000 series AGP video cards;[18] use of 10.12 or 11.1 AGP hotfix drivers is the recommended[by whom?] workaround. Several of the vendors listed above make available past versions of the AGP drivers. An AGP bus is a superset of a 66\u00a0MHz conventional PCI bus and, immediately after reset, follows the same protocol. The card must act as a PCI target, and optionally may act as a PCI master. (AGP 2.0 added a \"fast writes\" extension which allows PCI writes from the motherboard to the card to transfer data at higher speed.) After initialization, AGP transactions are permitted. For these, the card is always the AGP master and the motherboard is always the AGP target. The card queues multiple requests which correspond to the PCI address phase, and the motherboard schedules the corresponding data phases later. An important part of initialization is telling the card the maximum number of outstanding AGP requests which may be queued at a given time. AGP requests are similar to PCI memory read and write requests, but use a different encoding on command lines C/BE[3:0] and are always 8-byte aligned; their starting address and length are always multiples of 8 bytes (64 bits). The three low-order bits of the address are used instead to communicate the length of the request. Whenever the PCI GNT# signal is asserted, granting the bus to the card, three additional status bits ST[2:0] indicate the type of transfer to be performed next. If the three bits are 111, the card may begin a PCI transaction or (if sideband addressing is not in use) queue a request in-band using PIPE#. To queue a request in-band, the card must request the bus using the standard PCI REQ# signal, and receive GNT# plus bus status ST[2:0] equal to 111. Then the card asserts the PIPE# signal while driving the AGP command, address, and length on the C/BE[3:0], AD[31:3] and AD[2:0] lines, respectively. (If the address is 64 bits, a dual address cycle similar to PCI is used.) For every cycle that PIPE# is asserted, the card sends another request without waiting for acknowledgement from the motherboard, up to the configured maximum queue depth. The last cycle is marked by deasserting REQ#, and PIPE# is deasserted on the following idle cycle. In-band requests are always sent at single data rate (1\u00d7 speed). Possible request codes are: AGP 3.0 dropped high-priority requests and the long read commands, as they were little used. If side-band addressing is supported and configured, the PIPE# signal is not used. (And the signal is re-used for another purpose in the AGP 3.0 protocol, which requires side-band addressing.) Instead, requests are broken into 16-bit pieces and sent across the SBA bus. The possible values are: Sideband address bytes are sent at the same rate as data transfers, up to 8\u00d7 the 66\u00a0MHz basic bus clock. Sideband addressing has the advantage that it mostly eliminates the need for turnaround cycles on the AD bus between transfers, in the usual case when read operations greatly outnumber writes. While asserting GNT#, the motherboard may instead indicate via the ST bits that a data phase for a queued request will be performed next. There are four queues: two priorities (low- and high-priority) for each of reads and writes, and each is processed in order. Obviously, the motherboard will attempt to complete high-priority requests first, but there is no limit on the number of low-priority responses which may be delivered while the high-priority request is processed. For each cycle when the GNT# is asserted and the status bits have the value 00p, a read response of the indicated priority is scheduled to be returned. At the next available opportunity (typically the next clock cycle), the motherboard will assert TRDY# (target ready) and begin transferring the response to the oldest request in the indicated read queue. (Other PCI bus signals like FRAME#, DEVSEL# and IRDY# remain deasserted.) Up to four clock cycles worth of data (16 bytes at AGP 1\u00d7 or 128 bytes at AGP 8\u00d7) are transferred without waiting for acknowledgement from the card. If the response is longer than that, both the card and motherboard must indicate their ability to continue on the third cycle by asserting IRDY# (initiator ready) and TRDY#, respectively. If either one does not, wait states will be inserted until two cycles after they both do. (The value of IRDY# and TRDY# at other times is irrelevant and they are usually deasserted.) The C/BE# byte enable lines may be ignored during read responses, but are held asserted (all bytes valid) by the motherboard. The card may also assert the RBF# (read buffer full) signal to indicate that it is temporarily unable to receive more low-priority read responses. The motherboard will refrain from scheduling any more low-priority read responses. The card must still be able to receive the end of the current response, and the first four-cycle block of the following one if scheduled, plus any high-priority responses it has requested. For each cycle when GNT# is asserted and the status bits have the value 01p, write data is scheduled to be sent across the bus. At the next available opportunity (typically the next clock cycle), the card will assert IRDY# (initiator ready) and begin transferring the data portion of the oldest request in the indicated write queue. If the data is longer than four clock cycles, the motherboard will indicate its ability to continue by asserting TRDY# on the third cycle. Unlike reads, there is no provision for the card to delay the write; if it didn't have the data ready to send, it shouldn't have queued the request. The C/BE# lines are used with write data, and may be used by the card to select which bytes should be written to memory. The multiplier in AGP 2\u00d7, 4\u00d7 and 8\u00d7 indicates the number of data transfers across the bus during each 66\u00a0MHz clock cycle. Such transfers use source synchronous clocking with a \"strobe\" signal (AD_STB[0], AD_STB[1], and SB_STB) generated by the data source. AGP 4\u00d7 adds complementary strobe signals. At AGP 4\u00d7 and 8\u00d7 speeds, it is possible for a request to complete in the middle of a clock cycle. In such a case, the cycle is padded with dummy data transfers (with the C/BE# byte enable lines held deasserted). The AGP connector contains almost all PCI signals, plus several additions. The connector has 66 contacts on each side, although 4 are removed for each keying notch. Pin\u00a01 is closest to the I/O bracket, and the B and A sides are as in the table, looking down at the motherboard connector. Contacts are spaced at 1\u00a0mm intervals, however they are arranged in two staggered vertical rows so that there is 2\u00a0mm space between pins in each row. Odd-numbered A-side contacts, and even-numbered B-side contacts are in the lower row (1.0 to 3.5\u00a0mm from the card edge). The others are in the upper row (3.7 to 6.0\u00a0mm from the card edge). PCI signals omitted are: Signals added are: This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the \"relicensing\" terms of the GFDL, version 1.3 or later."}, "AH": {"link": "https://en.wikipedia.org/wiki/Active_hub", "full_form": "Active Hub", "content": "An Ethernet hub, active hub, network hub, repeater hub, multiport repeater, or simply hub is a network hardware device for connecting multiple Ethernet devices together and making them act as a single network segment. It has multiple input/output (I/O) ports, in which a signal introduced at the input of any port appears at the output of every port except the original incoming.[1] A hub works at the physical layer (layer 1) of the OSI model.[2] Repeater hubs also participate in collision detection, forwarding a jam signal to all ports if it detects a collision. In addition to standard 8P8C (\"RJ45\") ports, some hubs may also come with a BNC or Attachment Unit Interface (AUI) connector to allow connection to legacy 10BASE2 or 10BASE5 network segments. Hubs are now largely obsolete, having been replaced by network switches except in very old installations or specialized applications. As of 2011, connecting network segments by repeaters or hubs is deprecated by IEEE 802.3.[3][4][5]   A network hub is an unsophisticated device in comparison with a switch. As a multiport repeater it works by repeating bits (symbols) received from one of its ports to all other ports. It is aware of physical layer packets, that is it can detect their start (preamble), an idle line (interpacket gap) and sense a collision which it also propagates by sending a jam signal. A hub cannot further examine or manage any of the traffic that comes through it: any packet entering any port is rebroadcast on all other ports.[6] A hub/repeater has no memory to store any data in \u2013 a packet must be transmitted while it is received or is lost when a collision occurs (the sender should detect this and retry the transmission). Due to this, hubs can only run in half duplex mode. Consequently, due to a larger collision domain, packet collisions are more frequent in networks connected using hubs than in networks connected using more sophisticated devices.[2] The need for hosts to be able to detect collisions limits the number of hubs and the total size of a network built using hubs (a network built using switches does not have these limitations). For 10\u00a0Mbit/s networks built using repeater hubs, the 5-4-3 rule must be followed: up to five segments (four hubs) are allowed between any two end stations.[6] For 10BASE-T networks, up to five segments and four repeaters are allowed between any two hosts.[7] For 100\u00a0Mbit/s networks, the limit is reduced to 3 segments (2 Class II hubs) between any two end stations, and even that is only allowed if the hubs are of Class II. Some hubs have manufacturer specific stack ports allowing them to be combined in a way that allows more hubs than simple chaining through Ethernet cables, but even so, a large Fast Ethernet network is likely to require switches to avoid the chaining limits of hubs.[2] Most hubs detect typical problems, such as excessive collisions and jabbering on individual ports, and partition the port, disconnecting it from the shared medium. Thus, hub-based twisted-pair Ethernet is generally more robust than coaxial cable-based Ethernet (e.g. 10BASE2), where a misbehaving device can adversely affect the entire collision domain.[6] Even if not partitioned automatically, a hub simplifies troubleshooting because hubs remove the need to troubleshoot faults on a long cable with multiple taps; status lights on the hub can indicate the possible problem source or, as a last resort, devices can be disconnected from a hub one at a time much more easily than from a coaxial cable.[citation needed] To pass data through the repeater in a usable fashion from one segment to the next, the framing and data rate must be the same on each segment. This means that a repeater cannot connect an 802.3 segment (Ethernet) and an 802.5 segment (Token Ring) or a 10\u00a0Mbit/s segment to 100\u00a0Mbit/s Ethernet.[citation needed] 100 Mbit/s hubs and repeaters come in two different speed grades: Class I delay the signal for a maximum of 140 bit times (enabling translation/recoding between 100Base-TX, 100Base-FX and 100Base-T4) and Class II hubs delay the signal for a maximum of 92 bit times (enabling installation of two hubs in a single collision domain).[8] In the early days of Fast Ethernet, Ethernet switches were relatively expensive devices. Hubs suffered from the problem that if there were any 10BASE-T devices connected then the whole network needed to run at 10\u00a0Mbit/s. Therefore, a compromise between a hub and a switch was developed, known as a dual-speed hub. These devices make use of an internal two-port switch, bridging the 10\u00a0Mbit/s and 100\u00a0Mbit/s segments. When a network device becomes active on any of the physical ports, the device attaches it to either the 10\u00a0Mbit/s segment or the 100\u00a0Mbit/s segment, as appropriate. This obviated the need for an all-or-nothing migration to Fast Ethernet networks. These devices are considered hubs because the traffic between devices connected at the same speed is not switched.[citation needed] Repeater hubs have been defined for Gigabit Ethernet but commercial products have failed to appear due to the industry's transition to switching.[9] Historically, the main reason for purchasing hubs rather than switches was their price. This motivator has largely been eliminated by reductions in the price of switches, but hubs can still be useful in special circumstances:"}, "AI": {"link": "https://en.wikipedia.org/wiki/Artificial_intelligence", "full_form": "Artificial Intelligence", "content": "Collective intelligence\nCollective action\nSelf-organized criticality\nHerd mentality\nPhase transition\nAgent-based modelling\nSynchronization\nAnt colony optimization\nParticle swarm optimization Social network analysis\nSmall-world networks\nCommunity identification\nCentrality\nMotifs\nGraph Theory\nScaling\nRobustness\nSystems biology\nDynamic networks Evolutionary computation\nGenetic algorithms\nGenetic programming\nArtificial life\nMachine learning\nEvolutionary developmental biology\nArtificial intelligence\nEvolutionary robotics Reaction-diffusion systems\nPartial differential equations\nDissipative structures\nPercolation\nCellular automata\nSpatial ecology\nSelf-replication\nSpatial evolutionary biology Operationalization\nFeedback\nSelf-reference\nGoal-oriented\nSystem dynamics\nSensemaking\nEntropy\nCybernetics\nAutopoiesis\nInformation theory\nComputation theory Ordinary differential equations\nIterative maps\nPhase space\nAttractors\nStability analysis\nPopulation dynamics\nChaos\nMultistability\nBifurcation Rational choice theory\nBounded rationality\nIrrational behaviour Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, the field of AI research defines itself as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal.[1] Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".[2] As machines become increasingly capable, mental facilities once thought to require intelligence are removed from the definition. For instance, optical character recognition is no longer perceived as an example of \"artificial intelligence\", having become a routine technology.[3] Capabilities currently classified as AI include successfully understanding human speech,[4] competing at a high level in strategic game systems (such as chess and Go[5]), autonomous cars, intelligent routing in content delivery networks, military simulations, and interpreting complex data. AI research is divided into subfields[6] that focus on specific problems, approaches, the use of a particular tool, or towards satisfying particular applications. The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.[7] General intelligence is among the field's long-term goals.[8] Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience, artificial psychology and many others. The field was founded on the claim that human intelligence \"can be so precisely described that a machine can be made to simulate it\".[9] This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity.[10] Some people also consider AI a danger to humanity if it progresses unabatedly.[11] Attempts to create artificial intelligence have experienced many setbacks, including the ALPAC report of 1966, the abandonment of perceptrons in 1970, the Lighthill Report of 1973, the second AI winter 1987\u20131993 and the collapse of the Lisp machine market in 1987. In the twenty-first century, AI techniques, both hard (using a symbolic approach) and soft (sub-symbolic), have experienced a resurgence following concurrent advances in computer power, sizes of training sets, and theoretical understanding, and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.[12] Recent advancements in AI, and specifically in machine learning, have contributed to the growth of Autonomous Things such as drones and self-driving cars, becoming the main driver of innovation in the automotive industry. While thought-capable artificial beings appeared as storytelling devices in antiquity,[13] the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers.[14] Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's Frankenstein or Karel \u010capek's R.U.R. (Rossum's Universal Robots).[15] The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church\u2013Turing thesis.[16][page\u00a0needed] Along with concurrent discoveries in neurology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain.[17] The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".[14] The field of AI research was born at a workshop at Dartmouth College in 1956.[18] Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research.[19] They and their students produced programs that the press described as \"astonishing\":[20] computers were winning at checkers, solving word problems in algebra, proving logical theorems and speaking English.[21] By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[22] and laboratories had been established around the world.[23] AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do.\" Marvin Minsky agreed, writing, \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved.\"[24] They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[25] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\",[26] a period when funding AI projects was difficult. In the early 1980s, AI research was revived by the commercial success of expert systems,[27] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[28] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.[29] In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas.[12] The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields and a commitment by researchers to mathematical methods and scientific standards.[30] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.[31] Advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception.[32] By the mid 2010s, machine learning applications were used throughout the world.[33] In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.[34] The Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research[35] as do intelligent personal assistants in smartphones.[36] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[5][37] In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie,[38] who at the time continuously held the world No. 1 ranking for two years[39][40] According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011.[41] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.[41] The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[7] Erik Sandwell emphasizes planning and learning that is relevant and applicable to the given situation.[42] Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[43] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[44] For difficult problems, algorithms can require enormous computational resources\u2014most experience a \"combinatorial explosion\": the amount of memory or computer time required becomes astronomical for problems of a certain size. The search for more efficient problem-solving algorithms is a high priority.[45] Human beings ordinarily use fast, intuitive judgments rather than step-by-step deduction that early AI research was able to model.[46] AI has progressed using \"sub-symbolic\" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the human ability to guess. Knowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language.[47] The most general ontologies are called upper ontologies, which act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.[48]. Among the most difficult problems in knowledge representation are: Intelligent agents must be able to set goals and achieve them.[55] They need a way to visualize the future\u2014a representation of the state of the world and be able to make predictions about how their actions will change it\u2014and be able to make choices that maximize the utility (or \"value\") of available choices.[56] In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[57] However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.[58] Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[59] Machine learning, a fundamental concept of AI research since the field's inception,[60] is the study of computer algorithms that improve automatically through experience.[61][62] Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning[63] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.[64] Within developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).[65][66][67][68] Natural language processing[69] gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[70] and machine translation.[71] A common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency. Machine perception[72] is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision[73] is the ability to analyze visual input. A few selected subproblems are speech recognition,[74] facial recognition and object recognition.[75] The field of robotics[76] is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation[77] and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).[78][79] Affective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects.[81][82] It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science.[83] While the origins of the field may be traced as far back as the early philosophical inquiries into emotion,[84] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[85] on \"affective computing\".[86][87] A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions. Emotion and social skills[88] are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human-computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction. A sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs). Many researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas.[8][89] A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.[90][91] Many of the problems above also require that general intelligence be solved. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", but all of these problems need to be solved simultaneously in order to reach human-level machine performance. There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues.[92] A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?[93] Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?[94] Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require \"sub-symbolic\" processing?[95] John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence,[96] a term which has since been adopted by some non-GOFAI researchers.[97][98] Stuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior.[99] Computational philosophy, is used to develop an adaptive, free-flowing computer mind.[99] Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish.[99] Together, the humanesque behavior, mind, and actions make up artificial intelligence. In the 1940s and 1950s, a number of researchers explored the connection between neurology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England.[17] By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s. When access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI \"good old fashioned AI\" or \"GOFAI\".[100] During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background.[101] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Economist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[102][103] Unlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms.[93] His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[104] Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[105] Researchers at MIT (such as Marvin Minsky and Seymour Papert)[106] found that solving difficult problems in vision and natural language processing required ad-hoc solutions \u2013 they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).[94] Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[107] When computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications.[108] This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software.[27] The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications. By the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems.[95] Sub-symbolic methods manage to approach intelligence without specific representations of knowledge. This includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive.[109] Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence. Interest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of 1980s.[110] Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.[111] In the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a \"revolution\" and \"the victory of the neats\".[30] Critics argue that these techniques (with few exceptions[112]) are too focused on particular problems and have failed to address the long-term goal of general intelligence.[113] There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.[114][115] In the course of 60+ years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below. Many problems in AI can be solved in theory by intelligently searching through many possible solutions:[119] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[120] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[121] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[77] Many learning algorithms use search algorithms based on optimization. Simple exhaustive searches[122] are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that eliminate choices that are unlikely to lead to the goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies.[123] Heuristics limit the search for solutions into a smaller sample size.[78] A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.[124] Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization)[125] and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).[126] Logic[127] is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[128] and inductive logic programming is a method for learning.[129] Several different forms of logic are used in AI research. Propositional or sentential logic[130] is the logic of statements which can be true or false. First-order logic[131] also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic,[132] is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic[133] models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence. Default logics, non-monotonic logics and circumscription[50] are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics;[134] situation calculus, event calculus and fluent calculus (for representing events and time);[135] causal calculus;[136] belief calculus;[137] and modal logics.[138] Many problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.[139] Bayesian networks[140] are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm),[141] learning (using the expectation-maximization algorithm),[142] planning (using decision networks)[143] and perception (using dynamic Bayesian networks).[144] Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[144] A key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[145] and information value theory.[56] These tools include models such as Markov decision processes,[146] dynamic decision networks,[144] game theory and mechanism design.[147] The simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if shiny then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[148] A classifier can be trained in various ways; there are many statistical and machine learning approaches. The most widely used classifiers are the neural network,[149] kernel methods such as the support vector machine,[150] k-nearest neighbor algorithm,[151] Gaussian mixture model,[152] naive Bayes classifier,[153] and decision tree.[154] The performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the \"no free lunch\" theorem. Determining a suitable classifier for a given problem is still more an art than science.[155] The study of non-learning artificial neural networks[149] began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others. The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[156] Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or competitive learning.[157] Today, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa,[158][159] and was introduced to neural networks by Paul Werbos.[160][161][162] Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.[163] Deep learning in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.[164][165][166] According to a survey,[167] the expression \"Deep Learning\" was introduced to the Machine Learning community by Rina Dechter in 1986[168] and gained traction after Igor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000.[169] The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965.[170][page\u00a0needed] These networks are trained one layer at a time. Ivakhnenko's 1971 paper[171] describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning.[172] Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[173] Deep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980.[174] In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.[175] Since 2011, fast implementations of CNNs on GPUs have won many visual pattern recognition competitions.[166] Deep feedforward neural networks were used in conjunction with reinforcement learning by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.[176] Early on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs)[177] which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence.[166] RNNs can be trained by gradient descent[178][179][180] but suffer from the vanishing gradient problem.[164][181] In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.[182] Numerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997.[183] LSTM is often trained by Connectionist Temporal Classification (CTC).[184] At Google, Microsoft and Baidu this approach has revolutionised speech recognition.[185][186][187] For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users.[188] Google also used LSTM to improve machine translation,[189] Language Modeling[190] and Multilingual Language Processing.[191] LSTM combined with CNNs also improved automatic image captioning[192] and a plethora of other applications. Control theory, the grandchild of cybernetics, has many important applications, especially in robotics.[193] AI researchers have developed several specialized languages for AI research, including Lisp[194] and Prolog.[195] In 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.[196] Artificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.[197] For example, performance at draughts (i.e. checkers) is optimal,[198] performance at chess is high-human and nearing super-human (see computer chess:\u00a0computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human. A quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression.[199] Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers. A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.[200] AI is relevant to any intellectual task.[201] Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[202] High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions[203] and targeting online advertisements.[201][204][205] With social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution,[206] major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.[207] There are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games. Artificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.[208] \u00a0There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are way too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers.[209] Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.[210] According to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed.[211] Advancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.[212] Many components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers are integrated into one complex vehicle.[213] One main factor that influences the ability for a driver-less car to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings.[214] Some self-driving cars are not equipped with steering wheels or brakes, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.[215] Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. Use of AI in banking can be tracked back to 1987 when Security Pacific National Bank in USA set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Apps like Kasisito and Moneystream are using AI in financial services Banks use artificial intelligence systems to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place.[216] In August 2001, robots beat humans in a simulated financial trading competition.[217] AI has also reduced fraud and crime by monitoring behavioral patterns of users for any changes or anomalies.[218] Artificial intelligence is used to generate intelligent behaviors primarily in non-player characters (NPCs), often simulating human-like intelligence.[219] A platform (or \"computing platform\") is defined as \"some sort of hardware architecture or software framework (including application frameworks), that allows software to run\". As Rodney Brooks pointed out many years ago,[220] it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation. A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface.[221] Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch. Collective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors.[222][223] With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good.[224] As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit.[223] Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.[225][223] Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence.[226] They stated: \"This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning.\"[226] Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[227][223] There are three philosophical questions related to AI: Can a machine be intelligent? Can it \"think\"? Widespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to be how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.[237] Machines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes \"machine ethics\", \"artificial moral agents\", and the study of \"malevolent vs. friendly AI\". The development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded. A common concern about the development of artificial intelligence is the potential threat it could pose to mankind. This concern has recently gained attention after mentions by celebrities including Stephen Hawking, Bill Gates,[239] and Elon Musk.[240] A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development.[241] The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[242] In his book Superintelligence, Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's - one example is an AI told to compute as many digits of pi as possible - it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. For this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching.[243][244] Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.[245] Concern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence.[246] I think there is potentially a dangerous outcome there.\"[247][248] Development of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.[249] Joseph Weizenbaum wrote that AI applications can not, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy[250] was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.[251] Martin Ford, author of The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future,[252] and others argue that specialized artificial intelligence applications, robotics and other forms of automation will ultimately result in significant unemployment as machines begin to match and exceed the capability of workers to perform most routine and repetitive jobs. Ford predicts that many knowledge-based occupations\u2014and in particular entry level jobs\u2014will be increasingly susceptible to automation via expert systems, machine learning[253] and other AI-enhanced applications. AI-based applications may also be used to amplify the capabilities of low-wage offshore workers, making it more feasible to outsource knowledge work.[254][page\u00a0needed] This raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled Moral Machines in which he introduced the concept of artificial moral agents (AMA).[255] For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\"[256] and \"Can (Ro)bots Really Be Moral\".[257] For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.[258] The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making.[259] The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems\u2014it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\"[260] Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\"[259] that stems from the AAAI Fall 2005 Symposium on Machine Ethics.[260] Political scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent.[261] He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably, because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of mankind, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth. Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".[262] One proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place. Leading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.\"[263] If an AI system replicates all key aspects of human intelligence, will that system also be sentient \u2013 will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness. Computationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing.[264] Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam. The philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[265] Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be.[266] Mary Shelley's Frankenstein considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film A.I.: Artificial Intelligence, in which humanoid machines have the ability to feel emotions. This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature.[267] Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[268] The subject is profoundly discussed in the 2010 documentary film Plug & Pray.[269] Are there limits to how intelligent machines\u00a0\u2013 or human-machine hybrids\u00a0\u2013 can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. \u2018\u2019Superintelligence\u2019\u2019 may also refer to the form or degree of intelligence possessed by such an agent.[89] If research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[270] The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\".[271] Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[271][89] Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.[271] You awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence. Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either.[273] This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form. Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" (1863), and expanded upon by George Dyson in his book of the same name in 1998.[274] Thought-capable artificial beings have appeared as storytelling devices since antiquity.[13] The implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word \"robot\" itself was coined by Karel \u010capek in his 1921 play R.U.R., the title standing for \"Rossum's Universal Robots\". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics which he subsequently explored in a long series of robot stories. Asimov's laws are often brought up during layman discussions of machine ethics;[275] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[276] The novel Do Androids Dream of Electric Sheep?, by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.[277] Nowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL, the murderous computer in charge of the spaceship in 2001: A Space Odyssey (1968), is an example of the common \"robotic rampage\" archetype in science fiction movies. The Terminator (1984) and The Matrix (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[278] Cite error: A list-defined reference named \"Knowledge_representation\" is not used in the content (see the help page).\nCite error: A list-defined reference named \"Knowledge_engineering\" is not used in the content (see the help page).\nCite error: A list-defined reference named \"Ontology\" is not used in the content (see the help page). See also: Logic machines in fiction and List of fictional computers"}, "AIX": {"link": "https://en.wikipedia.org/wiki/IBM_AIX", "full_form": "Advanced Interactive eXecutive", "content": "AIX (Advanced Interactive eXecutive, pronounced /\u02cce\u026aa\u026a\u02c8\u025bks/[3]) is a series of proprietary Unix operating systems developed and sold by IBM for several of its computer platforms. Originally released for the IBM 6150 RISC workstation, AIX now supports or has supported a wide variety of hardware platforms, including the IBM RS/6000 series and later POWER and PowerPC-based systems, IBM System i, System/370 mainframes, PS/2 personal computers, and the Apple Network Server. AIX is based on UNIX System V with 4.3BSD-compatible extensions. It is one of six commercial operating systems that have versions certified to The Open Group's UNIX 03 standard (the others being macOS, Solaris, Inspur K-UX, HP-UX, and eulerOS ).[4] The AIX family of operating systems debuted in 1986, became the standard operating system for the RS/6000 series on its launch in 1990, and is still actively developed by IBM. It is currently supported on IBM Power Systems alongside IBM i and Linux. AIX was the first operating system to have a journaling file system, and IBM has continuously enhanced the software with features like processor, disk and network virtualization, dynamic hardware resource allocation (including fractional processor units), and reliability engineering ported from its mainframe designs.[5]   Unix started life at AT&T's Bell Labs research center in the early 1970s, running on DEC minicomputers. By 1976, the operating system was in use at various academic institutions, including Princeton, where Tom Lyon and others ported it to the S/370, to run as a guest OS under VM/370.[6] This port would later grow out to become UTS,[7] a mainframe Unix offering by IBM's competitor Amdahl Corporation.[8] IBM's own involvement in Unix can be dated to 1979, when it assisted Bell Labs in doing its own Unix port to the 370 (to be used as a build host for the 5ESS switch's software). In the process, IBM made modifications to the TSS/370 hypervisor to better support Unix.[9] It took until 1985 for IBM to offer its own Unix on the S/370 platform, IX/370, which was developed by Interactive Systems Corporation and intended by IBM to compete with Amdahl UTS.[10] The operating system offered special facilities for interoperating with PC/IX, Interactive/IBM's version of Unix for IBM PC compatible hardware, and was licensed at $10,000 per sixteen concurrent users.[11] AIX Version 1, introduced in 1986 for the IBM 6150 RT workstation, was based on UNIX System V Releases 1 and 2. In developing AIX, IBM and Interactive Systems Corporation (whom IBM contracted) also incorporated source code from 4.2 and 4.3 BSD UNIX. Among other variants, IBM later produced AIX Version 3 (also known as AIX/6000), based on System V Release 3, for their POWER-based RS/6000 platform. Since 1990, AIX has served as the primary operating system for the RS/6000 series (later renamed IBM eServer pSeries, then IBM System p, and now IBM Power Systems). AIX Version 4, introduced in 1994, added symmetric multiprocessing with the introduction of the first RS/6000 SMP servers and continued to evolve through the 1990s, culminating with AIX 4.3.3 in 1999. Version 4.1, in a slightly modified form, was also the standard operating system for the Apple Network Server systems sold by Apple Computer to complement the Macintosh line. In the late 1990s, under Project Monterey, IBM and the Santa Cruz Operation planned to integrate AIX and UnixWare into a single 32-bit/64-bit multiplatform UNIX with particular emphasis on running on Intel IA-64 (Itanium) architecture CPUs. A beta test version of AIX 5L for IA-64 systems was released, but according to documents released in the SCO v. IBM lawsuit, less than forty licenses for the finished Monterey Unix were ever sold before the project was terminated in 2002.[12] In 2003, the SCO Group alleged that (among other infractions) IBM had misappropriated licensed source code from UNIX System V Release 4 for incorporation into AIX; SCO subsequently withdrew IBM's license to develop and distribute AIX. IBM maintains that their license was irrevocable, and continued to sell and support the product until the litigation was adjudicated. AIX was a component of the 2003 SCO v. IBM lawsuit, in which the SCO Group filed a lawsuit against IBM, alleging IBM contributed SCO's intellectual property to the Linux codebase. The SCO Group, who argued they were the rightful owners of the copyrights covering the Unix operating system, attempted to revoke IBM's license to sell or distribute the AIX operating system. In March 2010, a jury returned a verdict finding that Novell, not the SCO Group, owns the rights to Unix.[13] AIX 6 was announced in May 2007, and it ran as an open beta from June 2007 until the general availability (GA) of AIX 6.1 on November 9, 2007. Major new features in AIX 6.1 included full role-based access control, workload partitions (which enable application mobility), enhanced security (Addition of AES encryption type for NFS v3 and v4), and Live Partition Mobility on the POWER6 hardware. AIX 7.1 was announced in April 2010, and an open beta ran until general availability of AIX 7.1 in September 2010. Several new features, including better scalability, enhanced clustering and management capabilities were added. AIX 7.1 includes a new built-in clustering capability called Cluster Aware AIX. AIX is able to organize multiple LPARs through the multipath communications channel to neighboring CPUs, enabling very high-speed communication between processors. This enables multi-terabyte memory address range and page table access to support global petabyte shared memory space for AIX POWER7 clusters so that software developers can program a cluster as if it were a single system, without using message passing (i.e. semaphore-controlled Inter-process Communication). AIX administrators can use this new capability to cluster a pool of AIX nodes. By default, AIX V7.1 pins kernel memory and includes support to allow applications to pin their kernel stack. Pinning kernel memory and the kernel stack for applications with real-time requirements can provide performance improvements by ensuring that the kernel memory and kernel stack for an application is not paged out.[14] AIX 7.2[15] was announced in October 2015, and released in December 2015. AIX 7.2 principal feature is the Live Kernel Update capability which allows OS fixes to replace the entire AIX kernel with no impact to applications. AIX 7.2 was also restructured to remove obsolete components. The networking component, bos.net.tcp.client was repackaged to allow additional installation flexibility. Unlike AIX 7.1, AIX 7.2 is only supported on systems based on POWER7 or later processors. The original AIX (sometimes called AIX/RT) was developed for the IBM 6150 RT workstation by IBM in conjunction with Interactive Systems Corporation, who had previously ported UNIX System III to the IBM PC for IBM as PC/IX.[16] According to its developers, the AIX source (for this initial version) consisted of one million lines of code.[17] Installation media consisted of eight 1.2M floppy disks. The RT was based on the ROMP microprocessor, the first commercial RISC chip. This was based on a design pioneered at IBM Research (the IBM 801) . One of the novel aspects of the RT design was the use of a microkernel, called Virtual Resource Manager (VRM). The keyboard, mouse, display, disk drives and network were all controlled by a microkernel. One could \"hotkey\" from one operating system to the next using the Alt-Tab key combination. Each OS in turn would get possession of the keyboard, mouse and display. Besides AIX v2, the PICK OS also included this microkernel. Much of the AIX v2 kernel was written in the PL/8 programming language, which proved troublesome during the migration to AIX v3.[citation needed] AIX v2 included full TCP/IP networking, as well as SNA and two networking file systems: NFS, licensed from Sun Microsystems, and Distributed Services (DS). DS had the distinction of being built on top of SNA, and thereby being fully compatible with DS on the IBM midrange AS/400 and mainframe systems. For the graphical user interfaces, AIX v2 came with the X10R3 and later the X10R4 and X11 versions of the X Window System from MIT, together with the Athena widget set. Compilers for Fortran and C were available. One of the more popular desktop applications was the PageMaker desktop publishing software. AIX PS/2 (also known as AIX/386) was developed by Locus Computing Corporation under contract to IBM.[16] AIX PS/2, first released in 1987,[18] ran on IBM PS/2 personal computers with Intel 386 and compatible processors. The product was announced in September 1988 with a baseline tag price of $595, although some utilities like uucp were included in a separate Extension package priced at $250. nroff and troff for AIX were also sold separately in a Text Formatting System package priced at $200. The TCP/IP stack for AIX PS/2 retailed for another $300. The X Window package was priced at $195, while the C and FORTRAN compilers each had a price tag of $275. Locus also made available their DOS Merge virtual machine environment for AIX, which could run MS DOS 3.3 applications inside AIX; DOS Merge was sold separately for another $250.[19] IBM also offered a $150 AIX PS/2 DOS Server Program, which provided file server and print server services for client computers running PC DOS 3.3.[20] The last version of PS/2 AIX is 1.3. It was released in 1992 and announced to add support for non-IBM (non-microchannel) computers as well.[21] Support for PS/2 AIX ended in March 1995.[22] In 1988, IBM announced AIX/370, also developed by Locus Computing. AIX/370 was IBM's third attempt to offer Unix-like functionality for their mainframe line, specifically the System/370 (the prior versions were a TSS/370-based Unix system developed jointly with AT&T c.1980,[9] and VM/IX, a VM/370-based system developed jointly with Interactive Systems Corporation c.1984). AIX/370 was released in 1990 with functional equivalence to System V Release 2 and 4.3BSD as well as IBM enhancements. With the introduction of the ESA/390 architecture, AIX/370 was replaced by AIX/ESA in 1991, which was based on OSF/1, and also ran on the System/390 platform. This development effort was made partly to allow IBM to compete with Amdahl UTS.[citation needed] Unlike AIX/370, AIX/ESA ran both natively as the host operating system, and as a guest under VM. AIX/ESA, while technically advanced, had little commercial success, partially because UNIX functionality was added as an option to the existing mainframe operating system, MVS, which became MVS/ESA OpenEdition in 1999.[citation needed] As part of Project Monterey, IBM released a beta test version of AIX 5L for the IA-64 (Itanium) architecture in 2001, but this never became an official product due to lack of interest.[12] The Apple Network Server systems were PowerPC-based systems designed by Apple Computer to have numerous high-end features that standard Apple hardware did not have, including swappable hard drives, redundant power supplies, and external monitoring capability. These systems were more or less based on the Power Macintosh hardware available at the time but were designed to use AIX (versions 4.1.4 or 4.1.5) as their native operating system in a specialized version specific to the ANS. AIX was only compatible with the Network Servers and was not ported to standard Power Macintosh hardware. Not to be confused is A/UX, Apple's earlier version of Unix for 68k-based Macintoshes. The release of AIX version 3 (sometimes called AIX/6000) coincided with the announcement of the first POWER1-based IBM RS/6000 models in 1990. AIX v3 innovated in several ways on the software side. It was the first operating system to introduce the idea of a journaling file system, JFS, which allowed for fast boot times by avoiding the need to ensure the consistency of the file systems on disks (see fsck) on every reboot. Another innovation was shared libraries which avoid the need for static linking from an application to the libraries it used. The resulting smaller binaries used less of the hardware RAM to run, and used less disk space to install. Besides improving performance, it was a boon to developers: executable binaries could be in the tens of kilobytes instead of a megabyte for an executable statically linked to the C library. AIX v3 also scrapped the microkernel of AIX v2, a contentious move that resulted in v3 containing no PL/I code and being somewhat more \"pure\" than v2. Other notable subsystems included: As of 2015[update], AIX runs on IBM Power, System p, System i, System p5, System i5, eServer p5, eServer pSeries and eServer i5 server product lines, as well as IBM BladeCenter blades[23] and IBM PureFlex compute nodes based on Power Architecture technology. AIX 7.1 fully exploits systems based on POWER7 processors include the Active Memory Expansion feature, which increases system flexibility where system administrators can configure logical partitions (LPARs) to use less physical memory. For example, an LPAR running AIX appears to the OS applications to be configured with 80 GB of physical memory but the hardware actually only consumes 60 GB of physical memory. Active Memory Expansion is a virtual memory compression system which employs memory compression technology to transparently compress in-memory data, allowing more data to be placed into memory and thus expanding the memory capacity of POWER7 systems. Using Active Memory Expansion can improve system utilization and increase a system\u2019s throughput. AIX 7 automatically manages the size of memory pages used to automatically use 4\u00a0KB, 64\u00a0KB or a combination of those page sizes. This self-tuning feature results in optimized performance without administrative effort. AIX 7.2 exploits POWER8 hardware features including accelerators and eight-way hardware multithreading. The default shell was Bourne shell up to AIX version 3, but was changed to Korn shell (ksh88) in version 4 in view of XPG4 and POSIX compliance.[1] The Common Desktop Environment (CDE) is AIX's default graphical user interface. As part of Linux Affinity and the free AIX Toolbox for Linux Applications (ATLA), open-source KDE Plasma Workspaces and GNOME desktop are also available. SMIT is the System Management Interface Tool for AIX. It allows a user to navigate a menu hierarchy of commands, rather than using the command line. Invocation is typically achieved with the command smit. Experienced system administrators make use of the F6 function key which generates the command line that SMIT will invoke to complete it. SMIT also generates a log of commands that are performed in the smit.script file. The smit.script file automatically records the commands with the command flags and parameters used. The smit.script file can be used as an executable shell script to rerun system configuration tasks. SMIT also creates the smit.log file, which contains additional detailed information that can be used by programmers in extending the SMIT system. smit and smitty refer to the same program, though smitty invokes the text-based version, while smit will invoke an X Window System based interface if possible; however, if smit determines that X Window System capabilities are not present, it will present the text-based version instead of failing. Determination of X Window System capabilities is typically performed by checking for the existence of the DISPLAY variable. Object Data Manager (ODM) is a database of system information integrated into IBM's AIX operating system,[36][37] analogous to the registry in Microsoft Windows.[38] A good understanding of the ODM is essential for managing AIX systems.[39] Data managed in ODM is stored and maintained as objects with associated attributes.[40] Interaction with ODM is possible via application programming interface (API) library for programs, and command-line utilities such us odmshow, odmget, odmadd, odmchange and odmdelete for shell scripts and users. SMIT and its associated AIX commands can also be used to query and modify information in the ODM.[41] Example of information stored in the ODM database are:"}, "Ajax": {"link": "https://en.wikipedia.org/wiki/Ajax_(programming)", "full_form": "Asynchronous JavaScript and XML", "content": "Ajax (also AJAX; /\u02c8e\u026ad\u0292\u00e6ks/; short for \"asynchronous JavaScript and XML\")[1][2] is a set of Web development techniques using many Web technologies on the client side to create asynchronous Web applications. With Ajax, Web applications can send data to and retrieve from a server asynchronously (in the background) without interfering with the display and behavior of the existing page. By decoupling the data interchange layer from the presentation layer, Ajax allows for Web pages, and by extension Web applications, to change content dynamically without the need to reload the entire page.[3] In practice, modern implementations commonly substitute JSON for XML due to the advantages of being native to JavaScript.[4] Ajax is not a single technology, but rather a group of technologies. HTML and CSS can be used in combination to mark up and style information. The DOM is accessed with JavaScript to dynamically display \u2013 and allow the user to interact with \u2013 the information presented. JavaScript and the XMLHttpRequest object provide a method for exchanging data asynchronously between browser and server to avoid full page reloads.   In the early-to-mid 1990s, most Web sites were based on complete HTML pages. Each user action required that a complete new page be loaded from the server. This process was inefficient, as reflected by the user experience: all page content disappeared, then the new page appeared. Each time the browser reloaded a page because of a partial change, all of the content had to be re-sent, even though only some of the information had changed. This placed additional load on the server and made bandwidth a limiting factor on performance. In 1996, the iframe tag was introduced by Internet Explorer; like the object element it can load or fetch content asynchronously. In 1998, the Microsoft Outlook Web App team developed the concept behind the XMLHttpRequest scripting object.[5] It appeared as XMLHTTP in the second version of the MSXML library,[5][6] which shipped with Internet Explorer 5.0 in March 1999.[7] The functionality of the XMLHTTP ActiveX control in IE 5 was later implemented by Mozilla, Safari, Opera and other browsers as the XMLHttpRequest JavaScript object.[8] Microsoft adopted the native XMLHttpRequest model as of Internet Explorer 7. The ActiveX version is still supported in Internet Explorer, but not in Microsoft Edge. The utility of these background HTTP requests and asynchronous Web technologies remained fairly obscure until it started appearing in large scale online applications such as Outlook Web App (2000)[9] and Oddpost (2002). Google made a wide deployment of standards-compliant, cross browser Ajax with Gmail (2004) and Google Maps (2005).[10] In October 2004 Kayak.com's public beta release was among the first large-scale e-commerce uses of what their developers at that time called \"the xml http thing\".[11] The term Ajax was publicly used on 18 February 2005 by Jesse James Garrett in an article titled \"Ajax: A New Approach to Web Applications\", based on techniques used on Google pages.[2] On 5 April 2006, the World Wide Web Consortium (W3C) released the first draft specification for the XMLHttpRequest object in an attempt to create an official Web standard.[12][13] The latest draft of the XMLHttpRequest object was published on 30 January 2014.[14] The term Ajax has come to represent a broad group of Web technologies that can be used to implement a Web application that communicates with a server in the background, without interfering with the current state of the page. In the article that coined the term Ajax,[2][3] Jesse James Garrett explained that the following technologies are incorporated: Since then, however, there have been a number of developments in the technologies used in an Ajax application, and in the definition of the term Ajax itself. XML is no longer required for data interchange and, therefore, XSLT is no longer required for the manipulation of data. JavaScript Object Notation (JSON) is often used as an alternative format for data interchange,[15] although other formats such as preformatted HTML or plain text can also be used.[16] A variety of popular JavaScript libraries, including JQuery, include abstractions to assist in executing Ajax requests. Asynchronous HTML and HTTP (AHAH) involves using XMLHTTPRequest to retrieve (X)HTML fragments, which are then inserted directly into the Web page. An example of a simple Ajax request using the GET method, written in JavaScript. get-ajax-data.js: send-ajax-data.php: The same example as above written in the popular JavaScript library jQuery."}, "AL": {"link": "https://en.wikipedia.org/wiki/Access_list", "full_form": "Access List", "content": "An access control list (ACL), with respect to a computer file system, is a list of permissions attached to an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.[1] Each entry in a typical ACL specifies a subject and an operation. For instance, if a file object has an ACL that contains (Alice: read,write; Bob: read), this would give Alice permission to read and write the file and Bob to only read it.   Many kinds of systems implement ACLs, or have a historical implementation. A filesystem ACL is a data structure (usually a table) containing entries that specify individual user or group rights to specific system objects such as programs, processes, or files. These entries are known as access-control entries (ACEs) in the Microsoft Windows NT,[2] OpenVMS, Unix-like, and Mac OS X operating systems. Each accessible object contains an identifier to its ACL. The privileges or permissions determine specific access rights, such as whether a user can read from, write to, or execute an object. In some implementations, an ACE can control whether or not a user, or group of users, may alter the ACL on an object. Most of the Unix and Unix-like operating systems (e.g. Linux,[3] BSD, or Solaris) support POSIX.1e ACLs, based on an early POSIX draft that was withdrawn in 1997. Many of them, for example AIX, FreeBSD,[4] Mac OS X beginning with version 10.4 (\"Tiger\"), or Solaris with ZFS filesystem,[5] support NFSv4 ACLs, which are part of the NFSv4 standard. There are two experimental implementations of NFSv4 ACLs for Linux: NFSv4 ACLs support for Ext3 filesystem[6] and the more recent Richacls,[7] which brings NFSv4 ACLs support for Ext4 filesystem. PRIMOS featured ACLs at least as early as 1984.[8] In the 1990s the ACL and RBAC models were extensively tested[by whom?] and used to administer file permissions. On some types of proprietary computer-hardware (in particular routers and switches), an access control list provides rules that are applied to port numbers or IP addresses that are available on a host or other layer 3, each with a list of hosts and/or networks permitted to use the service. Although it is additionally possible to configure access control lists based on network domain names, this is a questionable idea because individual TCP, UDP, and ICMP headers do not contain domain names. Consequently, the device enforcing the access control list must separately resolve names to numeric addresses. This presents an additional attack surface for an attacker who is seeking to compromise security of the system which the access control list is protecting. Both individual servers as well as routers can have network ACLs. Access control lists can generally be configured to control both inbound and outbound traffic, and in this context they are similar to firewalls. Like firewalls, ACLs could be subject to security regulations and standards such as PCI DSS. ACL algorithms have been ported to SQL and to relational database systems. Many \"modern\" (2000s and 2010s) SQL-based systems, like enterprise resource planning and content management systems, have used ACL models in their administration modules. The main alternative to the ACL model is the role-based access control (RBAC) model. A \"minimal RBAC model\", RBACm, can be compared with an ACL mechanism, ACLg, where only groups are permitted as entries in the ACL. Barkley (1997)[9] showed that RBACm and ACLg are equivalent. In modern SQL implementations, ACL also manage groups and inheritance in a hierarchy of groups. So \"modern ACLs\" can express all that RBAC express, and are notably powerful (compared to \"old ACLs\") in their ability to express access control policy in terms of the way in which administrators view organizations. For data interchange, and for \"high level comparisons\", ACL data can be translated to XACML.[10]"}, "ALAC": {"link": "https://en.wikipedia.org/wiki/Apple_Lossless_Audio_Codec", "full_form": "Apple Lossless Audio Codec", "content": "Apple Lossless, also known as Apple Lossless Audio Codec (ALAC), or Apple Lossless Encoder (ALE), is an audio coding format, and its reference audio codec implementation, developed by Apple Inc. for lossless data compression of digital music. After initially keeping it proprietary from its inception in 2004, in late 2011 Apple made the codec available open source and royalty-free. Traditionally, Apple has referred to the codec as Apple Lossless, though more recently they have begun to use the abbreviated term ALAC when referring to the codec.[1][2]   Apple Lossless supports up to 8 channels of audio at 16, 20, 24 and 32 bit depth with a maximum sample rate of 384kHz. Apple Lossless data is frequently stored within an MP4 container with the filename extension .m4a. This extension is also used by Apple for lossy AAC audio data in an MP4 container (same container, different audio encoding). However, Apple Lossless is not a variant of AAC (which is a lossy format), but rather a distinct lossless format that uses linear prediction similar to other lossless codecs. These other lossless codecs, such as FLAC and Shorten, are not natively supported by Apple's iTunes software (either the Mac OS or Windows versions) or by iOS devices, so users of iTunes software who want to use a lossless format which allows the addition of metadata (unlike WAV/AIFF or other PCM-type formats, where metadata is usually ignored) have to use ALAC.[3] All current iOS devices can play ALAC\u2013encoded files. ALAC also does not use any DRM scheme; but by the nature of the MP4 container, it is feasible that DRM could be applied to ALAC much in the same way it is applied to files in other QuickTime containers.[citation needed] According to Apple, audio files compressed with its lossless codec will use up \"about half the storage space\" that the uncompressed data would require. Testers using a selection of music have found that compressed files are about 40% to 60% the size of the originals depending on the kind of music, which is similar to other lossless formats.[4][5] Furthermore, compared to some other formats, it is not as difficult to decode, making it practical for a limited-power device, such as older iOS devices.[6][7] Partly because of the use of an MP4 container, Apple Lossless does not contain integrated error checking.[8] While not nearly as common, the ALAC format can also use the .CAF file type container. The software for encoding into ALAC files, Apple Lossless Encoder, was introduced into the Mac OS X Core Audio framework on April 28, 2004 together with the QuickTime 6.5.1 update; thus making it available in iTunes since version 4.5 and above.[9] The codec is also used in the AirPort and AirPlay implementation. The Apple Lossless Encoder (and decoder) were released as open source software under the Apache License version 2.0 on October 27, 2011,[10][11][12] however an independent reverse-engineered open-source encoder and decoder were already available before the release. David Hammerton and Cody Brocious have analyzed and decoded this codec without any documents on the format. On March 5, 2005, Hammerton published a simple open source decoder in the programming language C on the basis of the reverse engineering work.[13] The open source library libavcodec incorporates both a decoder and an encoder for Apple Lossless format, which means that media players based on that library (including VLC media player and MPlayer, as well as many media center applications for home theater computers, such as Plex, XBMC, and Boxee) are able to play Apple Lossless files. The library was subsequently optimized for ARM processors and included in Rockbox. Foobar2000 will also play Apple Lossless files as will JRiver Media Center and BitPerfect."}, "ALGOL": {"link": "https://en.wikipedia.org/wiki/ALGOL", "full_form": "Algorithmic Language", "content": "ALGOL (short for Algorithmic Language)[1] is a family of imperative computer programming languages, originally developed in the mid-1950s, which greatly influenced many other languages and was the standard method for algorithm description used by the ACM in textbooks and academic sources for more than thirty years.[2] In the sense that the syntax of most modern languages is \"Algol-like\",[3] it was arguably the most influential of the four high-level programming languages with which it was roughly contemporary: FORTRAN, Lisp, and COBOL.[4] It was designed to avoid some of the perceived problems with FORTRAN and eventually gave rise to many other programming languages, including PL/I, Simula, BCPL, B, Pascal, and C. ALGOL introduced code blocks and the begin\u2026end pairs for delimiting them. It was also the first language implementing nested function definitions with lexical scope. Moreover, it was the first programming language which gave detailed attention to formal language definition and through the Algol 60 Report introduced Backus\u2013Naur form, a principal formal grammar notation for language design. There were three major specifications, named after the year they were first published: Niklaus Wirth based his own ALGOL W on ALGOL 60 before developing Pascal. ALGOL-W was based on the proposal for the next generation ALGOL, but the ALGOL 68 committee decided on a design that was more complex and advanced, rather than a cleaned, simplified ALGOL 60. ALGOL 68 is substantially different from ALGOL 60 and was not well received, so that in general \"Algol\" means ALGOL 60 and dialects thereof.   The International Algebraic Language (IAL) was extremely influential and generally considered the ancestor of most of the modern programming languages (the so-called Algol-like languages). Additionally, ALGOL object code was a simple, compact, and stack-based instruction set architecture commonly used in teaching compiler construction and other high order languages (of which Algol is generally considered the first). ALGOL was developed jointly by a committee of European and American computer scientists in a meeting in 1958 at ETH Zurich (cf. ALGOL 58). It specified three different syntaxes: a reference syntax, a publication syntax, and an implementation syntax. The different syntaxes permitted it to use different keyword names and conventions for decimal points (commas vs periods) for different languages. ALGOL was used mostly by research computer scientists in the United States and in Europe. Its use in commercial applications was hindered by the absence of standard input/output facilities in its description and the lack of interest in the language by large computer vendors other than Burroughs Corporation. ALGOL 60 did however become the standard for the publication of algorithms and had a profound effect on future language development. John Backus developed the Backus normal form method of describing programming languages specifically for ALGOL 58. It was revised and expanded by Peter Naur for ALGOL 60, and at Donald Knuth's suggestion renamed Backus\u2013Naur form.[8] Peter Naur: \"As editor of the ALGOL Bulletin I was drawn into the international discussions of the language and was selected to be member of the European language design group in November 1959. In this capacity I was the editor of the ALGOL 60 report, produced as the result of the ALGOL 60 meeting in Paris in January 1960.\"[9] The following people attended the meeting in Paris (from 1 to 16 January): Alan Perlis gave a vivid description of the meeting: \"The meetings were exhausting, interminable, and exhilarating. One became aggravated when one's good ideas were discarded along with the bad ones of others. Nevertheless, diligence persisted during the entire period. The chemistry of the 13 was excellent.\" ALGOL 60 inspired many languages that followed it. C. A. R. Hoare remarked: \"Here is a language so far ahead of its time that it was not only an improvement on its predecessors but also on nearly all its successors.\"[10] The Scheme programming language, a variant of Lisp that adopted the block structure and lexical scope of ALGOL, also adopted the wording \"Revised Report on the Algorithmic Language Scheme\" for its standards documents in homage to ALGOL.[11] As Peter Landin noted, the language Algol was the first language to combine seamlessly imperative effects with the (call-by-name) lambda calculus. Perhaps the most elegant formulation of the language is due to John C. Reynolds, and it best exhibits its syntactic and semantic purity. Reynolds's idealized Algol also made a convincing methodological argument regarding the suitability of local effects in the context of call-by-name languages, to be contrasted with the global effects used by call-by-value languages such as ML. The conceptual integrity of the language made it one of the main objects of semantic research, along with PCF and ML.[12] To date there have been at least 70 augmentations, extensions, derivations and sublanguages of Algol 60.[13] The Burroughs dialects included special Bootstrapping dialects such as ESPOL and NEWP. The latter is still used for Unisys MCP system software. ALGOL 60 as officially defined had no I/O facilities; implementations defined their own in ways that were rarely compatible with each other. In contrast, ALGOL 68 offered an extensive library of transput (input/output) facilities. ALGOL 60 allowed for two evaluation strategies for parameter passing: the common call-by-value, and call-by-name. Call-by-name has certain effects in contrast to call-by-reference. For example, without specifying the parameters as value or reference, it is impossible to develop a procedure that will swap the values of two parameters if the actual parameters that are passed in are an integer variable and an array that is indexed by that same integer variable.[19] Think of passing a pointer to swap(i, A[i]) in to a function. Now that every time swap is referenced, it is reevaluated. Say i\u00a0:= 1 and A[i]\u00a0:= 2, so every time swap is referenced it'll return the other combination of the values ([1,2], [2,1], [1,2] and so on). A similar situation occurs with a random function passed as actual argument. Call-by-name is known by many compiler designers for the interesting \"thunks\" that are used to implement it. Donald Knuth devised the \"man or boy test\" to separate compilers that correctly implemented \"recursion and non-local references.\" This test contains an example of call-by-name. ALGOL 68 was defined using a two-level grammar formalism invented by Adriaan van Wijngaarden and which bears his name. Van Wijngaarden grammars use a context-free grammar to generate an infinite set of productions that will recognize a particular ALGOL 68 program; notably, they are able to express the kind of requirements that in many other programming language standards are labelled \"semantics\" and have to be expressed in ambiguity-prone natural language prose, and then implemented in compilers as ad hoc code attached to the formal language parser. (The way the bold text has to be written depends on the implementation, e.g. 'INTEGER' -- quotation marks included -- for integer. This is known as stropping.) Here's an example of how to produce a table using Elliott 803 ALGOL.[20] PUNCH(3) sends output to the teleprinter rather than the tape punch.\nSAMELINE suppresses the carriage return + line feed normally printed between arguments.\nALIGNED(1,6) controls the format of the output with 1 digit before and 6 after the decimal point. The following code samples are ALGOL 68 versions of the above ALGOL 60 code samples. ALGOL 68 implementations used ALGOL 60's approaches to stropping. In ALGOL 68's case tokens with the bold typeface are reserved words, types (modes) or operators. Note: lower (\u230a) and upper (\u2308) bounds of an array, and array slicing, are directly available to the programmer. The variations and lack of portability of the programs from one implementation to another is easily demonstrated by the classic hello world program. ALGOL 58 had no I/O facilities. Since ALGOL 60 had no I/O facilities, there is no portable hello world program in ALGOL. A simpler program using an inline format: An even simpler program using the Display statement: An alternative example, using Elliott Algol I/O is as follows. Elliott Algol used different characters for \"open-string-quote\" and \"close-string-quote\": Here's a version for the Elliott 803 Algol (A104) The standard Elliott 803 used 5 hole paper tape and thus only had upper case. The code lacked any quote characters so \u00a3 (UK Pound Sign) was used for open quote and\u00a0? (Question Mark) for close quote. Special sequences were placed in double quotes (e.g. \u00a3\u00a3L?? produced a new line on the teleprinter). The ICT 1900 series Algol I/O version allowed input from paper tape or punched card. Paper tape 'full' mode allowed lower case. Output was to a line printer. The open and close quote characters were represented using '(' and ')' and spaces by\u00a0%.[21] ALGOL 68 code was published with reserved words typically in lowercase, but bolded or underlined. In the language of the \"Algol 68 Report\" the input/output facilities were collectively called the \"Transput\". The ALGOLs were conceived at a time when character sets were diverse and evolving rapidly; also, the ALGOLs were defined so that only uppercase letters were required. 1960: IFIP \u2013 The Algol 60 language and report included several mathematical symbols which are available on modern computers and operating systems, but, unfortunately, were not supported on most computing systems at the time. For instance: \u00d7, \u00f7, \u2264, \u2265, \u2260, \u00ac, \u2228, \u2227, \u2282, \u2261, \u2423 and \u23e8. 1961 September: ASCII\u00a0\u2013 The ASCII character set, then in an early stage of development, had the \\ (Back slash) character added to it in order to support ALGOL's boolean operators /\\ and \\/.[22] 1962: ALCOR \u2013 This character set included the unusual \"\u16ed\" (iron/runic cross[23]) character and the \"\u23e8\" (Decimal Exponent Symbol[24]) for floating point notation.[25][26][27] 1964: GOST \u2013 The 1964 Soviet standard GOST 10859 allowed the encoding of 4-bit, 5-bit, 6-bit and 7-bit characters in ALGOL.[28] 1968: The \"Algol 68 Report\" \u2013 used existing ALGOL characters, and further adopted \u2192, \u2193, \u2191, \u25a1, \u230a, \u2308, \u23a9, \u23a7, \u25cb, \u22a5 and \u00a2 characters which can be found on the IBM 2741 keyboard with \"golf-ball\" print heads inserted (such as the APL golfball). These became available in the mid-1960s while ALGOL 68 was being drafted. The report was translated into Russian, German, French and Bulgarian, and allowed programming in languages with larger character sets, e.g. Cyrillic alphabet of the Soviet BESM-4. All ALGOL's characters are also part of the Unicode standard and most of them are available in several popular fonts. 2009 October: Unicode \u2013 The \"\u23e8\" (Decimal Exponent Symbol[24]) for floating point notation was added to Unicode 5.2 for backward compatibility with historic Buran (spacecraft) ALGOL software.[29]"}, "ALSA": {"link": "https://en.wikipedia.org/wiki/Advanced_Linux_Sound_Architecture", "full_form": "Advanced Linux Sound Architecture", "content": "Advanced Linux Sound Architecture (ALSA) is a software framework and part of the Linux kernel that provides an application programming interface (API) for sound card device drivers. Some of the goals of the ALSA project at its inception were automatic configuration of sound-card hardware and graceful handling of multiple sound devices in a system. ALSA is released under the GNU General Public License (GPL) and the GNU Lesser General Public License (LGPL).[4] The sound servers PulseAudio and JACK (low-latency professional-grade audio editing and mixing), the higher-level abstraction APIs OpenAL, SDL audio, etc. work on top of ALSA and implemented sound card device drivers.   ALSA was designed with some features which were not, at the time of its conception, supported by the Open Sound System (OSS): ALSA has a larger and more complex API than OSS, so it can be more difficult to develop an application that uses ALSA as its sound technology. While ALSA may be configured to provide an OSS emulation layer, such functionality is no longer available or is not installed by default in many Linux distributions. Besides the sound device drivers, ALSA bundles a user-space library for application developers who want to use driver features through an interface that is higher-level than the interface provided for direct interaction with the kernel drivers. Unlike the kernel API, which tries to reflect the capabilities of the hardware directly, ALSA's user-space library presents an abstraction that remains as standardized as possible across disparate underlying hardware elements. This goal is achieved in part by using software plug-ins; for example, many modern sound cards or built-in sound chips do not have a \"master volume\" control. Instead, for these devices, the user space library provides a software volume control using the \"softvol\" plug-in, and ordinary application software need not care whether such a control is implemented by underlying hardware or software emulation of such underlying hardware. Additional to the software framework internal to the Linux kernel, the ALSA project also provides the command-line utilities alsactl, amixer, arecord/aplay and alsamixer, an ncurses-based TUI. There also are GUIs programmed by third-party developers, such as gnome-alsamixer (using GTK+), kmix, xfce4-mixer, lxpanel, qashctl, pavucontrol, alsamixergui (using FLTK) and probably even more.  This section provides an overview of basic concepts pertaining to ALSA.[5][6][7] Typically, ALSA supports up to eight cards, numbered 0 through 7; each card is a physical or logical kernel device capable of input, output. Furthermore, each card may also be addressed by its id, which is an explanatory string such as \"Headset\" or \"ICH9\". A card has devices, numbered starting at 0; a device may be of playback type, meaning it outputs sound from the computer, or some other type such as capture, control, timer, or sequencer[citation needed]; device number 0 is used by default when no particular device is specified. A device may have subdevices, numbered starting at 0; a subdevice represents some relevant sound endpoint for the device, such as a speaker pair. If the subdevice is not specified, or if subdevice number \u22121 is specified, then any available subdevice is used. A card's interface is a description of an ALSA protocol for accessing the card; possible interfaces include: hw, plughw, default, and plug:dmix. The hw interface provides direct access to the kernel device, but no software mixing or stream adaptation support. The plughw and default enable sound output where the hw interface would produce an error. An application typically describes sound output by combining all of the aforementioned specifications together in a device string[citation needed], which has one of the following forms (which are case-sensitive): An ALSA stream is a data flow representing sound; the most common stream format is PCM that must be produced in such a way as to match the characteristics or parameters of the hardware, including: The ALSA System on Chip (ASoC) layer aims to provide better support for ALSA on embedded systems that use a system-on-chip (SoC) design.[8] The project to develop ALSA was led by Jaroslav Kysela, and was based on the Linux device driver for the Gravis Ultrasound sound card. It started in 1998 and was developed separately from the Linux kernel until it was introduced in the 2.5 development series in 2002 (2.5.4\u20132.5.5).[9] In the 2.6 version, it replaced the previous system, Open Sound System (OSS), by default (although a backwards-compatibility layer does exist).[10] "}, "ALU": {"link": "https://en.wikipedia.org/wiki/Arithmetic_logic_unit", "full_form": "Arithmetic and Logical Unit", "content": "An arithmetic logic unit (ALU) is a combinational digital electronic circuit that performs arithmetic and bitwise operations on integer binary numbers. This is in contrast to a floating-point unit (FPU), which operates on floating point numbers. An ALU is a fundamental building block of many types of computing circuits, including the central processing unit (CPU) of computers, FPUs, and graphics processing units (GPUs). A single CPU, FPU or GPU may contain multiple ALUs. The inputs to an ALU are the data to be operated on, called operands, and a code indicating the operation to be performed and, optionally, status information from a previous operation; the ALU's output is the result of the performed operation. In many designs, the ALU also exchanges additional information with a status register, which relates to the result of the current or previous operations.   An ALU has a variety of input and output nets, which are the electrical conductors used to convey digital signals between the ALU and external circuitry. When an ALU is operating, external circuits apply signals to the ALU inputs and, in response, the ALU produces and conveys signals to external circuitry via its outputs. A basic ALU has three parallel data buses consisting of two input operands (A and B) and a result output (Y). Each data bus is a group of signals that conveys one binary integer number. Typically, the A, B and Y bus widths (the number of signals comprising each bus) are identical and match the native word size of the external circuitry (e.g., the encapsulating CPU or other processor). The opcode input is a parallel bus that conveys to the ALU an operation selection code, which is an enumerated value that specifies the desired arithmetic or logic operation to be performed by the ALU. The opcode size (its bus width) is related to the number of different operations the ALU can perform; for example, a four-bit opcode can specify up to sixteen different ALU operations. Generally, an ALU opcode is not the same as a machine language opcode, though in some cases it may be directly encoded as a bit field within a machine language opcode. The status outputs are various individual signals that convey supplemental information about the result of an ALU operation. These outputs are usually stored in registers so they can be used in future ALU operations or for controlling conditional branching. The collection of bit registers that store the status outputs are often treated as a single, multi-bit register, which is referred to as the \"status register\" or \"condition code register\". General-purpose ALUs commonly have status signals such as: The status input allows additional information to be made available to the ALU when performing an operation. Typically, this is a \"carry-in\" bit that is the stored carry-out from a previous ALU operation. An ALU is a combinational logic circuit, meaning that its outputs will change asynchronously in response to input changes. In normal operation, stable signals are applied to all of the ALU inputs and, when enough time (known as the \"propagation delay\") has passed for the signals to propagate through the ALU circuitry, the result of the ALU operation appears at the ALU outputs. The external circuitry connected to the ALU is responsible for ensuring the stability of ALU input signals throughout the operation, and for allowing sufficient time for the signals to propagate through the ALU before sampling the ALU result. In general, external circuitry controls an ALU by applying signals to its inputs. Typically, the external circuitry employs sequential logic to control the ALU operation, which is paced by a clock signal of a sufficiently low frequency to ensure enough time for the ALU outputs to settle under worst-case conditions. For example, a CPU begins an ALU addition operation by routing operands from their sources (which are usually registers) to the ALU's operand inputs, while the control unit simultaneously applies a value to the ALU's opcode input, configuring it to perform addition. At the same time, the CPU also routes the ALU result output to a destination register that will receive the sum. The ALU's input signals, which are held stable until the next clock, are allowed to propagate through the ALU and to the destination register while the CPU waits for the next clock. When the next clock arrives, the destination register stores the ALU result and, since the ALU operation has completed, the ALU inputs may be set up for the next ALU operation. A number of basic arithmetic and bitwise logic functions are commonly supported by ALUs. Basic, general purpose ALUs typically include these operations in their repertoires: ALU shift operations cause operand A (or B) to shift left or right (depending on the opcode) and the shifted operand appears at Y. Simple ALUs typically can shift the operand by only one bit position, whereas more complex ALUs employ barrel shifters that allow them to shift the operand by an arbitrary number of bits in one operation. In all single-bit shift operations, the bit shifted out of the operand appears on carry-out; the value of the bit shifted into the operand depends on the type of shift. Although an ALU can be designed to perform complex functions, the resulting higher circuit complexity, cost, power consumption and larger size makes this impractical in many cases. Consequently, ALUs are often limited to simple functions that can be executed at very high speeds (i.e., very short propagation delays), and the external processor circuitry is responsible for performing complex functions by orchestrating a sequence of simpler ALU operations. For example, computing the square root of a number might be implemented in various ways, depending on ALU complexity: The implementations above transition from fastest and most expensive to slowest and least costly. The square root is calculated in all cases, but processors with simple ALUs will take longer to perform the calculation because multiple ALU operations must be performed. An ALU is usually implemented either as a stand-alone integrated circuit (IC), such as the 74181, or as part of a more complex IC. In the latter case, an ALU is typically instantiated by synthesizing it from a description written in VHDL, Verilog or some other hardware description language. For example, the following VHDL code describes a very simple 8-bit ALU: Mathematician John von Neumann proposed the ALU concept in 1945 in a report on the foundations for a new computer called the EDVAC.[1] The cost, size, and power consumption of electronic circuitry was relatively high throughout the infancy of the information age. Consequently, all serial computers and many early computers, such as the PDP-8, had a simple ALU that operated on one data bit at a time, although they often presented a wider word size to programmers. One of the earliest computers to have multiple discrete single-bit ALU circuits was the 1948 Whirlwind\u00a0I, which employed sixteen of such \"math units\" to enable it to operate on 16-bit words. In 1967, Fairchild introduced the first ALU implemented as an integrated circuit, the Fairchild 3800, consisting of an eight-bit ALU with accumulator.[2] Other integrated-circuit ALUs soon emerged, including four-bit ALUs such as the Am2901 and 74181. These devices were typically \"bit slice\" capable, meaning they had \"carry look ahead\" signals that facilitated the use of multiple interconnected ALU chips to create an ALU with a wider word size. These devices quickly became popular and were widely used in bit-slice minicomputers. Microprocessors began to appear in the early 1970s. Even though transistors had become smaller, there was often insufficient die space for a full-word-width ALU and, as a result, some early microprocessors employed a narrow ALU that required multiple cycles per machine language instruction. Examples of this includes the popular Zilog Z80, which performed eight-bit additions with a four-bit ALU.[3] Over time, transistor geometries shrank further, following Moore's law, and it became feasible to build wider ALUs on microprocessors. Modern integrated circuit (IC) transistors are orders of magnitude smaller than those of the early microprocessors, making it possible to fit highly complex ALUs on ICs. Today, many modern ALUs have wide word widths, and architectural enhancements such as barrel shifters and binary multipliers that allow them to perform, in a single clock cycle, operations that would have required multiple operations on earlier ALUs."}, "AM": {"link": "https://en.wikipedia.org/wiki/Active-matrix_liquid_crystal_display", "full_form": "Active Matrix", "content": "An active-matrix liquid-crystal display (AMLCD) is a type of flat panel display, the only viable technology for high-resolution TVs, computer monitors, notebook computers, tablet computers and smartphones with an LCD screen, due to low weight, very good image quality, wide color gamut and response time. The concept of active-matrix LCDs was proposed by Bernard J. Lechner at the RCA Laboratories in 1968.[1][citation not found] The first functional AMLCD with thin-film transistors was made by T Peter Brody and his team at Westinghouse Electric Corporation in 1972.[2][3] However, it took years of additional research and development by others to launch successful products.   The most common type of LCD contains, besides the polarizing sheets and cells of liquid crystal, a matrix of thin-film transistors to make a thin-film-transistor liquid-crystal display.[4] These devices store the electrical state of each pixel on the display while all the other pixels are being updated. This method provides a much brighter, sharper display than a passive matrix of the same size. An important specification for these displays is their viewing-angle. Thin-film transistors are usually used for constructing an active matrix so that the two terms are often interchanged, even though a thin-film transistor is just one component in an active matrix and some active-matrix designs have used other components such as diodes. Whereas a passive matrix display uses a simple conductive grid to apply a voltage to the liquid crystals in the target area, an active-matrix display uses a grid of transistors and capacitors with the ability to hold a charge for a limited period of time. Because of the switching action of transistors, only the desired pixel receives a charge, and the pixel acts as a capacitor to hold the charge until the next refresh cycle, improving image quality over a passive matrix. This is a special version of a sample-and-hold circuit."}}